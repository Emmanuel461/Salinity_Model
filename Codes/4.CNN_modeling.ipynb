{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CNN model with the selected index\n",
    "\n",
    "### 1. Call the data and split it for model process\n",
    "### 2. Prepare the arquitecture, run and evaluate the model\n",
    "### 3. Try to find the best solution using gridsearch function\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Call the data and split it for model process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SR</th>\n",
       "      <th>...</th>\n",
       "      <th>b1_1 (Clay+Silt)</th>\n",
       "      <th>b2_1(Sand_Raster)</th>\n",
       "      <th>b3_1 (Silt Raster)</th>\n",
       "      <th>b4_1 (Clay Raster</th>\n",
       "      <th>b8_asm</th>\n",
       "      <th>Id</th>\n",
       "      <th>SAR</th>\n",
       "      <th>CE</th>\n",
       "      <th>Tanbanca</th>\n",
       "      <th>Tabanca_Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>548.50</td>\n",
       "      <td>707.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>926.50</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>1325.00</td>\n",
       "      <td>1664.50</td>\n",
       "      <td>2842.00</td>\n",
       "      <td>0.364127</td>\n",
       "      <td>2.145283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>1</td>\n",
       "      <td>26.926614</td>\n",
       "      <td>7.089333</td>\n",
       "      <td>Cafine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>541.00</td>\n",
       "      <td>662.00</td>\n",
       "      <td>784.50</td>\n",
       "      <td>844.50</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>967.00</td>\n",
       "      <td>1356.50</td>\n",
       "      <td>2455.50</td>\n",
       "      <td>0.439585</td>\n",
       "      <td>2.568796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519413</td>\n",
       "      <td>0.473399</td>\n",
       "      <td>0.337389</td>\n",
       "      <td>0.181547</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>2</td>\n",
       "      <td>86.327645</td>\n",
       "      <td>30.558000</td>\n",
       "      <td>Cafine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>491.00</td>\n",
       "      <td>623.50</td>\n",
       "      <td>765.50</td>\n",
       "      <td>818.00</td>\n",
       "      <td>1034.5</td>\n",
       "      <td>903.50</td>\n",
       "      <td>1446.50</td>\n",
       "      <td>2762.50</td>\n",
       "      <td>0.508677</td>\n",
       "      <td>3.070660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654887</td>\n",
       "      <td>0.343329</td>\n",
       "      <td>0.461192</td>\n",
       "      <td>0.192468</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>3</td>\n",
       "      <td>112.754286</td>\n",
       "      <td>44.382000</td>\n",
       "      <td>Cafine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>607.00</td>\n",
       "      <td>689.00</td>\n",
       "      <td>840.50</td>\n",
       "      <td>885.00</td>\n",
       "      <td>1112.5</td>\n",
       "      <td>983.00</td>\n",
       "      <td>1362.50</td>\n",
       "      <td>2370.00</td>\n",
       "      <td>0.416839</td>\n",
       "      <td>2.429589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524789</td>\n",
       "      <td>0.470715</td>\n",
       "      <td>0.431571</td>\n",
       "      <td>0.093336</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>4</td>\n",
       "      <td>193.130080</td>\n",
       "      <td>93.534000</td>\n",
       "      <td>Cafine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585.50</td>\n",
       "      <td>657.00</td>\n",
       "      <td>762.00</td>\n",
       "      <td>800.50</td>\n",
       "      <td>951.5</td>\n",
       "      <td>861.00</td>\n",
       "      <td>1086.50</td>\n",
       "      <td>1832.00</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>2.100475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517816</td>\n",
       "      <td>0.474641</td>\n",
       "      <td>0.407218</td>\n",
       "      <td>0.110598</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>5</td>\n",
       "      <td>126.060248</td>\n",
       "      <td>60.254000</td>\n",
       "      <td>Cafine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>974.50</td>\n",
       "      <td>703.75</td>\n",
       "      <td>1257.25</td>\n",
       "      <td>930.25</td>\n",
       "      <td>1528.5</td>\n",
       "      <td>1228.00</td>\n",
       "      <td>1406.75</td>\n",
       "      <td>2492.50</td>\n",
       "      <td>0.344621</td>\n",
       "      <td>2.075156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692295</td>\n",
       "      <td>0.310875</td>\n",
       "      <td>0.336182</td>\n",
       "      <td>0.356843</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>96</td>\n",
       "      <td>23.296082</td>\n",
       "      <td>7.089333</td>\n",
       "      <td>Enchugal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>794.75</td>\n",
       "      <td>445.00</td>\n",
       "      <td>946.75</td>\n",
       "      <td>534.00</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>698.25</td>\n",
       "      <td>819.50</td>\n",
       "      <td>1532.00</td>\n",
       "      <td>0.360797</td>\n",
       "      <td>2.177036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508479</td>\n",
       "      <td>0.489504</td>\n",
       "      <td>0.404964</td>\n",
       "      <td>0.103152</td>\n",
       "      <td>0.105903</td>\n",
       "      <td>97</td>\n",
       "      <td>63.716873</td>\n",
       "      <td>13.406000</td>\n",
       "      <td>Enchugal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>931.25</td>\n",
       "      <td>664.50</td>\n",
       "      <td>1210.50</td>\n",
       "      <td>884.75</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>1163.50</td>\n",
       "      <td>1338.00</td>\n",
       "      <td>2224.75</td>\n",
       "      <td>0.313487</td>\n",
       "      <td>1.940419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605472</td>\n",
       "      <td>0.394263</td>\n",
       "      <td>0.350643</td>\n",
       "      <td>0.254651</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>98</td>\n",
       "      <td>135.108655</td>\n",
       "      <td>72.286000</td>\n",
       "      <td>Enchugal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>882.50</td>\n",
       "      <td>599.50</td>\n",
       "      <td>1174.50</td>\n",
       "      <td>843.50</td>\n",
       "      <td>1389.5</td>\n",
       "      <td>1079.00</td>\n",
       "      <td>1335.00</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>2.424643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508871</td>\n",
       "      <td>0.488434</td>\n",
       "      <td>0.413577</td>\n",
       "      <td>0.095293</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>99</td>\n",
       "      <td>122.818091</td>\n",
       "      <td>62.814000</td>\n",
       "      <td>Enchugal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1023.50</td>\n",
       "      <td>569.00</td>\n",
       "      <td>1314.00</td>\n",
       "      <td>779.50</td>\n",
       "      <td>1422.0</td>\n",
       "      <td>1021.50</td>\n",
       "      <td>1272.00</td>\n",
       "      <td>2438.00</td>\n",
       "      <td>0.413979</td>\n",
       "      <td>2.412872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357063</td>\n",
       "      <td>0.650669</td>\n",
       "      <td>0.244212</td>\n",
       "      <td>0.112711</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>100</td>\n",
       "      <td>3.416785</td>\n",
       "      <td>0.459333</td>\n",
       "      <td>Enchugal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          b1      b2       b3      b4      b5       b6       b7       b8  \\\n",
       "0     548.50  707.00   850.00  926.50  1254.0  1325.00  1664.50  2842.00   \n",
       "1     541.00  662.00   784.50  844.50  1083.0   967.00  1356.50  2455.50   \n",
       "2     491.00  623.50   765.50  818.00  1034.5   903.50  1446.50  2762.50   \n",
       "3     607.00  689.00   840.50  885.00  1112.5   983.00  1362.50  2370.00   \n",
       "4     585.50  657.00   762.00  800.50   951.5   861.00  1086.50  1832.00   \n",
       "..       ...     ...      ...     ...     ...      ...      ...      ...   \n",
       "377   974.50  703.75  1257.25  930.25  1528.5  1228.00  1406.75  2492.50   \n",
       "378   794.75  445.00   946.75  534.00  1075.0   698.25   819.50  1532.00   \n",
       "379   931.25  664.50  1210.50  884.75  1473.0  1163.50  1338.00  2224.75   \n",
       "380   882.50  599.50  1174.50  843.50  1389.5  1079.00  1335.00  2600.00   \n",
       "381  1023.50  569.00  1314.00  779.50  1422.0  1021.50  1272.00  2438.00   \n",
       "\n",
       "         NDVI        SR  ...  b1_1 (Clay+Silt)  b2_1(Sand_Raster)  \\\n",
       "0    0.364127  2.145283  ...          0.850000           0.150000   \n",
       "1    0.439585  2.568796  ...          0.519413           0.473399   \n",
       "2    0.508677  3.070660  ...          0.654887           0.343329   \n",
       "3    0.416839  2.429589  ...          0.524789           0.470715   \n",
       "4    0.354755  2.100475  ...          0.517816           0.474641   \n",
       "..        ...       ...  ...               ...                ...   \n",
       "377  0.344621  2.075156  ...          0.692295           0.310875   \n",
       "378  0.360797  2.177036  ...          0.508479           0.489504   \n",
       "379  0.313487  1.940419  ...          0.605472           0.394263   \n",
       "380  0.413462  2.424643  ...          0.508871           0.488434   \n",
       "381  0.413979  2.412872  ...          0.357063           0.650669   \n",
       "\n",
       "     b3_1 (Silt Raster)  b4_1 (Clay Raster    b8_asm   Id         SAR  \\\n",
       "0              0.460000           0.390000  0.104167    1   26.926614   \n",
       "1              0.337389           0.181547  0.104167    2   86.327645   \n",
       "2              0.461192           0.192468  0.104167    3  112.754286   \n",
       "3              0.431571           0.093336  0.104167    4  193.130080   \n",
       "4              0.407218           0.110598  0.104167    5  126.060248   \n",
       "..                  ...                ...       ...  ...         ...   \n",
       "377            0.336182           0.356843  0.104167   96   23.296082   \n",
       "378            0.404964           0.103152  0.105903   97   63.716873   \n",
       "379            0.350643           0.254651  0.104167   98  135.108655   \n",
       "380            0.413577           0.095293  0.104167   99  122.818091   \n",
       "381            0.244212           0.112711  0.104167  100    3.416785   \n",
       "\n",
       "            CE  Tanbanca  Tabanca_Id  \n",
       "0     7.089333    Cafine           1  \n",
       "1    30.558000    Cafine           1  \n",
       "2    44.382000    Cafine           1  \n",
       "3    93.534000    Cafine           1  \n",
       "4    60.254000    Cafine           1  \n",
       "..         ...       ...         ...  \n",
       "377   7.089333  Enchugal           3  \n",
       "378  13.406000  Enchugal           3  \n",
       "379  72.286000  Enchugal           3  \n",
       "380  62.814000  Enchugal           3  \n",
       "381   0.459333  Enchugal           3  \n",
       "\n",
       "[382 rows x 115 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_root_dir = \"/content/drive/MyDrive/Datos/GEE\"\n",
    "dataframe = pd.read_excel('../DataIntermediate/All_data.xlsx')\n",
    "dataframe.head(382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RNDSI</th>\n",
       "      <th>YRNDSI</th>\n",
       "      <th>NDSI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>GNDVI</th>\n",
       "      <th>GCVI</th>\n",
       "      <th>GRVI</th>\n",
       "      <th>YRNDVI</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>RNDVI</th>\n",
       "      <th>b1_1 (Clay+Silt)</th>\n",
       "      <th>b2_1(Sand_Raster)</th>\n",
       "      <th>b3_1 (Silt Raster)</th>\n",
       "      <th>b4_1 (Clay Raster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.257474</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>-0.364127</td>\n",
       "      <td>-0.538803</td>\n",
       "      <td>0.538803</td>\n",
       "      <td>2.336546</td>\n",
       "      <td>3.336546</td>\n",
       "      <td>0.386275</td>\n",
       "      <td>0.364127</td>\n",
       "      <td>0.242722</td>\n",
       "      <td>0.257474</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.288211</td>\n",
       "      <td>-0.391789</td>\n",
       "      <td>-0.439585</td>\n",
       "      <td>-0.513050</td>\n",
       "      <td>0.513050</td>\n",
       "      <td>2.107235</td>\n",
       "      <td>3.107235</td>\n",
       "      <td>0.391789</td>\n",
       "      <td>0.439585</td>\n",
       "      <td>0.293014</td>\n",
       "      <td>0.288211</td>\n",
       "      <td>0.519413</td>\n",
       "      <td>0.473399</td>\n",
       "      <td>0.337389</td>\n",
       "      <td>0.181547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.314288</td>\n",
       "      <td>-0.453342</td>\n",
       "      <td>-0.508677</td>\n",
       "      <td>-0.562505</td>\n",
       "      <td>0.562505</td>\n",
       "      <td>2.571547</td>\n",
       "      <td>3.571547</td>\n",
       "      <td>0.453342</td>\n",
       "      <td>0.508677</td>\n",
       "      <td>0.339072</td>\n",
       "      <td>0.314288</td>\n",
       "      <td>0.654887</td>\n",
       "      <td>0.343329</td>\n",
       "      <td>0.461192</td>\n",
       "      <td>0.192468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.265498</td>\n",
       "      <td>-0.365170</td>\n",
       "      <td>-0.416839</td>\n",
       "      <td>-0.473273</td>\n",
       "      <td>0.473273</td>\n",
       "      <td>1.797036</td>\n",
       "      <td>2.797036</td>\n",
       "      <td>0.365170</td>\n",
       "      <td>0.416839</td>\n",
       "      <td>0.277851</td>\n",
       "      <td>0.265498</td>\n",
       "      <td>0.524789</td>\n",
       "      <td>0.470715</td>\n",
       "      <td>0.431571</td>\n",
       "      <td>0.093336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.261639</td>\n",
       "      <td>-0.318139</td>\n",
       "      <td>-0.354755</td>\n",
       "      <td>-0.402379</td>\n",
       "      <td>0.402379</td>\n",
       "      <td>1.346602</td>\n",
       "      <td>2.346602</td>\n",
       "      <td>0.318139</td>\n",
       "      <td>0.354755</td>\n",
       "      <td>0.236459</td>\n",
       "      <td>0.261639</td>\n",
       "      <td>0.517816</td>\n",
       "      <td>0.474641</td>\n",
       "      <td>0.407218</td>\n",
       "      <td>0.110598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RNDSI    YRNDSI      NDSI      NDWI     GNDVI      GCVI      GRVI  \\\n",
       "0 -0.257474 -0.386275 -0.364127 -0.538803  0.538803  2.336546  3.336546   \n",
       "1 -0.288211 -0.391789 -0.439585 -0.513050  0.513050  2.107235  3.107235   \n",
       "2 -0.314288 -0.453342 -0.508677 -0.562505  0.562505  2.571547  3.571547   \n",
       "3 -0.265498 -0.365170 -0.416839 -0.473273  0.473273  1.797036  2.797036   \n",
       "4 -0.261639 -0.318139 -0.354755 -0.402379  0.402379  1.346602  2.346602   \n",
       "\n",
       "     YRNDVI      NDVI      SAVI     RNDVI  b1_1 (Clay+Silt)  \\\n",
       "0  0.386275  0.364127  0.242722  0.257474          0.850000   \n",
       "1  0.391789  0.439585  0.293014  0.288211          0.519413   \n",
       "2  0.453342  0.508677  0.339072  0.314288          0.654887   \n",
       "3  0.365170  0.416839  0.277851  0.265498          0.524789   \n",
       "4  0.318139  0.354755  0.236459  0.261639          0.517816   \n",
       "\n",
       "   b2_1(Sand_Raster)  b3_1 (Silt Raster)  b4_1 (Clay Raster  \n",
       "0           0.150000            0.460000           0.390000  \n",
       "1           0.473399            0.337389           0.181547  \n",
       "2           0.343329            0.461192           0.192468  \n",
       "3           0.470715            0.431571           0.093336  \n",
       "4           0.474641            0.407218           0.110598  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear DataFrames X1 y y1\n",
    "X1 = pd.DataFrame(dataframe, columns=['RNDSI','YRNDSI','NDSI','NDWI','GNDVI', 'GCVI', 'GRVI' ,'YRNDVI' ,'NDVI', 'SAVI' ,'RNDVI', 'b1_1 (Clay+Silt)',\t'b2_1(Sand_Raster)',\t'b3_1 (Silt Raster)',\t'b4_1 (Clay Raster']).dropna() # 'b1_1','b2_1','b3_1', 'b4_1'\n",
    "y1 = pd.DataFrame(dataframe[[\"CE\"]]).dropna()\n",
    "\n",
    "\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (305, 15)\n",
      "X_test: (77, 15)\n",
      "y_train: (305, 1)\n",
      "y_test: (77, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state= 71) \n",
    "\n",
    "pd.DataFrame(y_train).head(200)\n",
    "# print(y_train)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the arquitecture, run and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "# List of features\n",
    "features = ['RNDSI', 'NDSI', 'NDWI','b3_1 (Silt Raster)']  # Add any other indices here 'NDSI','RNDVI','NDWI','b3_1'\n",
    "num_features = len(features)\n",
    "\n",
    "# DataFrames X1 and y1\n",
    "X1 = pd.DataFrame(dataframe, columns=features).dropna()\n",
    "y1 = pd.DataFrame(dataframe[[\"CE\"]]).dropna()  # or use \"CE\"\n",
    "\n",
    "# Model parameters\n",
    "input_size = num_features  # Number of features\n",
    "hidden_size = 78\n",
    "batch_size = 34\n",
    "num_epochs = 32\n",
    "learning_rate =  0.023702857142857144\n",
    "regularization_param = 1e-2\n",
    "dropout_p = 0.001\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20, random_state=65)  # Same parameters for consistency\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values.flatten(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values.flatten(), dtype=torch.float32)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_size, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size * (input_size - 2), hidden_size)  # Adjust according to the output size of conv1d\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.unsqueeze(1))  # Add channel dimension\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output of the convolution\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/32], Train Loss: 47.68468081364866, Test Loss: 43.2469292182427\n",
      "Train Pearson Correlation: 0.5403462320964536, Test Pearson Correlation: 0.5115060203593202\n",
      "Epoch [4/32], Train Loss: 43.19916553184634, Test Loss: 37.15765836641386\n",
      "Train Pearson Correlation: 0.5843668627890111, Test Pearson Correlation: 0.5873476349879638\n",
      "Epoch [6/32], Train Loss: 38.45671982061668, Test Loss: 38.50858926153802\n",
      "Train Pearson Correlation: 0.5960322535261184, Test Pearson Correlation: 0.5962344593863236\n",
      "Epoch [8/32], Train Loss: 35.51581352734175, Test Loss: 35.004418311181006\n",
      "Train Pearson Correlation: 0.6004040684912866, Test Pearson Correlation: 0.5840270717271778\n",
      "Epoch [10/32], Train Loss: 32.448569013251635, Test Loss: 32.50682568240475\n",
      "Train Pearson Correlation: 0.6199363926973831, Test Pearson Correlation: 0.576733451896871\n",
      "Epoch [12/32], Train Loss: 30.632518905889793, Test Loss: 33.63433134400999\n",
      "Train Pearson Correlation: 0.6316264063689232, Test Pearson Correlation: 0.6101743442350198\n",
      "Epoch [14/32], Train Loss: 29.48646963776135, Test Loss: 31.74358013698033\n",
      "Train Pearson Correlation: 0.6396427547201711, Test Pearson Correlation: 0.6079908006759069\n",
      "Epoch [16/32], Train Loss: 27.712620613223216, Test Loss: 30.19605639073756\n",
      "Train Pearson Correlation: 0.6210716262129949, Test Pearson Correlation: 0.6088828909669246\n",
      "Epoch [18/32], Train Loss: 27.422474489055695, Test Loss: 29.913769610516436\n",
      "Train Pearson Correlation: 0.6493522290506917, Test Pearson Correlation: 0.6302450066195888\n",
      "Epoch [20/32], Train Loss: 26.18127013659868, Test Loss: 29.212563180304194\n",
      "Train Pearson Correlation: 0.6617345614433816, Test Pearson Correlation: 0.6429653717750625\n",
      "Epoch [22/32], Train Loss: 26.173321045422163, Test Loss: 29.51499931533615\n",
      "Train Pearson Correlation: 0.6393620888868554, Test Pearson Correlation: 0.6339264999369334\n",
      "Epoch [24/32], Train Loss: 26.433136173936187, Test Loss: 28.80784122665207\n",
      "Train Pearson Correlation: 0.666209992607179, Test Pearson Correlation: 0.661808393557294\n",
      "Epoch [26/32], Train Loss: 25.90505434880491, Test Loss: 29.54598378515863\n",
      "Train Pearson Correlation: 0.6443164175761922, Test Pearson Correlation: 0.6523730217231787\n",
      "Epoch [28/32], Train Loss: 24.615072713132765, Test Loss: 28.171139952424284\n",
      "Train Pearson Correlation: 0.6697767579978517, Test Pearson Correlation: 0.6733741971656855\n",
      "Epoch [30/32], Train Loss: 25.020094174244363, Test Loss: 28.231843774968926\n",
      "Train Pearson Correlation: 0.6738903556511104, Test Pearson Correlation: 0.6946826262174\n",
      "Epoch [32/32], Train Loss: 25.148645125842485, Test Loss: 28.351516426383675\n",
      "Train Pearson Correlation: 0.675315133962954, Test Pearson Correlation: 0.681003199879342\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.SmoothL1Loss()  # Regression loss\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=regularization_param)\n",
    "\n",
    "# Training the model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_pearson_corrs = []\n",
    "test_pearson_corrs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.unsqueeze(1))  # Add output dimension\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dl:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(1))  # Add output dimension\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "    test_loss /= len(test_dl.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Calculate Pearson correlation for train and test sets\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor).squeeze().numpy()\n",
    "        train_labels = y_train_tensor.numpy()\n",
    "        train_pearson_corr, _ = pearsonr(train_labels, train_outputs)\n",
    "        train_pearson_corrs.append(train_pearson_corr)\n",
    "\n",
    "        test_outputs = model(X_test_tensor).squeeze().numpy()\n",
    "        test_labels = y_test_tensor.numpy()\n",
    "        test_pearson_corr, _ = pearsonr(test_labels, test_outputs)\n",
    "        test_pearson_corrs.append(test_pearson_corr)\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss}, Test Loss: {test_loss}')\n",
    "        print(f'Train Pearson Correlation: {train_pearson_corr}, Test Pearson Correlation: {test_pearson_corr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses and Pearson correlations\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(range(num_epochs), train_losses, label='Train Loss', color=color)\n",
    "ax1.plot(range(num_epochs), test_losses, label='Test Loss', color=color, linestyle='dashed')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Pearson Correlation', color=color)\n",
    "ax2.plot(range(num_epochs), train_pearson_corrs, label='Train Pearson Correlation', color=color)\n",
    "ax2.plot(range(num_epochs), test_pearson_corrs, label='Test Pearson Correlation', color=color, linestyle='dashed')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Loss and Pearson Correlation over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1                [-1, 78, 2]             312\n",
      "       BatchNorm1d-2                [-1, 78, 2]             156\n",
      "            Linear-3                   [-1, 78]          12,246\n",
      "       BatchNorm1d-4                   [-1, 78]             156\n",
      "            Linear-5                   [-1, 78]           6,162\n",
      "       BatchNorm1d-6                   [-1, 78]             156\n",
      "            Linear-7                   [-1, 78]           6,162\n",
      "       BatchNorm1d-8                   [-1, 78]             156\n",
      "           Dropout-9                   [-1, 78]               0\n",
      "           Linear-10                    [-1, 1]              79\n",
      "================================================================\n",
      "Total params: 25,585\n",
      "Trainable params: 25,585\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Obtener el summary del modelo\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(4,))  # input_size debe ser el tamaÃ±o de la entrada de tus datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../Results/model_CNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = \"../Results/model_CNN.pth\"\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# FunciÃ³n para calcular RPIQ\n",
    "def calculate_rpiq(observed, predicted):\n",
    "    Q1 = np.percentile(observed, 25)\n",
    "    Q3 = np.percentile(observed, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    SEP = np.sqrt(np.mean((observed - predicted) ** 2))  # Equivalent to RMSE\n",
    "    RPIQ = IQR / SEP\n",
    "    return RPIQ\n",
    "\n",
    "# FunciÃ³n para calcular weighted R2 (wR2)\n",
    "def calculate_wr2(observed, predicted):\n",
    "    r2 = r2_score(observed, predicted)\n",
    "    b, _ = np.polyfit(observed, predicted, 1)  # Slope of the regression line\n",
    "    if b <= 1:\n",
    "        wr2 = abs(b) * r2\n",
    "    else:\n",
    "        wr2 = (1 / abs(b)) * r2\n",
    "    return wr2\n",
    "\n",
    "# FunciÃ³n para obtener las predicciones del modelo\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.tolist())\n",
    "    return predictions\n",
    "\n",
    "# Obtener las predicciones del modelo en el conjunto de prueba\n",
    "y_pred = get_predictions(model, test_dl)\n",
    "\n",
    "# Convertir los datos a numpy arrays\n",
    "y_test_numpy = y_test_tensor.numpy().flatten()\n",
    "y_pred_numpy = np.array(y_pred).flatten()\n",
    "\n",
    "# Calcular mÃ©tricas\n",
    "r2 = r2_score(y_test_numpy, y_pred_numpy)\n",
    "mse = mean_squared_error(y_test_numpy, y_pred_numpy)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_numpy, y_pred_numpy)\n",
    "bias = np.mean(y_pred_numpy - y_test_numpy)\n",
    "pbias = 100 * np.sum(y_pred_numpy - y_test_numpy) / np.sum(y_test_numpy)\n",
    "pearson_corr, _ = pearsonr(y_test_numpy, y_pred_numpy)\n",
    "nrmse = rmse / np.mean(y_test_numpy)\n",
    "rpiq = calculate_rpiq(y_test_numpy, y_pred_numpy)\n",
    "wr2 = calculate_wr2(y_test_numpy, y_pred_numpy)\n",
    "\n",
    "# Plotting the scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test_numpy, y_pred_numpy)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title(\"Actual vs Predicted\")\n",
    "\n",
    "# Adding the regression line\n",
    "sns.regplot(x=y_test_numpy, y=y_pred_numpy, scatter=False, ax=ax, color='red')\n",
    "\n",
    "# Adding the metrics as annotations\n",
    "ax.annotate(f'R2 Score: {r2:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'MSE: {mse:.2f}', xy=(0.05, 0.90), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'RMSE: {rmse:.2f}', xy=(0.05, 0.85), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'MAE: {mae:.2f}', xy=(0.05, 0.80), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'Bias: {bias:.2f}', xy=(0.05, 0.75), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'PBIAS: {pbias:.2f}%', xy=(0.05, 0.70), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'Pearson Corr: {pearson_corr:.2f}', xy=(0.05, 0.65), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'NRMSE: {nrmse:.2f}', xy=(0.05, 0.60), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'RPIQ: {rpiq:.2f}', xy=(0.05, 0.55), xycoords='axes fraction', ha='left', va='top')\n",
    "ax.annotate(f'wR2: {wr2:.2f}', xy=(0.05, 0.50), xycoords='axes fraction', ha='left', va='top')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define your dataset\n",
    "X_t = pd.DataFrame(dataframe, columns=features).dropna()\n",
    "y_t = pd.DataFrame(dataframe[[\"CE\"]]).dropna()  # or use \"CE\"\n",
    "\n",
    "# Scale features X_t\n",
    "X_t_scaled = StandardScaler().fit_transform(X_t)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X1_tensor = torch.tensor(X_t_scaled, dtype=torch.float32)\n",
    "y1_tensor = torch.tensor(y_t.values.flatten(), dtype=torch.float32)  # Convert to a 1D tensor\n",
    "\n",
    "# Create a DataLoader for the X1 and y1 data\n",
    "data_tensor = TensorDataset(X1_tensor, y1_tensor)\n",
    "data_loader = DataLoader(data_tensor, batch_size=32)\n",
    "\n",
    "# Define a function to get predictions from the model\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.numpy().flatten())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAADRkElEQVR4nOydd1gUVxeH36V3EAQBRUWwxq6xR7Fjiy323qJGjCXxM5rYYzexRGMXe4mxRI0lNqyosRDFgooFC9iQLn2+PyZsWJay9AXv+zz76N65c+cMbX577ikKSZIkBAKBQCAQCAopOvltgEAgEAgEAkFuIsSOQCAQCASCQo0QOwKBQCAQCAo1QuwIBAKBQCAo1AixIxAIBAKBoFAjxI5AIBAIBIJCjRA7AoFAIBAICjVC7AgEAoFAICjUCLEjEAgEAoGgUCPEjkDwEaNQKJg+fXp+m5HvuLm54ebmpnz/5MkTFAoFGzduzDebUpLSxvSIiIjAzs6Obdu25a5R2eDdu3eYmppy+PDh/DZF8BEgxI5AkEP8+uuvKBQK6tatm+U1Xr58yfTp0/Hx8ck5w7QcLy8vFAqF8qWvr0+ZMmXo378/jx49ym/zMsXFixeZPn06ISEh+WrH0qVLMTc3p2fPnmrHfHx86Nu3L05OThgaGmJtbU2LFi3w9PQkISFBOS/p+/HTTz+prbFx40YUCgVXr15Vjk2fPh2FQkGxYsWIiopSO6d06dK0b99e+d7GxoahQ4cyZcqU7N6uQJAhQuwIBDnEtm3bKF26NFeuXOHhw4dZWuPly5fMmDHjoxI7SXz99dds2bKFNWvW0K5dO3bt2sWnn37Ky5cv89yWUqVK8eHDB/r165ep8y5evMiMGTPyVezExcWxdOlShg4diq6ursqxdevWUbt2bU6fPk2fPn349ddfmTp1KsbGxgwZMoT58+errbdw4cJUxUtavH79mpUrV2o0d8SIEVy/fp1Tp05pvL5AkBWE2BEIcoDHjx9z8eJFfv75Z2xtbbV6+0Bb+eyzz+jbty+DBg3il19+YdGiRQQHB7Np06Y0z4mMjMwVWxQKBUZGRmpioSBw6NAh3rx5Q/fu3VXGL126xIgRI6hfvz737t1j3rx5DBkyhLFjx3Lw4EGuXLmCo6OjyjnVq1fn1atXrFq1SuPrV69enYULF/Lhw4cM51asWJHKlStr1XahoHAixI5AkANs27aNIkWK0K5dO7744os0xU5ISAjjxo2jdOnSGBoaUqJECfr378/bt2/x8vLi008/BWDQoEHKbYSkB0Hp0qUZOHCg2popYzliY2OZOnUqtWrVwtLSElNTUz777DNOnz6d6ft69eoVenp6zJgxQ+2Yn58fCoWC5cuXA7JHYcaMGZQtWxYjIyNsbGxo1KgRx48fz/R1AZo1awbIQhL+2ya5c+cOvXv3pkiRIjRq1Eg5f+vWrdSqVQtjY2Osra3p2bMnz549U1t3zZo1uLi4YGxsTJ06dTh37pzanLRidu7du0f37t2xtbXF2NiY8uXL8/333yvtmzBhAgDOzs7K79+TJ09yxca02L9/P6VLl8bFxUVlfMaMGSgUCrZt24a5ubnaebVr11b7+WrYsCHNmjVjwYIFGokXgKlTp/Lq1SuNvTstW7bk4MGDSJKk0XyBICsIsSMQ5ADbtm2jS5cuGBgY0KtXLx48eMDff/+tMiciIoLPPvuMX375hVatWrF06VJGjBjBvXv3eP78ORUrVmTmzJkAfPnll2zZsoUtW7bQuHHjTNkSFhbGunXrcHNzY/78+UyfPp03b97QunXrTG+PFStWjCZNmvDbb7+pHdu1axe6urp069YNkB/2M2bMoGnTpixfvpzvv/+ekiVLcv369UxdMwl/f39Aju1ITrdu3YiKimLOnDkMGzYMgNmzZ9O/f3/Kli3Lzz//zNixYzl58iSNGzdW2VJav349w4cPx97engULFtCwYUM+//zzVAVHSm7evEndunU5deoUw4YNY+nSpXTq1ImDBw8C0KVLF3r16gXA4sWLld8/W1vbPLMR5K20mjVrqoxFRUUpr1WyZEmN1kli+vTpmRIvn332WaYEUq1atQgJCeH27duZsksgyBSSQCDIFlevXpUA6fjx45IkSVJiYqJUokQJacyYMSrzpk6dKgHS3r171dZITEyUJEmS/v77bwmQPD091eaUKlVKGjBggNp4kyZNpCZNmijfx8fHSzExMSpz3r9/LxUrVkwaPHiwyjggTZs2Ld37W716tQRIt27dUhmvVKmS1KxZM+X7atWqSe3atUt3rdQ4ffq0BEgbNmyQ3rx5I718+VL6888/pdKlS0sKhUL6+++/JUmSpGnTpkmA1KtXL5Xznzx5Iunq6kqzZ89WGb9165akp6enHI+NjZXs7Oyk6tWrq3x91qxZIwEqX8PHjx+rfR8aN24smZubS0+fPlW5TtL3TpIkaeHChRIgPX78ONdtTI24uDhJoVBI33zzjcr4P//8IwFqP5PpAUijRo2SJEmSmjZtKtnb20tRUVGSJEmSp6enBCi/N5L03/fnzZs30pkzZyRA+vnnn5XHS5UqlerPx8WLFyVA2rVrl8a2CQSZRXh2BIJssm3bNooVK0bTpk0BOd6jR48e7Ny5UyW7Zc+ePVSrVo3OnTurraFQKHLMHl1dXQwMDABITEwkODiY+Ph4ateunSUvS5cuXdDT02PXrl3KMV9fX+7cuUOPHj2UY1ZWVty+fZsHDx5kye7Bgwdja2uLo6Mj7dq1IzIykk2bNlG7dm2VeSNGjFB5v3fvXhITE+nevTtv375Vvuzt7Slbtqxy++7q1au8fv2aESNGKL8+AAMHDsTS0jJd2968ecPZs2cZPHiwmmdEk+9dXtgIEBwcjCRJFClSRGU8LCwMINXtK02YPn06QUFBGsfuNG7cmKZNm2rk3Umy9e3bt1myTSDQBCF2BIJskJCQwM6dO2natCmPHz/m4cOHPHz4kLp16/Lq1StOnjypnOvv70/lypXzxK5NmzZRtWpVZeyMra0tf/75J6GhoZleq2jRojRv3lxlK2vXrl3o6enRpUsX5djMmTMJCQmhXLlyVKlShQkTJnDz5k2NrzN16lSOHz/OqVOnuHnzJi9fvkw1G8rZ2Vnl/YMHD5AkibJly2Jra6vyunv3Lq9fvwbg6dOnAJQtW1bl/KRU9/RISoHP6vcvL2xMjpQi/sXCwgKA8PDwLNmfGfGShKYCKcnWnBT8AkFK9PLbAIGgIHPq1CkCAwPZuXMnO3fuVDu+bds2WrVqlSPXSuthkJCQoJI1tHXrVgYOHEinTp2YMGECdnZ26OrqMnfuXGUcTGbp2bMngwYNwsfHh+rVq/Pbb7/RvHlzihYtqpzTuHFj/P39+eOPP/jrr79Yt24dixcvZtWqVQwdOjTDa1SpUoUWLVpkOM/Y2FjlfWJiIgqFgiNHjqSaPWVmZqbBHeYueWWjtbU1CoWC9+/fq4y7urqip6fHrVu3srz2tGnTcHNzY/Xq1VhZWWU4v3Hjxri5ubFgwQI1b1xykmxN/rMkEOQ0QuwIBNlg27Zt2NnZsWLFCrVje/fuZd++faxatQpjY2NcXFzw9fVNd730Pt0WKVIk1fotT58+VfnU//vvv1OmTBn27t2rst60adM0uKPU6dSpE8OHD1duZd2/f59JkyapzbO2tmbQoEEMGjSIiIgIGjduzPTp0zUSO1nFxcUFSZJwdnamXLlyac4rVaoUIHtZkjK9QM4ie/z4MdWqVUvz3KSvb1a/f3lhI4Cenh4uLi7KDLYkTExMaNasGadOneLZs2c4OTmlu05qNGnSRBn0PnXqVI3OmT59ulIgpUWSrRUrVsy0TQKBpohtLIEgi3z48IG9e/fSvn17vvjiC7WXh4cH4eHhHDhwAICuXbvyzz//sG/fPrW1klz5pqamAKmKGhcXFy5dukRsbKxy7NChQ2pZOkmeg+RbGZcvX8bb2zvL92plZUXr1q357bff2LlzJwYGBnTq1Ellzrt371Tem5mZ4erqSkxMTJavqwldunRBV1eXGTNmqG3fSJKktKt27drY2tqyatUqla/hxo0bMywCaGtrS+PGjdmwYQMBAQFq10gire9fXtiYRP369VUqGycxbdo0JEmiX79+REREqB2/du1aujWN4L+tqTVr1mhkS3KBFB0dneqca9euYWlpySeffKLRmgJBVhCeHYEgixw4cIDw8HA+//zzVI/Xq1dPWWCwR48eTJgwgd9//51u3boxePBgatWqRXBwMAcOHGDVqlVUq1YNFxcXrKysWLVqFebm5piamlK3bl2cnZ0ZOnQov//+O+7u7nTv3h1/f3+2bt2qVk+lffv27N27l86dO9OuXTseP37MqlWrqFSpUqoPOU3p0aMHffv25ddff6V169ZqWxmVKlXCzc2NWrVqYW1tzdWrV/n999/x8PDI8jU1wcXFhR9//JFJkybx5MkTOnXqhLm5OY8fP2bfvn18+eWXfPvtt+jr6/Pjjz8yfPhwmjVrRo8ePXj8+DGenp4axcMsW7aMRo0aUbNmTb788kucnZ158uQJf/75pzKlv1atWgB8//339OzZE319fTp06JBnNgJ07NiRLVu2cP/+fRUvUoMGDVixYgVfffUVFSpUoF+/fpQtW5bw8HC8vLw4cOAAP/74Y7prN2nShCZNmnDmzBmNbAFZZCUF76fG8ePH6dChg4jZEeQu+ZABJhAUCjp06CAZGRlJkZGRac4ZOHCgpK+vL719+1aSJEl69+6d5OHhIRUvXlwyMDCQSpQoIQ0YMEB5XJIk6Y8//pAqVaok6enpqaU///TTT1Lx4sUlQ0NDqWHDhtLVq1fVUs8TExOlOXPmSKVKlZIMDQ2lGjVqSIcOHZIGDBgglSpVSsU+NEg9TyIsLEwyNjaWAGnr1q1qx3/88UepTp06kpWVlWRsbCxVqFBBmj17thQbG5vuukmp57t37053XvLU5tTYs2eP1KhRI8nU1FQyNTWVKlSoII0aNUry8/NTmffrr79Kzs7OkqGhoVS7dm3p7Nmzal/D1FLPJUmSfH19pc6dO0tWVlaSkZGRVL58eWnKlCkqc2bNmiUVL15c0tHRUUtDz0kb0yImJkYqWrSoNGvWrFSPX7t2Terdu7fk6Ogo6evrS0WKFJGaN28ubdq0SUpISFDOI1nqeXKSvl+kk3qekiZNmkiAWur53bt3JUA6ceJEhvclEGQHhSSJspUCgUBQmJg1axaenp48ePBAq1tejB07lrNnz3Lt2jXh2RHkKiJmRyAQCAoZ48aNIyIiItUMQW3h3bt3rFu3jh9//FEIHUGuIzw7AoFAIBAICjXCsyMQCAQCgaBQI8SOQCAQCASCQo0QOwKBQCAQCAo1+Sp25s6dy6effoq5uTl2dnZ06tQJPz8/lTnR0dGMGjUKGxsbzMzM6Nq1K69evVKZExAQQLt27TAxMcHOzo4JEyYQHx+fl7ciEAgEAoFAS8nXooJnzpxh1KhRfPrpp8THxzN58mRatWrFnTt3lJVIx40bx59//snu3buxtLTEw8ODLl26cOHCBUDuC9SuXTvs7e25ePEigYGB9O/fH319febMmaORHYmJibx8+RJzc3ORFSAQCAQCQQFBkiTCw8NxdHRERycd/02+VvlJwevXryVAOnPmjCRJkhQSEiLp6+urFBtLKkLl7e0tSZIkHT58WNLR0ZGCgoKUc1auXClZWFhIMTExGl332bNnyiJZ4iVe4iVe4iVe4lWwXs+ePUv3Oa9V7SJCQ0MBuZkgyD1T4uLiVDohV6hQgZIlS+Lt7U29evXw9vamSpUqFCtWTDmndevWjBw5ktu3b1OjRo0Mr2tubg7As2fPsLCwyMlbEggEAoFAkEuEhYXh5OSkfI6nhdaIncTERMaOHUvDhg2pXLkyAEFBQRgYGKj14ClWrBhBQUHKOcmFTtLxpGOpERMTo9KcMDw8HAALCwshdgQCgUAgKGBkFIKiNdlYo0aNwtfXN08qfs6dOxdLS0vly8nJKdevKRAIBAKBIH/QCrHj4eHBoUOHOH36NCVKlFCO29vbExsbS0hIiMr8V69eYW9vr5yTMjsr6X3SnJRMmjSJ0NBQ5evZs2c5eDcCgUAgEAi0iXwVO5Ik4eHhwb59+zh16hTOzs4qx2vVqoW+vj4nT55Ujvn5+REQEED9+vUBqF+/Prdu3eL169fKOcePH8fCwoJKlSqlel1DQ0PllpXYuhIIBAKBoHCTrzE7o0aNYvv27fzxxx+Ym5srY2wsLS0xNjbG0tKSIUOGMH78eKytrbGwsGD06NHUr1+fevXqAdCqVSsqVapEv379WLBgAUFBQfzwww+MGjUKQ0PD/Lw9gUAgEAgEWkC+NgJNK6DI09OTgQMHAnJRwW+++YYdO3YQExND69at+fXXX1W2qJ4+fcrIkSPx8vLC1NSUAQMGMG/ePPT0NNNyYWFhWFpaEhoaKrw8AoFAIBAUEDR9fouu5wixIxAIBAJBQUTT57dWBCgLBAKBQCAQ5BZC7AgEAoFAICjUCLEjEAgEAoGgUKM1FZQFAoFAoD0kJEpceRzM6/Bo7MyNqONsja6OaJQsKJgIsSMQCAQCFY76BjLj4B0CQ6OVYw6WRkzrUAn3yg75aJlAkDXENpZAIBAIlBz1DWTk1usqQgcgKDSakVuvc9Q3MJ8sEwiyjhA7AoFAIADkrasZB++QWj2SpLEZB++QkPjRVywRFDCE2BEIBAIBAFceB6t5dJIjAYGh0Vx5HJx3RgkEOYAQOwKBQCAA4HV42kInK/MEAm1BiB2BQCAQAGBnbpSj8wQCbUGIHYFAIBAAUMfZGgdLI9JKMFcgZ2XVcbbOS7MEgmwjxE4eMnDgQBQKBQqFAn19fZydnfnf//5HdPR/LuEnT54wZMgQnJ2dMTY2xsXFhWnTphEbG5vu2v/88w+ff/45dnZ2GBkZUbp0aXr06MHr169z+7ZyhODgYPr06YOFhQVWVlYMGTKEiIgIjc6VJIk2bdqgUCjYv3+/yrGAgADatWuHiYkJdnZ2TJgwgfj4+Fy4A4Gg4KOro2Bah0oAaoIn6f20DpVEvR1BgUPU2clj3N3d8fT0JC4ujmvXrjFgwAAUCgXz588H4N69eyQmJrJ69WpcXV3x9fVl2LBhREZGsmjRolTXfPPmDc2bN6d9+/YcO3YMKysrnjx5woEDB4iMjMy1e4mLi0NfXz9H1urTpw+BgYEcP36cuLg4Bg0axJdffsn27dszPHfJkiUoFOp/fBMSEmjXrh329vZcvHiRwMBA+vfvj76+PnPmzMkRuwWCwoZ7ZQdW9q2pVmfHXtTZERRkJIEUGhoqAVJoaGiuXmfAgAFSx44dVca6dOki1ahRI93zFixYIDk7O6d5fN++fZKenp4UFxeX7jq+vr5Su3btJHNzc8nMzExq1KiR9PDhQ0mSJCkhIUGaMWOGVLx4ccnAwECqVq2adOTIEeW5jx8/lgBp586dUuPGjSVDQ0PJ09NTkiRJWrt2rVShQgXJ0NBQKl++vLRixYp07UjJnTt3JED6+++/lWNHjhyRFAqF9OLFi3TPvXHjhlS8eHEpMDBQAqR9+/Ypjx0+fFjS0dGRgoKClGMrV66ULCwspJiYmEzZKBB8bMQnJEoXH76V9t94Ll18+FaKT0jMb5MEAjU0fX6Lbax8xNfXl4sXL2JgYJDuvNDQUKyt094jt7e3Jz4+nn379iFJqde/ePHiBY0bN8bQ0JBTp05x7do1Bg8erNzSWbp0KT/99BOLFi3i5s2btG7dms8//5wHDx6orPPdd98xZswY7t69S+vWrdm2bRtTp05l9uzZ3L17lzlz5jBlyhQ2bdqkPMfNzY2BAwemab+3tzdWVlbUrl1bOdaiRQt0dHS4fPlymudFRUXRu3dvVqxYgb29farrVqlShWLFiinHWrduTVhYGLdv305zXYFAIG9p1XexoWP14tR3sRFbV4ICjdjGymMOHTqEmZkZ8fHxxMTEoKOjw/Lly9Oc//DhQ3755Zc0t7AA6tWrx+TJk+nduzcjRoygTp06NGvWjP79+ysf9CtWrMDS0pKdO3cqt57KlSunXGPRokVMnDiRnj17AjB//nxOnz7NkiVLWLFihXLe2LFj6dKli/L9tGnT+Omnn5Rjzs7O3Llzh9WrVzNgwAAASpYsiYND2q7voKAg7OzsVMb09PSwtrYmKCgozfPGjRtHgwYN6NixY5rrJhc6gPJ9eusKBAKBoHAhxE4e07RpU1auXElkZCSLFy9GT0+Prl27pjr3xYsXuLu7061bN4YNG5buurNnz2b8+PGcOnWKy5cvs2rVKubMmcPZs2epUqUKPj4+fPbZZ6nG2ISFhfHy5UsaNmyoMt6wYUP++ecflbHk3pfIyEj8/f0ZMmSIin3x8fFYWloq32/evDld27PCgQMHOHXqFDdu3MjxtQUCgUBQuBDbWHmMqakprq6uVKtWjQ0bNnD58mXWr1+vNu/ly5c0bdqUBg0asGbNGo3WtrGxoVu3bixatIi7d+/i6Oio9AgZGxvnmP1JJGVLrV27Fh8fH+XL19eXS5cuabymvb29WtZYfHw8wcHBqW5PAZw6dQp/f3+srKzQ09NDT0/W7V27dsXNzU257qtXr1TOS3qf1roCgUAgKHwIsZOP6OjoMHnyZH744Qc+fPigHH/x4gVubm7UqlULT09PdHQy/20yMDDAxcVFmY1VtWpVzp07R1xcnNpcCwsLHB0duXDhgsr4hQsXqFSpUprXKFasGI6Ojjx69AhXV1eVl7Ozs8a21q9fn5CQEK5du6YcO3XqFImJidStWzfVc7777jtu3rypIrIAFi9ejKenp3LdW7duqQip48ePY2Fhke59CQQCgaCQkTfx0tpNfmZjxcXFScWLF5cWLlwoSZIkPX/+XHJ1dZWaN28uPX/+XAoMDFS+0uLgwYNSnz59pIMHD0p+fn7SvXv3pIULF0q6urrS5s2bJUmSpLdv30o2NjZSly5dpL///lu6f/++tHnzZunevXuSJEnS4sWLJQsLC2nnzp3SvXv3pIkTJ0r6+vrS/fv3JUn6Lxvrxo0bKtdeu3atZGxsLC1dulTy8/OTbt68KW3YsEH66aeflHP69esnfffdd+l+bdzd3aUaNWpIly9fls6fPy+VLVtW6tWrl/L48+fPpfLly0uXL19Ocw1SZGPFx8dLlStXllq1aiX5+PhIR48elWxtbaVJkyala4tAIBAICgaaPr9FzE4+o6enh4eHBwsWLGDkyJEcP36chw8f8vDhQ0qUKKEyV0oj06pSpUqYmJjwzTff8OzZMwwNDSlbtizr1q2jX79+gLzFderUKSZMmECTJk3Q1dWlevXqyjidr7/+mtDQUL755htev35NpUqVOHDgAGXLlk3X/qFDh2JiYsLChQuZMGECpqamVKlShbFjxyrnBAQEZOid2rZtGx4eHjRv3hwdHR26du3KsmXLlMfj4uLw8/MjKioq3XWSo6ury6FDhxg5ciT169fH1NSUAQMGMHPmTI3XEAgEAkHBRyGl9QT9iAgLC8PS0pLQ0FAsLCzy2xyBQCAQCAQaoOnzW8TsCAQCgUAgKNQIsSMQCAQCgaBQI8SOQCAQCASCQo0QOwKBQCAQCAo1QuwIBAKBQCAo1AixIxAIBAKBIPeQJPmVjwixIxAIBAKBIHeIioInTyAhIV/NEGInDxk4cCAKhYIRI0aoHRs1ahQKhYKBAwcqx968ecPIkSMpWbIkhoaG2Nvb07p1a5W2DqVLl0ahUKi95s2blynbvv76a2rVqoWhoSHVq1dPdc5vv/1G9erVMTExoVSpUixcuFBtTkxMDN9//z2lSpXC0NCQ0qVLs2HDBuXxjRs3qtlqZGSUoX0rVqygYsWKGBsbU758+VSbi4aEhDBq1CgcHBwwNDSkXLlyHD58WPMvgkAgEAhyhsREePUKnj+HVNoU5TWignIe4+TkxM6dO1m8eLGyOWd0dDTbt2+nZMmSKnO7du1KbGwsmzZtokyZMrx69YqTJ0/y7t07lXkzZ85U64pubm6eadsGDx7M5cuXuXnzptqxI0eO0KdPH3755RdatWrF3bt3GTZsGMbGxnh4eCjnde/enVevXrF+/XpcXV0JDAwkMTFRZS0LCwv8/PyU7xUKRbp2rVy5kkmTJrF27Vo+/fRTrly5wrBhwyhSpAgdOnQAIDY2lpYtW2JnZ8fvv/9O8eLFefr0KVZWVpn+OggEAoEgG0REwOvXEB+f35YoEWInj6lZsyb+/v7s3buXPn36ALB3715Kliyp0jwzJCSEc+fO4eXlRZMmTQAoVaoUderUUVvT3Nw82128k1ozvHnzJlWxs2XLFjp16qT0SpUpU4ZJkyYxf/58pVfq6NGjnDlzhkePHmFtbQ3InqeUKBSKTNm7ZcsWhg8fTo8ePZTX/vvvv5k/f75S7GzYsIHg4GAuXryIvr5+mtcWCAQCQS4RHy+LnIiI/LZEDbGNlQ8MHjxY2Zkb5Af1oEGDVOaYmZlhZmbG/v37iYmJydb1SpcuzfTp07O1RkxMjNp2k7GxMc+fP+fp06cAHDhwgNq1a7NgwQKKFy9OuXLl+Pbbb1U6ugNERERQqlQpnJyc6NixI7dv387w2gaGhnj7v+MPnxd4+7/D0MiIK1euKLu4HzhwgPr16zNq1CiKFStG5cqVmTNnDgn5vE8sEAgEHwWhoXJsjhYKHchnsXP27Fk6dOiAo6MjCoWC/fv3qxxPLRZFoVCoxIqkFrOS2XiVvKZv376cP3+ep0+f8vTpUy5cuEDfvn1V5ujp6bFx40Y2bdqElZUVDRs2ZPLkyal6XSZOnKgUR0mvc+fOKY+7uLhQtGjRbNncunVr9u7dy8mTJ0lMTOT+/fv89NNPAAQGBgLw6NEjzp8/j6+vL/v27WPJkiX8/vvvfPXVV8p1ypcvz4YNG/jjjz/YunUriYmJNGjQgOfPn6d57bI1G7Jw2Uq6zNrK1ztu0GXmFhYtW0lcXBxv375VXvv3338nISGBw4cPM2XKFH766Sd+/PHHbN23QCAQCNIhNhaePZPjc1KELGgT+bqNFRkZSbVq1Rg8eDBdunRRO570EE3iyJEjDBkyhK5du6qMp4xZyUq8Sl5ia2tLu3bt2LhxI5Ik0a5du1TFSNeuXWnXrh3nzp3j0qVLHDlyhAULFrBu3TqVQOYJEyaovAcoXry48v8nT57Mts3Dhg3D39+f9u3bExcXh4WFBWPGjGH69OnKjuaJiYkoFAq2bduGpaUlAD///DNffPEFv/76K8bGxtSvX5/69esr123QoAEVK1Zk9erVzJo1S+26R30D8bZww6D0XYK2fAOShK6pFaafNCPu8h68/N7Qy8GBxMRE7OzsWLNmDbq6utSqVYsXL16wcOFCpk2blu37FwgEAkEyJAnev4d37/I9rVwT8lXstGnThjZt2qR5PGVcxx9//EHTpk0pU6aMynhOxKzkNYMHD1YG9q5YsSLNeUZGRrRs2ZKWLVsyZcoUhg4dyrRp01TETdGiRXF1dc1VexUKBfPnz2fOnDkEBQVha2urFFFJ3w8HBweKFy+uFDoAFStWRJIknj9/TtmyZdXW1dfXp0aNGjx8+FDtWEKixIyDd1DoG1K07VhsWnuQEBmCrlkRIv45hsLAmKUXXtG9sYSDgwP6+vro6uqqXDsoKIjY2FgMDAxy+ksiEAgEHyfR0bInJ5shFnlJgYnZefXqFX/++SdDhgxROzZv3jxsbGyoUaMGCxcuJD6DCPCYmBjCwsJUXnmNu7s7sbGxxMXF0bp1a43Pq1SpEpGRkbloWfro6upSvHhxDAwM2LFjB/Xr18fW1haAhg0b8vLlSyKS7dnev38fHR0dSpQokep6CQkJ3Lp1CwcHB7VjVx4HExgarXyv0NVDz6IoCh1dIu+exdilDkHhsVx5HEzDhg15+PChSubX/fv3cXBwEEJHIBAIcoLERDkAOSCgQAkdKEDZWJs2bcLc3Fxtu+vrr7+mZs2aWFtbc/HiRSZNmkRgYCA///xzmmvNnTuXGTNm5LbJ6aKrq8vdu3eV/0/Ju3fv6NatG4MHD6Zq1aqYm5tz9epVFixYQMeOHVXmhoeHExQUpDJmYmKChYUFAM2bN6dz584qKeIpefjwIREREQQFBfHhwwd8fHwAWVwZGBjw9u1bfv/9d9zc3IiOjsbT05Pdu3dz5swZ5Rq9e/dm1qxZDBo0iBkzZvD27VsmTJjA4MGDlWn2M2fOpF69eri6uhISEsLChQt5+vQpQ4cOVa4zadIkXrx4QdfxcwGIC35BTOB9DB3KkRgdQdjf+4l785Si7cYB8Do8mpEjR7J8+XLGjBnD6NGjefDgAXPmzOHrr7/W6PshEAgEgnSIjJS9OVqUTp4ZCozY2bBhA3369FHLCBo/frzy/1WrVsXAwIDhw4czd+5cDA0NU11r0qRJKueFhYXh5OSUO4anQ5IYSQ0zMzPq1q3L4sWL8ff3Jy4uDicnJ4YNG8bkyZNV5k6dOpWpU6eqjA0fPpxVq1YB4O/vrwzkTYuhQ4eqCJcaNWoA8PjxY2UK96ZNm/j222+RJIn69evj5eWlkgpvZmbG8ePHGT16NLVr18bGxobu3burBAm/f/+eYcOGERQURJEiRahVqxYXL16kUqVKyjmBgYEEBARgZ/7v9zoxkbAr+4gPfgE6uhiVqop934XoWRYDwM7cCCcnG44dO8a4ceOoWrUqxYsXZ8yYMUycODHd+xYIBAJBOiQkyN6c8PD8tiRbKCRJOyKLFAoF+/bto1OnTmrHzp07R+PGjfHx8aFatWrprnP79m0qV67MvXv3KF++vEbXDgsLw9LSktDQ0HQFiCBvSUiUaDT/FEGh0aT2Q6oA7C2NOD+xGbo66RcmFAgEAkEmCQuDN29yptVDmTKgl/P+FU2f3wUiZmf9+vXUqlUrQ6ED4OPjg46ODnZ2dnlgmSA30dVRMK2D7PFJKWWS3k/rUEkIHYFAIMhJ4uLkNg9BQfne0yqnyNdtrIiICJUsnMePH+Pj44O1tbWydUJYWBi7d+9W1nRJjre3N5cvX6Zp06aYm5vj7e3NuHHj6Nu3L0WKFMmz+xDkHu6VHVjZtyYzDt5RCVa2tzRiWodKuFdWD2wWCAQCQRYJDi4w6eSZIV+3sby8vGjatKna+IABA9i4cSMAa9asYezYsQQGBqqkNANcv36dr776inv37hETE4OzszP9+vVj/PjxacbrpIbYxtJ+EhIlrjwO5nV4NHbmRtRxthYeHYFAIMgpYmJkT05uZVnl8zaW1sTs5CdC7AgEAoHgo0SS4O1buUBgbpLPYqfAZGMJBAKBQCDIQaKi5HTyf3sMFmaE2BEIBAKB4GMiIUHOssqHgrr5hRA7AoFAIBB8LISHy3VzCkmWlaYIsSMQCAQCQWEnPl7essrHdkP5SYGos1NYGDhwIAqFAoVCgb6+Ps7Ozvzvf/8jOjpaZV7SnEuXLqmMx8TEYGNjg0KhwMvLSzl+5swZmjVrhrW1NSYmJpQtW5YBAwYQGxsLyFlvSWumfKVsM6EpO3fuRKFQpFoEMokRI0agUChYsmRJumutXLmSqlWrYmFhgYWFBfXr1+fIkSOpzpUkiTZt2qBQKNi/f3+WbBcIBIKPivfv4cmTj1bogBA7eY67uzuBgYE8evSIxYsXs3r1aqZNm6Y2z8nJCU9PT5Wxffv2YWZmpjJ2584d3N3dqV27NmfPnuXWrVv88ssvGBgYkJDCTenn50dgYKDKKyvFF588ecK3337LZ599luacffv2cenSJRwdHTNcr0SJEsybN49r165x9epVmjVrRseOHbl9+7ba3CVLlqBQiJRzgUAgyJCYGLlp55s3chPPjxghdvIYQ0ND7O3tcXJyolOnTrRo0YLjx4+rzRswYAA7d+7kw4cPyrENGzYwYMAAlXl//fUX9vb2LFiwgMqVK+Pi4oK7uztr165VNt9Mws7ODnt7e5WXjk7mfgQSEhLo06cPM2bMoEyZMqnOefHiBaNHj2bbtm3o6+tnuGaHDh1o27YtZcuWpVy5csyePRszMzM1z5aPjw8//fQTGzZsyJTNAoFA8FGRlE4eEAApdg4+VoTYyUd8fX25ePEiBgYGasdq1apF6dKl2bNnDwABAQGcPXuWfv36qcyzt7cnMDCQs2fPZsuWpK2uJ0+epDtv5syZ2NnZMWTIkFSPJyYm0q9fPyZMmMAnn3ySaTsSEhLYuXMnkZGR1K9fXzkeFRVF7969WbFiBfb29pleVyAQCD4KPnyAp0/lSsiijJ4SEaCcxxw6dAgzMzPi4+OJiYlBR0eH5cuXpzp38ODBbNiwgb59+7Jx40batm2Lra2typxu3bpx7NgxmjRpgr29PfXq1aN58+b0799frcBSiRIlVN6XKlVKuVVkYmJC+fLl0/XEnD9/nvXr1+Pj45PmnPnz56Onp8fXX3+d3pdBjVu3blG/fn2io6MxMzNj3759Kp3Qx40bR4MGDejYsWOm1hUIBIKPgsREebsqNDS/LdFKhNjJY5o2bcrKlSuJjIxk8eLF6Onp0bVr11Tn9u3bl++++45Hjx6xceNGli1bpjZHV1cXT09PfvzxR06dOsXly5eZM2cO8+fP58qVKzg4/Nc76ty5c5ibmyvfJxc2derU4d69e2naHR4eTr9+/Vi7di1FixZNdc61a9dYunQp169fz3RcTfny5fHx8SE0NJTff/+dAQMGcObMGSpVqsSBAwc4deoUN27cyNSaAoFA8FEQESGnk8fH57clWovYxspjTE1NcXV1pVq1amzYsIHLly+zfv36VOfa2NjQvn17hgwZQnR0NG3atElz3eLFi9OvXz+WL1/O7du3iY6OZtWqVSpznJ2dcXV1Vb5KlSqlsd3+/v48efKEDh06oKenh56eHps3b+bAgQPo6enh7+/PuXPneP36NSVLllTOefr0Kd988w2lS5dOd30DAwNcXV2pVasWc+fOpVq1aixduhSAU6dO4e/vj5WVlXJdgK5du+Lm5qbxPQgEAkGhIj4eXr6UX0LopIvw7OQjOjo6TJ48mfHjx9O7d2+1gGKQt7Latm3LxIkT0dXV1WjdIkWK4ODgQGQOphlWqFCBW7duqYz98MMPhIeHs3TpUpycnOjXrx8tWrRQmdO6dWv69evHoEGDMnW9xMREYv5tSPfdd98xdOhQleNVqlRh8eLFdOjQIQt3IxAIBAWc0FCRZZUJhNjJZ7p168aECRNYsWIF3377rdpxd3d33rx5k2aDs9WrV+Pj40Pnzp1xcXEhOjqazZs3c/v2bX755ReVua9fv1ar6WNjY4O+vj5Xrlyhf//+nDx5kuLFi6tdx8jIiMqVK6uMWVlZASjHbWxssLGxUZmjr6+Pvb095cuXV441b96czp074+HhAcCkSZNo06YNJUuWJDw8nO3bt+Pl5cWxY8cAlJljKSlZsiTOzs6pfl0EBRPR3V4gyIDYWLk4YLJMXUHGCLGTz+jp6eHh4cGCBQsYOXIkpqamKscVCkWaMTIgx9qcP3+eESNG8PLlS8zMzPjkk0/Yv38/TZo0UZmbXHAk4e3tTb169YiKisLPz4+4PGgI5+/vz9u3b5XvX79+Tf/+/QkMDMTS0pKqVaty7NgxWrZsmeu2CLSHo76BzDh4h8DQ/wS5g6UR0zpUwr2yQzpnCgQfAZIkZ1iJLKssoZAk8VXTtEW8QCDIHY76BjJy63VS/jFK8ums7FtTCB7Bx0t0NAQFyV6dgkqZMqCX8/4VTZ/fIkBZIBDkKwmJEjMO3lETOoBybMbBOyQkfvSfywQfG4mJcpZVQEDBFjpagBA7AoEgX7nyOFhl6yolEhAYGs2Vx8F5Z5RAkN9ERsr9rEJC8tuSQoGI2REIBPnK63DNytlrOk8gKNAkJMjenPDw/LakUCHEjkAgyFfszI1ydJ5AUGAJC5PTyVM0cRZkH7GNJRAI8pU6ztY4WBqRVoK5Ajkrq46zdV6aJRDkHXFx8Py5HIQshE6uIMSOQCDIV3R1FEzrIPdBSyl4kt5P61BJ1NsRFD6S0smfPIGoqPy2plAjxI5AIMh33Cs7sLJvTewtVbeq7C2NRNq5oHASHS1nWb19K+rm5AFC7OQhAwcORKFQMGLECLVjo0aNQqFQMHDgQLVj3t7e6Orq0q5dO7VjT548QaFQpPq6dOmSxrbt3buXli1bYmtri4WFBfXr11dWME5i+vTpateoUKFCuuvevn2brl27Urp0aRQKBUuWLEl13osXL+jbty82NjYYGxtTpUoVrl69qrH9goKPe2UHzk9sxo5h9Vjaszo7htXj/MRmQugICheSJMflBATAvy1xBLmPEDt5jJOTEzt37uRDslLf0dHRbN++nZIlS6Z6zvr16xk9ejRnz57l5cuXqc45ceIEgYGBKq9atWppbNfZs2dp2bIlhw8f5tq1azRt2pQOHTqodRr/5JNPVK5x/vz5dNeNioqiTJkyzJs3L9WWDwDv37+nYcOG6Ovrc+TIEe7cucNPP/1EkSJFNLZfUDjQ1VFQ38WGjtWLU9/FRmxdCQoXUVHyltX79/ltyUeHyMbKY2rWrIm/vz979+6lT58+gOxVSavPU0REBLt27eLq1asEBQWxceNGJk+erDbPxsYmTTGhCSk9LnPmzOGPP/7g4MGD1KhRQzmup6eXqet8+umnfPrpp4Dc0DM15s+fj5OTE56ensox0fNKIBAUGhISZG9OWFh+W/LRIjw7+cDgwYNVHuwbNmxIsyv4b7/9RoUKFShfvjx9+/Zlw4YNZLbDR9JWl5eXl8bnJCYmEh4ejrW1agbMgwcPcHR0pEyZMvTp04eAgIBM2ZIaBw4coHbt2nTr1g07Oztq1KjB2rVrs72uQCAQ5DthYbI3RwidfEWInXygb9++nD9/nqdPn/L06VMuXLhA3759U527fv165TF3d3dCQ0M5c+aM2rwGDRpgZmam8kpCX1+f8uXLY2JiorGNixYtIiIigu7duyvH6taty8aNGzl69CgrV67k8ePHfPbZZ4Rns/jVo0ePWLlyJWXLluXYsWOMHDmSr7/+mk2bNmVrXYFAIMg34uLgxQuRTq4liG2sfMDW1pZ27dqxceNGJEmiXbt2qXY29/Pz48qVK+zbtw+Qt5B69OjB+vXrcXNzU5m7a9cuKlasmOr1ihcvzr179zS2b/v27cyYMYM//vgDOzs75XibNm2U/69atSp169alVKlS/PbbbwwZMkTj9VOSmJhI7dq1mTNnDgA1atTA19eXVatWMWDAgCyvKxAIBPnC+/fw7p3c20qgFQixk08MHjwYDw8PAFasWJHqnPXr1xMfH4+jo6NyTJIkDA0NWb58OZaWlspxJycnXF1ds23Xzp07GTp0KLt376ZFixbpzrWysqJcuXI8fPgwW9d0cHCgUqVKKmMVK1Zkz5492VpXUPBJSJS48jiY1+HR2JnLhQVF0LJAa4mJgVev5LRygVYhxE4+4e7uTmxsLAqFgtatW6sdj4+PZ/Pmzfz000+0atVK5VinTp3YsWNHqins2WHHjh0MHjyYnTt3pprmnpKIiAj8/f3p169ftq7bsGFD/Pz8VMbu379PqVKlsrWuoGBz1DeQGQfvqDQJdbA0YlqHSiIdXaBdSJLsyXn/XtTMSY1nz+CXX2DRIjA0zBcThNjJJ3R1dbl7967y/yk5dOgQ79+/Z8iQISoeHICuXbuyfv16FbHz7t07goKCVOZZWVlhZGTEixcvaN68OZs3b6ZOnTqp2rN9+3YGDBjA0qVLqVu3rnItY2Nj5fW//fZbOnToQKlSpXj58iXTpk1DV1eXXr16Kdfp378/xYsXZ+7cuQDExsZy584d5f9fvHiBj48PZmZmSk/UuHHjaNCgAXPmzKF79+5cuXKFNWvWsGbNGs2/oIJCxVHfQEZuvU7Kx0ZQaDQjt14XhQYF2kNUlNy4MzY2vy3RPl69gpUrYfduiI+HsmXh66/zxRQRoJyPWFhYYGFhkeqx9evX06JFCzWhA7LYuXr1Kjdv3lSOtWjRAgcHB5XX/v37AYiLi8PPz4+odMqRr1mzhvj4eEaNGqWyxpgxY5Rznj9/Tq9evShfvjzdu3fHxsaGS5cuYWtrq5wTEBBAYGCg8v3Lly+pUaMGNWrUIDAwkEWLFlGjRg2GDh2qnPPpp5+yb98+duzYQeXKlZk1axZLlixRpuYLPi4SEiVmHLyjJnQA5diMg3dISBSfoAX5SGKi/DB//lwInZS8fw8LFkDLlrBjhyx0AGbPhoiIfDFJIWU2j7kQEhYWhqWlJaGhoWmKD4FAkDd4+7+j19qMq3/vGFaP+i42eWCRQJCC8HC5bk7SQ1wgExEBGzfChg0QGal6rHRpmDED+vSBVHYzsoqmz+989eycPXuWDh064OjoiEKhUHoikkhqr5D85e7urjInODiYPn36YGFhgZWVFUOGDCEin5SjQCDIPq/DNQvu1HSeQJBjxMfDy5cQGCiETnKio8HTE1q0kGNzkgudokVh6lS4fRv6989RoZMZ8jVmJzIykmrVqjF48GC6dOmS6hx3d3eVAnyGKYKb+vTpQ2BgIMePHycuLo5Bgwbx5Zdfsn379ly1XSAQ5A525kYZT8rEPIEgRwgJkZt2inTy/4iLg717YcUKeUsvOZaWMHQo9O0LJiZgYJA/Nv5LvoqdNm3aqNRuSQ1DQ8M02xPcvXuXo0eP8vfff1O7dm0AfvnlF9q2bcuiRYtUUrYFAkHBoI6zNQ6WRgSFRqcat6NA7oZex9k6laMCQQ4TGys/yJP1M/zoSUyEP/+EZcvkhqbJMTGBAQNg8GDQorAQrQ9Q9vLyws7OjvLlyzNy5EjevXunPObt7Y2VlZVS6IAcqKujo8Ply5fTXDMmJoawsDCVl0Ag0A50dRRM6yDXXUpZUSfp/bQOlUS9HUHukpRO/vSpEDpJSBKcPAkdO8K336oKHX19WeScOAFjx2qV0AEtFzvu7u5s3ryZkydPMn/+fM6cOUObNm1I+Lf0dlBQkEqFX5CrDFtbW6ulYSdn7ty5WFpaKl9OTk65eh8CgSBzuFd2YGXfmthbqm5V2VsaibRzQe7z4YMsct69E3Vzkrh0CXr2hK++gvv3/xvX1YUvvoC//oLJk8FGO5MGtLrOTs+ePZX/r1KlClWrVsXFxQUvLy+aN2+e5XUnTZrE+PHjle/DwsKE4BEItAz3yg60rGQvKigL8o7ERDkuJyQkvy3RHm7ehMWL4eJF9WNt28p1c5yd896uTKLVYiclZcqUoWjRojx8+JDmzZtjb2/P69evVebEx8cTHBycZpwPyHFAKQOdBQKB9qGroxDp5YK8ISJCLg4osqxk7t+HpUvlbamUNGkC48ZBGv0YtZECJXaeP3/Ou3fvcHCQXdj169cnJCSEa9euUatWLQBOnTpFYmIidevWzU9TBYUQ0adJICiExMfLNXPCw/PbEu0gIEBOHz94UH0L79NPZZHz7/O2IJGvMTsRERH4+Pjg4+MDwOPHj/Hx8SEgIICIiAgmTJjApUuXePLkCSdPnqRjx464uroqe0lVrFgRd3d3hg0bxpUrV7hw4QIeHh707NlTKzOxktcNMjAwwNXVlZkzZxJfiD9J7NmzBzc3NywtLTEzM6Nq1arMnDmT4ODgfLMpOjqaUaNGYWNjg5mZGV27duVVyrTJFBz1DaTWN540d29L57rlaFixOJYlK7D5+N/KOUFBQfTr1w97e3tMTU2pWbOmaGYqEGgzoaHw5IkQOiBnnE2bBm3awIEDqkLnk09g/XrYsqVACh3IZ7Fz9epVZSsBgPHjx1OjRg2mTp2Krq4uN2/e5PPPP6dcuXIMGTKEWrVqce7cOZUtqG3btlGhQgWaN29O27ZtadSokVb3VHJ3dycwMJAHDx7wzTffMH36dBYuXJhr14vNxzLm33//PT169ODTTz/lyJEj+Pr68tNPP/HPP/+wZcuWLK8bFxenNpaZ+xw3bhwHDx5k9+7dnDlzhpcvX6ZZ5wlkoTP0lz+5tWos+tYlsO89F4dByzGt24MfDtznqK/cHqN///74+flx4MABbt26RZcuXejevTs3btzI/E0KBILcIzZWbvPw6pWomxMcDPPny60ddu5U3cZzcZG9PHv2QKNGoCi4nmzRLoK8axcxcOBAQkJCVCpFt2rVivDwcLy9vYmJieH7779nx44dhISEULlyZebPn4+bmxsgN/v08PDg7NmzvH//HhcXFyZPnqzSiNPNzY3KlSujp6fH1q1bqVKlCqdOnWLGjBls2LCBV69eYWNjwxdffMGyZcsAeP/+PWPGjOHgwYPExMTQpEkTli1bRtmyZQHYuHEjY8eOZdeuXYwdO5Znz57RqFEjPD09lVuKKbly5Qp169ZlyZIlKv21kggJCcHKygqAlStXsmjRIp49e4azszM//PCDSid1hULBr7/+ypEjRzh58iQTJkwAYP/+/Xh4eDB79myePn1KogZ/tEJDQ7G1tWX79u188cUXANy7d4+KFSvi7e1NvXr1VOYnJEo0mn+Km5tnoNDVo2j7b1SOJ9V8OT+xGZYW5qxcuVLFdhsbG+bPn6/SC0wgEOQTkiT3bRJZVum3diheXA487tAh5yoelykDejkfOVMg2kUI5K7iSV4JDw8PvL292blzJzdv3qRbt264u7vz4MEDQN5+qVWrFn/++Se+vr58+eWX9OvXjytXrqisuWnTJgwMDLhw4QKrVq1iz549LF68mNWrV/PgwQP2799PlSpVlPMHDhzI1atXOXDgAN7e3kiSRNu2bVU8KFFRUSxatIgtW7Zw9uxZAgIC+Pbbb9O8r23btmFmZsZXX32V6vEkobNv3z7GjBnDN998g6+vL8OHD2fQoEGcPn1aZf706dPp3Lkzt27dYvDgwQA8fPiQPXv2sHfvXuVW6MCBA5XiMDWuXbtGXFwcLVq0UI5VqFCBkiVL4u3trTb/yuNgXoZE8eHRVfSKOPJq1xSe/dKHwM3jibrvjQQEhkZz5XEwDRo0YNeuXQQHB5OYmMjOnTuJjo5O1x6BQJBHREfL8Shv337cQie91g62tnJrh6NHoVOnfGvtkBsUqADlwoQkSZw8eZJjx44xevRoAgIC8PT0JCAgQBlv9O2333L06FE8PT2ZM2cOxYsXVxEYo0eP5tixY/z222/UqVNHOV62bFkWLFigfP/nn39ib29PixYt0NfXp2TJksr5Dx484MCBA1y4cIEGDRoAslBxcnJi//79dOvWDZC3jlatWoWLiwsgC7OZM2emeX8PHjygTJky6Ovrp/t1WLRoEQMHDlSKovHjx3Pp0iUWLVpE06ZNlfN69+7NoEGDVM6NjY1l8+bNKl3XHRwc0vXwBAUFYWBgoBRbSRQrVizV2kyvw6NJjAxFiv1A2OXfsfqsH0XcBvHh8TXe7JtDsV5zMCpZhdfh0fz222/06NEDGxsb9PT0MDExYd++fbi6uqb7NRAIBLlIYqLsyXn/Pr8tyV/i4uTtqBUr5Kyz5FhawrBhcmsHY+P8sS+XEWInjzl06BBmZmbExcWRmJhI7969mT59Ol5eXiQkJFCuXDmV+TExMdj8W6QpISGBOXPm8Ntvv/HixQtiY2OJiYnBxMRE5ZxaKQLIunXrxpIlSyhTpgzu7u60bduWDh06oKenx927d9HT01PJXrOxsaF8+fLcvXtXOWZiYqIUOiCLipRp/8nRdHf07t27fPnllypjDRs2ZOnSpSpjyatkJ1GqVCkVoQNywcicxM7cCEmSxZOxaz0sPu0EgEGxMsS8uEu4zxGMSlbBztyIKVOmEBISwokTJyhatCj79++ne/funDt3TsWTJhAI8ojISDkupxAngWRIQoLc2uGXX1Jv7TBwoNzawdw8X8zLK4TYyWOaNm3KypUrMTAwwNHREb1/9zAjIiLQ1dXl2rVr6KZwHZqZmQGwcOFCli5dypIlS6hSpQqmpqaMHTtWLTjX1NRU5b2TkxN+fn6cOHGC48eP89VXX7Fw4ULOnDmjsd0pPTQKhSJdQVOuXDnOnz9PXFxcht4dTUh5T2mNZYS9vT2xsbEqMUMAr169SrU2Ux1na4rb2/FCRxf9oqqFJ/VtnIh5fgcHSyNsEt+zfPlyfH19+eSTTwCoVq0a586dY8WKFaxatSrTtgoEgiwi0sn/a+2wdKlqxWOQm3L27g1ffqm1FY9zGhGzk8eYmpri6upKyZIllUIHoEaNGiQkJPD69WtcXV1VXkkP4QsXLtCxY0f69u1LtWrVKFOmDPdT/hCngbGxMR06dGDZsmV4eXnh7e3NrVu3qFixIvHx8Sq9xN69e4efnx+VKlXK8n327t2biIgIfv3111SPh/xbobRixYpcuHBB5diFCxeyde30qFWrFvr6+pw8eVI55ufnR0BAAPXr11ebr6ujYEbnahjalyU++IXKsfjgF+hZ2DGtQyViouXeOTo6qr9Surq6GgVOCwSCHEKkk4O3N/ToAaNGqbd26N5dbu0wadJHI3RAeHa0hnLlytGnTx/69+/PTz/9RI0aNXjz5g0nT56katWqtGvXjrJly/L7779z8eJFihQpws8//8yrV68yFAYbN24kISGBunXrYmJiwtatWzE2NqZUqVLY2NjQsWNHhg0bxurVqzE3N+e7776jePHidOzYMcv3U7duXf73v//xzTff8OLFCzp37oyjoyMPHz5k1apVNGrUiDFjxjBhwgS6d+9OjRo1aNGiBQcPHmTv3r2cSK1qpwZMmjSJFy9esHnz5lSPW1paMmTIEMaPH4+1tTUWFhaMHj2a+vXrq2RiVahQgblz59K5c2fcKzsw4X8TmP3NcMJLfIJRqap8eHSNKP8rLPTcg3tlB+LiiuLq6srw4cNZtGgRNjY27N+/n+PHj3Po0KEs3YtAIMgEojs5/POP3NohlWSLgtTaITcQYkeL8PT05Mcff1QKhKJFi1KvXj3at28PwA8//MCjR49o3bo1JiYmfPnll3Tq1InQ0NB017WysmLevHmMHz+ehIQEqlSpwsGDB5WxQJ6enowZM4b27dsTGxtL48aNOXz4cLa3n+bPn0+tWrWU2ziJiYm4uLjwxRdfMGDAAAA6derE0qVLWbRoEWPGjMHZ2RlPT88sZzAFBgYSkHJfOgWLFy9GR0eHrl27EhMTQ+vWrdU8UH5+fipf11ljBlPKVGLGj7N57bWWUmXKsv73PXTu3AmQt/kOHz7Md999R4cOHYiIiMDV1ZVNmzbRtm3bLN2LQCDQAEmSa8UEB3+8WVZ+fvJ2VTKPtZKmTeUu5BUq5LlZ2oSos0Pe1dkRCAQCQQ7y4YPszcnH4qn5ytOncuDxoUPqQq9OHbm1Q82a+WNbSvK5zo7w7AgEAoGgYJGYKAcgZ+DVLrS8eiWnkO/Zo55pVrkyjB8PDRoU6IrHOY0QOwKBQCAoOHzM3cmDg2HtWti2DWJiVI+VLQtjxsjFAoXIUUOIHYFAIBBoP/HxssiJiMhvS/KeiAi56rGnp3prhxIlYPTonG3tUAgRYkcgEAgE2k1oqLxt9bGVcYiOlr04a9bAv+U6lNjayqnlXbvKdXME6SLEjkAgEAi0k7g4OT4lKiq/LclbYmPleJxff1Vv7WBlJbd26NOn0LZ2yA2E2BEIBAKB9vH+/cfXtDMhQc6s+uUXePZM9ZiJCQwaJL8KeWuH3ECIHYFAIBBoDzExsjcnOjq/Lck7Mmrt0KeP3NrB2jp/7CsEiHYRecjAgQNRKBTKl42NDe7u7ty8eVNlnkKhYP/+/WrnDx8+HF1dXXbv3q12LCoqikmTJuHi4oKRkRG2trY0adKEP/74I9t2r1mzBjc3NywsLFAoFMpWD0k8efKEIUOG4OzsjLGxMS4uLkybNk2tZ1dKgoKC6NevH/b29piamlKzZk327NmjMmf27Nk0aNAAExMTtU7lAoGgECFJsicnIODjEjoXL8otHFJr7dCjBxw/Dt99J4RONhFiJ49xd3cnMDCQwMBATp48iZ6enrJCcnpERUWxc+dO/ve//7Fhwwa14yNGjGDv3r388ssv3Lt3j6NHj/LFF1/w7t27bNscFRWFu7s7kydPTvX4vXv3SExMZPXq1dy+fZvFixezatWqNOcn0b9/f/z8/Dhw4AC3bt2iS5cudO/enRs3bijnxMbG0q1bN0aOHJnt+xAIBFrKhw9ygbyPqQqyjw8MGCBvSyX/wKtQQPv2cPgwzJwJqTQoFmQeUUGZvKugPHDgQEJCQlS8NufPn+ezzz7j9evX2NraArJnZ9++fXTq1Ek5b9OmTaxatYqjR4/i6OjIvXv3cHL6rwu3lZUVS5cuVbZhyA28vLxo2rQp79+/z9DLsnDhQlauXMmjR4/SnGNmZsbKlSvp16+fcszGxob58+czdOhQlbkbN25k7Nixal4lgUBQgPkYiwP6+cGSJXDqlPqxwtzaIZ8rKAvPTj4SERHB1q1bcXV1VfapSov169fTt29fLC0tadOmDRs3blQ5bm9vz+HDhwlPp9Pv9OnTKV26dA5YnjGhoaFYZ+B2bdCgAbt27SI4OJjExER27txJdHR0lvtiCQSCAkREhNyd/GMROk+fwjffQMeO6kKnTh3YuRNWrSqcQkcLEGInjzl06BBmZmaYmZlhbm7OgQMH2LVrFzo6aX8rHjx4wKVLl+jRowcAffv2xdPTk+ROuTVr1nDx4kVsbGz49NNPGTduHBcuXFBZp2jRori4uOTOjSXj4cOH/PLLLwwfPjzdeb/99htxcXHY2NhgaGjI8OHD2bdvH66urrluo0AgyCfi4yEwEF6+/DiqIAcFwdSp0KaNeg+rKlXkQoGbN0ONGvln40eAEDt5TNOmTfHx8cHHx4crV67QunVr2rRpw9OnT9M8Z8OGDbRu3ZqiRYsC0LZtW0JDQzmV7NNB48aNefToESdPnuSLL77g9u3bfPbZZ8yaNUs5x8PDg5OpdcX9lzlz5iiFmJmZWYbdw1PjxYsXuLu7061bN4YNG5bu3ClTphASEsKJEye4evUq48ePp3v37ty6dSvT1xUIBAWA0FDZm5OOB7rQEBwM8+ZBy5awa5ecVp5E2bJyb6vdu0UPqzxCxOyQvzE7CQkJWFpaMnbsWH788UdANWYnISEBJycngoKCVLw/CQkJ9O7dm23btqV5vR9//JGZM2cSERGBgQYVNoODgwkODla+L126NHrJ9lgzitl5+fIlbm5u1KtXj40bN6brrfL398fV1RVfX18++eQT5XiLFi1wdXVl1apVKvNFzI5AUICJjZWL430MxQHDw/9r7ZDyfkuUgK+/lgOQP5bWDjo6YGkJRYvmiqgTXc8LCAqFAh0dHT58+JDq8aQ4nBs3bqCb7JfD19eXQYMGERISkmawcKVKlYiPjyc6OlojsWNtbZ1hnE1avHjxgqZNm1KrVi08PT3TFTogZ3gBavN0dXVJ/NhKwgsEhRVJkosDvntX+LOsPnyQWzusXStaOwDo68vVni0tZcGTzwixk8fExMQQFBQEwPv371m+fDkRERF06NAh1fnr16+nXbt2VKtWTWW8UqVKjBs3jm3btjFq1Cjc3Nzo1asXtWvXxsbGhjt37jB58mSaNm2qVLvLly9n37596W5lpUZQUBBBQUE8fPgQgFu3bmFubk7JkiWxtrbmxYsXuLm5UapUKRYtWsSbN2+U59r/mzb54sULmjdvzubNm6lTpw4VKlTA1dWV4cOHs2jRImxsbNi/fz/Hjx/n0KFDyvMDAgIIDg4mICCAhIQEfHx8AHB1dcXMzCxT9yEQCPKQ6Gi5OGDK7tyFjdhY+P13ubVDsr99gPyw//JL6N3742ntYGwMRYqAlv19FmInjzl69CgODg4AmJubU6FCBXbv3p1qBtKrV6/4888/2b59u9oxHR0dOnfuzPr16xk1ahStW7dm06ZNTJ48maioKBwdHWnfvj1Tp05VnvP27Vv8/f0zbfOqVauYMWOG8n3jxo0B8PT0ZODAgRw/fpyHDx/y8OFDSpQooXJu0i5pXFwcfn5+So+Ovr4+hw8f5rvvvqNDhw5ERETg6urKpk2baNu2rfL8qVOnsmnTJuX7Gv8G8Z0+fVpkbQkE2khSccD37/PbktwlIQEOHpRbOzx/rnosqbXD4MFa99DPFRQKuYVFkSJgaJjf1qSKiNkh72J2BAKBoFATFSV7c+Li8tuS3COptcOSJfDggeoxAwPo21du1PkxVDzW1ZW3qayscqWGjiaImB2BQCAQ5A0fQ3FASQJvb/j5Z0iZMaqrC198AV999XFUPDYwkL04FhYFJpNMiB2BQCAQZJ2ICDnTqjDXzLlxAxYvhsuXVceTWjuMHg2lSuWPbXmJqansxTE1zW9LMo0QOwKBQCDIPAkJ8pZVRER+W5J73Lsnb1edPq1+rHlzGDMGypfPc7PyFIVC9uAUKVKgM8mE2BEIBAJB5ggLk7etkhfKK0w8eSIHHifLDFVSrx6MGwfVq+e1VXmLnt5/qeOFoCaQEDsCgUAg0Iy4ONmbU1iLAwYGyinke/aoC7mqVWH8eKhfP39syyuMjGSRY25eYOJxNEGIHYFAIBBkTFJxwMJY9DM4GFavhu3b5bo5ySlbVu5E3rx5oXr4q2FmJm9VFdJ6QELsCAQCgSBtYmPlZpbR0fltSc4THg4bNsDGjereKicnubVDu3aFYhsnVZJaOVhZyRWPCzH5WsP57NmzdOjQAUdHRxQKhUrPqLi4OCZOnEiVKlUwNTXF0dGR/v378/LlS5U1SpcujUKhUHnNmzcvj+9EICicJCRKePu/4w+fF3j7vyMh8aMvy/XxIEmyJ+fp08IndD58kNs6NG8ub1slFzp2djBjBhw5Ap9/XjiFjr6+3MKiTBn530IudCCfxU5kZCTVqlVjxYoVaseioqK4fv06U6ZM4fr16+zduxc/Pz8+//xztbkzZ84kMDBQ+Ro9enRemJ9pBg4cqBRkBgYGuLq6MnPmTOL/Tdn08vJSEW3GxsZ88sknrFmzRm2dTp06qa3v7e2Nrq4u7dq1S/X6+/bto169elhaWmJubs4nn3zC2LFjs31fa9eu5bPPPqNIkSIUKVKEFi1acOXKlTTvPenl7u6e7rrTp09XO6dChQrK40+ePFE7nvTavXs3IDc37dChA2ZmZtSoUYMbN26oXGPUqFH89NNP2f4aFEaO+gbSaP4peq29xJidPvRae4lG809x1Dcwv00T5DYfPsgip7D1tIqNlftXtWwJixap1gWysoKJE+H4cejZs3AKAGNjcHQEZ2d5y0oLelblFfm6jdWmTRvatGmT6jFLS0uOHz+uMrZ8+XLq1KlDQEAAJUuWVI6bm5srezBpO+7u7nh6ehITE8Phw4cZNWoU+vr6TJo0STnHz88PCwsLPnz4wMGDBxk5ciQuLi40b9483bXXr1/P6NGjWb9+PS9fvsTR0VF57OTJk/To0YPZs2fz+eefo1AouHPnjtrXOCt4eXnRq1cvGjRogJGREfPnz6dVq1bcvn2b4sWLq917EoYalBX/5JNPOHHihPJ98i7sTk5OBAaqPnjXrFnDwoULlT9Xs2fPJjw8nOvXr7Ny5UqGDRvG1atXAbh06RKXL19m2bJlWbvxQsxR30BGbr1OysdcUGg0I7deZ2XfmrhXdsgX2wS5SGEtDpheawdTU7mtw8CBhbO1Q1IrBysrOfj4I6VAxeyEhoaiUCjUunzPmzePWbNmUbJkSXr37s24ceNUHoopiYmJISZZc7qwsLDcMlkNQ0NDpTAbOXIk+/bt48CBAypix87OTnmPX3/9NcuWLeP69evpip2IiAh27drF1atXCQoKYuPGjUyePFl5/ODBgzRs2JAJEyYox8qVK5eqhyizbNu2TeX9unXr2LNnDydPnqR///7K8eT3ril6enppnqOrq6t2bN++fXTv3l3ZJPTu3bv07NmTcuXK8eWXXyq9ZHFxcYwYMYJ169apdJMXyFtXMw7eURM6ABKgAGYcvEPLSvbo6hTigM2PjfBwWegUpuKAkiR7apYuhX8bGSsxNIQ+fQpvawctaOWgTRQYH1Z0dDQTJ06kV69eKv0vvv76a3bu3Mnp06cZPnw4c+bM4X//+1+6a82dOxdLS0vly8nJKbfNTxNjY2NiU0b//4skSRw9epSAgADq1q2b7jq//fYbFSpUoHz58vTt25cNGzaQvO2Zvb09t2/fxtfXN801kraFvLy8snQvSURFRREXF4d1ij8gXl5e2NnZUb58eUaOHMm7d+8yXOvBgwc4OjpSpkwZ+vTpQ0BAQJpzr127ho+PD0OGDFGOVatWjVOnThEfH8+xY8eoWrUqAAsWLMDNzY3atWtn8S4LL1ceBxMYmnaMhgQEhkZz5XFw3hklyD3i4+HFCznturAIHUmC8+flFg6jR6sKHT096NFDFkETJxY+oWNgAMWKyfE4RYsKofMvBeKrEBcXR/fu3ZEkiZUrV6ocGz9+vPL/VatWxcDAgOHDhzN37tw0t0kmTZqkcl5YWFieCx5Jkjh58iTHjh1TizFK6hweExNDYmIiM2fOVHYaT4v169fTt29fQN4uCg0N5cyZM8rO4KNHj+bcuXNUqVKFUqVKUa9ePVq1akWfPn2UXyd9fX3Kly+PiYlJtu5t4sSJODo60qJFC+WYu7s7Xbp0wdnZGX9/fyZPnkybNm2UcUapUbduXTZu3Ej58uUJDAxkxowZfPbZZ/j6+mJubp7q16BixYo0aNBAOfbdd98ptwFLly7N+vXrefDgAZs2bcLb25sRI0bw119/Ubt2bdauXYulpWW27r0w8Dpcs2BUTecJtJjCmE5+/brc2iFF3CAKhZxZ9fXXhbO1QwFu5ZAXaL3YSRI6T58+5dSpUxl2Ja9bty7x8fE8efKE8mmU8TY0NNQoXiQ3OHToEGZmZsTFxZGYmEjv3r2ZPn26ypxz585hbm5OTEwMV65cwcPDA2tra0aOHJnqmn5+fly5coV9+/YB8tZPjx49WL9+vVLsmJqa8ueff+Lv78/p06e5dOkS33zzDUuXLsXb2xsTExOKFy/OvXv30rQ9ICCASpUqKd9PnjxZZasM5C3FnTt34uXlhVGy/eGePXsq/1+lShWqVq2Ki4sLXl5eaW7PJY/nqlq1KnXr1qVUqVL89ttvKt4bgA8fPrB9+3amTJmiMm5pacn27dtVxpo1a8bChQvZtm0bjx49ws/Pj2HDhjFz5kwRrAzYmWu2r6/pPIEWEhMjFwcsTFlW9+7JIic1z3Rhbe1QSFo55AVaLXaShM6DBw84ffo0NjY2GZ7j4+ODjo4OdnZ2eWBh5mnatCkrV67EwMAAR0fHVGOLnJ2dlTE7n3zyCZcvX2b27Nlpip3169cTHx+vEpAsSRKGhoYsX75cxVvh4uKCi4sLQ4cO5fvvv6dcuXLs2rWLQYMGZWi7o6MjPj4+yvcpt6kWLVrEvHnzOHHihHK7KC3KlClD0aJFefjwYYaB10lYWVlRrlw5Hqbcewd+//13oqKiVGKEUsPT0xMrKys6duxIly5d6NSpE/r6+nTr1o2pU6dqZEdhp46zNQ6WRgSFRqcat6MA7C2NqONcyNz/HwOJibIn5/37/LYk53jyBJYtgz//VD9Wv77c2qFatTw3K1cpZK0c8oJ8FTsREREqD67Hjx/j4+ODtbU1Dg4OfPHFF1y/fp1Dhw6RkJBAUFAQID9kDQwM8Pb25vLlyzRt2hRzc3O8vb0ZN24cffv2pUiRIvl1W+liamqKq6trps7R1dXlw4cPqR6Lj49n8+bN/PTTT7Rq1UrlWKdOndixYwcjRoxI9dzSpUtjYmJCZGSkRnbo6emlafuCBQuYPXs2x44d0ygO5vnz57x79w4HB80zeiIiIvD396dfv35qx9avX8/nn3+Ora1tmue/efOGmTNncv78eQASEhKIi4sDZGGdUFj7/GQSXR0F0zpUYuTW6yhARfAkhSNP61BJBCcXNCIj5e7k//7MF3gCA2HFCti7V721Q7VqssgpbK0dDA1lL04ha+WQF+Sr2Ll69SpNmzZVvk+KoxkwYADTp0/nwIEDAFRP0XDt9OnTuLm5YWhoyM6dO5k+fToxMTE4Ozszbtw4lXicgsjr16+Jjo5WbmNt2bKFL774ItW5hw4d4v379wwZMkQt3qRr166sX7+eESNGMH36dKKiomjbti2lSpUiJCSEZcuWERcXR8uWLQF48eIFzZs3Z/PmzdSpU0dje+fPn8/UqVPZvn07pUuXVopSMzMzzMzMiIiIYMaMGXTt2hV7e3v8/f353//+h6urK61bt1au07x5czp37oyHhwcA3377LR06dKBUqVK8fPmSadOmoaurS69evVSu//DhQ86ePcvhw4fTtXPs2LF88803ynT4hg0bsmXLFlq1asWaNWto2LChxvdc2HGv7MDKvjWZcfCOSrCyvaUR0zpUEmnnBYmEBFnkhIfntyU5w7t3/7V2SCncypWTWzs0a1a4xEAhb+WQF+Sr2HFzc1PJGEpJescAatasyaVLl3LarHwnKdZIT08PJycnhg8frhbXk8T69etp0aJFqoG1Xbt2ZcGCBdy8eZMmTZqwYsUK+vfvz6tXryhSpAg1atTgr7/+Ul4vLi4OPz8/ojLZ5G/lypXExsaqCbJp06Yxffp0dHV1uXnzJps2bSIkJARHR0datWrFrFmzVGKn/P39efv2rfL98+fP6dWrF+/evcPW1pZGjRpx6dIlNe/Nhg0bKFGihJpnKznHjh3j4cOHbNmyRTnm4eHB1atXqVu3LnXq1GHatGmZuu/CjntlB1pWsufK42Beh0djZy5vXQmPTgEiNBTevi0c3cnDwuTWDps2qbd2KFlSzroqTK0dPqJWDnmBQspIUXwEhIWFYWlpSWhoaIYB0AKBQKD1xMbKAchpbH8XKD58gC1bYN069WKHxYrBqFHQpUvhEQT6+v/F43xEFY6ziqbPb60OUBYICisJiZLwmAhyHkn6L528oH+OjY2F3bth5Uq52GFyrKxgxAjo1avwVAU2Npa3qgpjFWctQIgdgSCPOeobqBYL4yBiYZQIIZhFoqNlb06y6vAFkoQEOHBAbu3w4oXqscLW2kG0csgzhNgRCPIQ0XMqfYQQzAKFJZ08qbXDkiXg7696zNAQ+vaVWztoaaZtphCtHPIc8VUWCPII0XMqfYQQzAKRkbI3pyC3eZAkuHABfv4Zbt9WPaanJ7d8+OorOT6noGNgIIs1C4vClS1WABBiRyDIIzLTc6q+S8YFNAsTQghmksKSTn7tmuzJSa21w+efyxlW+di7MMcwMZFFjmjlkG8IsSMQ5BGi51TaCCGYCcLC5IDdgpxOfveuLHJSa+3QooXc2qFcuby2KmcRrRy0CiF2BII8QvScShshBDUgLk7esspkHSyt4vFjubVDagVAGzaUCwJm0GpG6xGtHLQSkcSfhwwcOBCFQsG8efNUxvfv348i2f6tl5cXCoVC+bK1taVt27bcunUr1fVSawcxatQoFAoFAwcOVI69efOGkSNHUrJkSQwNDbG3t6d169ZcuHBBOad06dIq1056pbQ5IwICAmjXrh0mJibY2dkxYcIE4jOIK7h//z4dO3akaNGiWFhY0KhRI06fPq0y5+TJkzRo0ABzc3Ps7e2ZOHFihutqC0k9p9LahFEgB+N+jD2nhBDMgOBguQdUQRU6gYHw/fdy0b+UQqd6dblQ4IYNBVvoGBqCvT04O4O1tRA6WoYQO3mMkZER8+fP570GmRN+fn4EBgZy7NgxYmJiaNeuHbGxsSpznJyc2Llzp0rvrOjoaLZv307JkiVV5nbt2pUbN26wadMm7t+/z4EDB3Bzc+Pdu3cq82bOnElgYKDKa/To0RrfY0JCgtLWixcvsmnTJjZu3Jhho8327dsTHx/PqVOnuHbtGtWqVaN9+/bK9hP//PMPbdu2xd3dnRs3brBr1y4OHDjAd999p7Ft+UlSzylATfB87D2nhBBMg5gYePpUroJcEOvmvHsHs2dDy5bw+++qW2/lysGvv8LOnVCvXv7ZmF3MzOS4olKlROCxFiPETh7TokUL7O3tmTt3boZz7ezssLe3p2bNmowdO5Znz55x7949lTk1a9bEycmJvXv3Ksf27t1LyZIlqVGjhnIsJCSEc+fOMX/+fJo2bUqpUqWoU6cOkyZN4vPPP1dZM8lrkvxlmonAur/++os7d+6wdetWqlevTps2bZg1axYrVqxQE2tJvH37lgcPHvDdd99RtWpVypYty7x584iKisLX1xeAXbt2UbVqVaZOnYqrqytNmjRhwYIFrFixgvACEqiZ1HPK3lLVQ2FvafRRZxsJIZgCSZLjcp4+LZh1c8LCYPFiOf5m82bVHlalSsFPP8Eff0Dz5gVTHOjoyLE4zs7g6Ch6VhUAhNjJY3R1dZkzZw6//PILz58/1+ic0NBQdu7cCYBBKoFugwcPxtPTU/l+w4YNDBo0SGVOUlPO/fv3E5PNP56lS5dOs1cXgLe3N1WqVKFYslTR1q1bExYWxu2UqaX/YmNjQ/ny5dm8eTORkZHEx8ezevVq7OzsqFWrFgAxMTEYpSi8ZWxsTHR0NNeuXcvWPeUl7pUdOD+xGTuG1WNpz+rsGFaP8xObZVroJCRKePu/4w+fF3j7vyMhsQB+8k+GEIL/EhUlb1kVxLo5UVGwZo0sclatUt12K1YMZs6EP/+E9u0LZisEfX2wtYUyZeR/C0uLio8AEaCcD3Tu3Jnq1aszbdo01q9fn+a8EiVKABAZGQnA559/ToUKFdTm9e3bl0mTJvH06VMALly4wM6dO/FKlumgp6fHxo0bGTZsGKtWraJmzZo0adKEnj17UjXFPvnEiRP54YcfVMaOHDnCZ599BoCLiwtFixZN0+6goCAVoQMo3ydtSaVEoVBw4sQJOnXqhLm5OTo6OtjZ2XH06FGK/FtErHXr1ixZsoQdO3bQvXt3goKCmDlzJgCBgYFp2qON6OoospVVVFiL733UzUcTEmRvTlhYfluSeWJj4bff5NYOyZr5ArIHJKm1Q7LGvwUK0cqhwCPETj4xf/58mjVrxrfffpvmnHPnzmFiYsKlS5eYM2cOq1atSnWera0t7dq1Y+PGjUiSRLt27VIVI127dqVdu3acO3eOS5cuceTIERYsWMC6detUApknTJig8h6gePHiyv+fPHkyczerAZIkMWrUKOzs7Dh37hzGxsasW7eODh068Pfff+Pg4ECrVq1YuHAhI0aMoF+/fhgaGjJlyhTOnTuHjhZ8SsyrNgeFvfhedoVggSQ8XK6bU9DSyTNq7TBkCAwYUDBFgkIh212kiGjlUAgQYiefaNy4Ma1bt2bSpElqwiIJZ2dnrKysKF++PK9fv6ZHjx6cPXs21bmDBw/Gw8MDgBUrVqR5XSMjI1q2bEnLli2ZMmUKQ4cOZdq0aSo2FC1aFFdX1yzfm729PVdSFAl79eqV8lhqnDp1ikOHDvH+/Xtl59pff/2V48ePs2nTJmUQ8vjx4xk3bhyBgYEUKVKEJ0+eMGnSJMqUKZNle3OCvPK0iOJ7hYy4OFnk/Ou9LTAkJsJff8lp5Km1dujXD4YOLZitHUQrh0KJxh+Hw8LCNH4JNGPevHkcPHgQb2/vDOeOGjUKX19f9u3bl+pxd3d3YmNjiYuLo3Xr1hrbUKlSJeU2WU5Rv359bt26xevXr5Vjx48fx8LCgkqVKqV6TtS/e/spPTQ6OjokJiaqjCkUChwdHTE2NmbHjh04OTlRs2bNNO3J7diWJE9LyqJ4SZ6Wo745t8WWmeJ7Ai3n/Xs5ALkgCR1JgrNn5RYOY8aoCh09PejdW+5vNWFCwRM6BgZyXFGZMlC0qBA6hQyNv5tWVlYqtWDSI6GguWLziSpVqtCnTx+WLVuW4VwTExOGDRvGtGnT6NSpk9r3QldXl7t37yr/n5J3797RrVs3Bg8eTNWqVTE3N+fq1assWLCAjh07qswNDw9Xi60xMTFRelyaN29O586dlZ6klLRq1YpKlSrRr18/FixYQFBQED/88AOjRo3C8N89+ytXrtC/f39OnjxJ8eLFqV+/PkWKFGHAgAFMnToVY2Nj1q5dy+PHj2nXrp1y7YULF+Lu7o6Ojg579+5l3rx5/Pbbb6neM+S+x0UTT8v3+3z5EJuAvaVxtra2EhIlLjx8o9Hcj7r4nrYTEyMXB4wuYN+jq1flDKurV1XHFQro2BE8PApmawfRyuGjQGOxk7y425MnT/juu+8YOHAg9evXB+QMnE2bNmmUUi34j5kzZ7Jr1y6N5np4ePDzzz+ze/duunfvrnY8SYykhpmZGXXr1mXx4sX4+/sTFxeHk5MTw4YNY/LkySpzp06dqlYTZ/jw4cqYIX9/f96mDEJMhq6uLocOHWLkyJHUr18fU1NTBgwYoAwmBtmT4+fnR9y/KalFixbl6NGjfP/99zRr1oy4uDg++eQT/vjjD6pVq6Y878iRI8yePZuYmBiqVavGH3/8QZs2bVK1Iy9iWzTxtLyLjGXcb/8AWRdaqYm29NCW4nt5FcdUIJCk/7qTF6SaOXfuyK0dzpxRP9aqFXz9NZQtm+dmZQvRyuGjQyFJmf+ta968OUOHDqVXr14q49u3b2fNmjUqWUAFgbCwMCwtLQkNDU1XMAgKDgmJEo3mn0pTHCiQU5rPT2yWrYfvHz4vGLPTR+P5SVfKjNBKS7SltX5O3FdOUFgzxrLEhw+yNyeNOlM5QUKixM3nIbyLjMXG1ICqJayy9zPw6JEck3PkiPqxBg1g3LiCV/FYV1eOxbGyEhWOCwmaPr+ztCnp7e2damZQ7dq1GTp0aFaWFAhylLxqLJlZD0pmg4jT2yZLiTYV3yvsGWMak5gop5OHhubqZc7ef83y0/68Cf+vhpatuSEeTV1oXM4uc4u9eAErVsC+fbL9yalRQ+5fVdAqHhsayl4cc/OCWcRQkG2ylK/r5OTE2rVr1cbXrVuHU0HcsxUUOvKqsWRGbQ5SIzNBxBmJtuRoS/G9jOKYQBZ7Bb0IYoZERMjFAfNA6Ew/cEdF6AC8DY9h+oE7nL3/Oo0zU/D2Lfz4I7RuDXv2qAqd8uXlIoE7dhQsoWNqCiVKiFYOgqx5dhYvXkzXrl05cuQIdevWBeSA0wcPHrBnz54cNVAgyAp51Vgyqc3ByK3XUYBGHpgkNBFamooxj6aujGtZLt89OpB3XjWtJT5eTiePiMj1SyUkSiw/7Z9ugPzy0/40dLVN+2cjNFRuwrlpk7zdlpxSpeSsqzZtCk7FYxGPI0iFLP30tm3blvv379OhQweCg4MJDg6mQ4cO3L9/n7Zt2+a0jQJBpsnLxpJptTnICE2ElqZirKFrUa0QOpB3XjWtJCRE9ubkgdABuPk8RM2jkxwJeBMew83nIeoHo6Jg9er/WjskFzr29rKX588/5U7lBUHo6OrKKeNlysgp5MmETmFrrSLIPFkuJODk5MScOXNy0haBIMdIz+OSG7EtydscBIVFM+vQbYIj41KdmxRErInQShJtQaHRqX56z8xaeUVeedW0ithYOQA5pWckFXIykPhdpGYBzyrzCltrhwzicUSgvACyIXbOnTvH6tWrefToEbt376Z48eJs2bIFZ2dnGjVqlJM2CgRZIsnjkvIPnX0u/aFL3ubAWF+HkVuvA9kTWnkt2nKCgijQsowkQXCw/NIgsTVHA4kBG1PNtmlsTA3k7bUDB2D5cvXWDmZmcmuH/v0LTmsHU1NZ5JiYpDlFBMoLksiSb3LPnj20bt0aY2Njrl+/ruyiHRoaKrw9Aq0ipzqMZ+W6OdXBu6B1A08SaIDaNqK2CrQs8eGDXAH53TuNhU6OBBIno2oJK2zNDdPdrrUz06fq7UvQoQNMmqQqdAwN5bYOJ0/CV19pv9BRKORWDqVLQ/Hi6QodESgvSE6W6uzUqFGDcePG0b9/f8zNzfnnn38oU6YMN27coE2bNml2ttZWRJ0dQW6Rk0X1ClqBvkK7fZCYKG//hIRofEpCokSvtZfSjK9RAEXNDdkxrF6q39P0tr6SRBSk8PxJEp8+v80PD45h7n9fdUF9fejWTd6yKlZM4/vIN7JQH8fb/x291l7KcN6OYfUKZ6D8R4Kmz+8seXb8/Pxo3Lix2rilpSUhmfgD8LExcOBAFAoFCoUCfX19nJ2d+d///kd0irLxSXMUCgWWlpY0bNiQU6dOqazTqVMnlXOePXvG4MGDcXR0xMDAgFKlSjFmzBjevXuXaTv37t1Lq1atsLGxQaFQ4OPjk+E5a9eu5bPPPqNIkSIUKVKEFi1apNoMdODAgTg6OmJiYoK7uzsPHjxQmVPYAgmTtrY6Vi9OfRebbImTnFwrL8gvr1qukpROnsm/c9kJJD57/zW91l5i/G//MPvPu4z/7R96rb2k9AQ1LmfH9M8rUdT8vxibykEPWX7kZ+Yf/UVV6OjoQOfOcqHAadOyJHQSEiVuBLznxN1X3Ah4n7u/o8n7VdnYZKoQ4EcdKC9QI0sxO/b29jx8+JDSpUurjJ8/fz7fu09rO+7u7nh6ehIXF8e1a9cYMGAACoWC+fPnq8zz9PTE3d2dt2/f8v3339O+fXt8fX1T/fo+evSI+vXrU65cOXbs2IGzszO3b99mwoQJHDlyhEuXLmFtrXl8RGRkJI0aNaJ79+4MGzZMo3O8vLzo1asXDRo0wMjIiPnz59OqVStu375N8eLFkSSJTp06oa+vzx9//IGFhQU///wzLVq04M6dO5iamhZeT4AGJPfaFDU1BAW8jYjJcQ9OXnuHkscxFWgSEuR08vDwLJ2epUBi/vPapJQTSVtf0z+XxU7jcnY0dLXlwelLFF23kqLXL6sv3rq13NrB1TVL95BkT07GHKVJDvSr+igD5QVpkiWxM2zYMMaMGcOGDRtQKBS8fPkSb29vvv32W6ZMmZLTNhYqDA0Nsbe3B+SMthYtWnD8+HE1sWNlZYW9vT329vasXLmS4sWLc/z4cYYPH6625qhRozAwMOCvv/7C2NgYgJIlS1KjRg1cXFz4/vvvWblypcY29uvXD5B7oGnKtm3bVN6vW7eOPXv2cPLkSfr378+DBw+4dOkSvr6+fPLJJwCsXLkSe3t7duzYQYl67fIskFDbtoMy6nuVU4LvYxaT2SI0VN62ykaD40wFEv9LpmroPH6E7rJlVDh6VH1yo0Zy1eMqVbJiuhJNhVeWUSjkjKoiRXIkE+yjCpQXZEiWxM53331HYmIizZs3JyoqisaNG2NoaMi3337L6NGjc9rGQouvry8XL16kVKlS6c5LEjCxqfTVCQ4O5tixY8yePVs5Lwl7e3v69OnDrl27+PXXX1EoFEyfPp2NGzdmSshkhaioKOLi4pQepaQgdiOj/z5F6ejoYGhoyLlz57n/xjndP+qatlfICG174GvS90oTwZeRgBNZKVkgLk5OJ4+KyvZSSYHEb8Nj0nzwFjU3pGoJK+WYJltfOi9fEjJuAjZ//Zl6a4dx4+Dfwq/ZIUeKF6aFjs5/8Th6WU4QVqMgZjIKco8s/WQpFAq+//57JkyYwMOHD4mIiKBSpUqYaXskvxZw6NAhzMzMiI+PJyYmBh0dHZYvX57m/KioKH744Qd0dXVp0qSJ2vEHDx4gSRIVK1ZM9fyKFSvy/v173rx5g52dHUWLFsXFxSXH7ictJk6ciKOjIy1atACgQoUKlCxZkkmTJrF69WpMTU1ZvHgxz58/596jAF455H7FXW174Gva9yojwZeRgMsoKyUnxWShQJLkzuQaZllpgq6OAo+mLkw/cCfNB69HUxeVr396W19FokLp63OE9nfPYZAYr3qwQgVZ5DRpkmPtETITc1SjZBHNFtXXlwWOpWWuFS3M6/ITAu0lSz9hgwcPJjw8HAMDAypVqkSdOnUwMzMjMjKSwYMHa7zO2bNn6dChA46OjigUCvbv369yXJIkpk6dioODA8bGxrRo0UItoDU4OJg+ffpgYWGBlZUVQ4YMISKPqpdmhaZNm+Lj48Ply5cZMGAAgwYNomvXrmrzevXqhZmZGebm5uzZs4f169dTNZ0Owxkl1Rn8W03Uw8ODkydPZu8mMmDevHns3LmTffv2KT05+vr67N27l/v372NtbY2JiQmnT5+mTZs2xGsY4JidQEJtTEPNTN+rtPppJQm4lOskCbijvoGZat/w0RMdDQEB8rZVDgmdJFILJAbZozP980pqW0CpbX2ZxUQy9O99bNv1A11un1YVOqVLw+LFcgNPN7cc7QOV1ZijVDEyAgcHcHaWt6xyuTpzoQyUF2SaLHl2Nm3axLx58zA3N1cZ//DhA5s3b2bDhg0arRMZGUm1atUYPHgwXbp0UTu+YMECli1bxqZNm3B2dmbKlCm0bt2aO3fuKB+iffr0ITAwkOPHjxMXF8egQYP48ssv2b59e1ZuLdcxNTXF9d8AwQ0bNlCtWjXWr1/PkCFDVOYtXryYFi1aYGlpia2tbZrrubq6olAouHv3Lp07d1Y7fvfuXWxtbbGyssrR+0iLRYsWMW/ePE6cOKEmzmrVqoWPjw+hoaHExsZia2tL3bp1KV22MprkjGUnkFAb+zVlRbwlP0dTj83/3Cvkmj2FhsRE2ZPz/n2uXiYpkFiTCsrJt74M46Lp6nuKnjf/wixWtUqzZG+PwsNDzrLKwW2g5GQl5kgNMzNZ3KTYbs8LCk2gvCDLZOo3IywsDEmSkCSJ8PBwlfiLhIQEDh8+jJ2d5gFqbdq0oU2bNqkekySJJUuW8MMPP9CxY0cANm/eTLFixdi/fz89e/bk7t27HD16lL///pvatWsD8Msvv9C2bVsWLVqEo6NjZm4vz9HR0WHy5MmMHz+e3r17q8Tc2NvbK0VRetjY2NCyZUt+/fVXxo0bp7JGUFAQ27ZtY9SoUblif0oWLFjA7NmzOXbsmPL7kRqWlpaAvAV39epVps+YycsberkaSKiNaahZEW/Jz9FUwAVHpL39kF17CgWRkXKmVVzq7T1yGl0dhUZbPbo6CkY3csJn4Rr6+BzG+oNqJth7I3OC+w7E5ethud7aISsxR/KBf4sAFikib1sJBPlEpvyHVlZWWFtbo1AoKFeunLKmSpEiRShatCiDBw/OsQfr48ePCQoKUsZ8gPyQrFu3Lt7e3gB4e3tjZWWl8mBt0aIFOjo6XL6cSuqlFtKtWzd0dXVZsWJFltdYvnw5MTExtG7dmrNnz/Ls2TOOHj1Ky5YtKVeuHFOnTlWZ27x583TXCw4OxsfHhzt35EJlfn5++Pj4qBSL7N+/P5MmTVK+nz9/PlOmTGHDhg2ULl2aoKAggoKCVLYUd+/ejZeXF48ePeKPP/6gZcuWdOrUiTburXO94m5upKFmtyZQRs1Kk5Na41JNhZm1qUGeNUUtUCQkQFCQXFE4j4SOxsTHw549fObRj9Heu1SEToS+EbsadObOpt9xmeCRJz2skmKOIO3fUZWYIz29/5py2tkJoSPIdzLl2Tl9+jSSJNGsWTP27NmjUrslqZBdTnlTkh6sxVIUvSpWrJjyWFBQkJonSU9PD2tr63SrOMfExCizg0D2WOUXenp6eHh4sGDBAkaOHIlpFupKlC1blr///pvp06fTvXt3Xr9+jSRJdOnShS1btmCSrKT627dv8ff3T3e9AwcOMGjQIOX7nj17AjBt2jSmT58OQEBAADrJ9tpXrlxJbGwsX3zxhcpayc8JDAxk/PjxvHr1CgcHB/r3768sVZDbgYQ5nYaaE1ld6WWLpLQN1AWfpsLM3tI4X7NStC3VH4CwMHjzJlvp5LlCYiIcOwZLl8Ljx6qHDA0JaNeVsD4D+KJSqTz/GsoxR6jV2SmavM6OgQFYW6fZlFMgyC+y1C7i6dOnlCxZEkUO/jArFAr27dunrAx88eJFGjZsyMuXL3Fw+O/h0b17dxQKBbt27WLOnDls2rQJPz8/lbXs7OyYMWMGI0eOTPVa06dPZ8aMGWrjhaldxLRp0/j55585fvw49erVy29zNCY3H4xJwbyQ+gNf02ystLK6MrtO8vWyUmcnIVGi0fxTGQq48xOboaujyJe0e21L9c/JdPIcRZLg7FlYsgT+9agqSWrtMHKk7CXRkJzsrp7humam2S4CKBBkBU3bRWQpmu3UqVOYmZnRrVs3lfHdu3cTFRXFgAEDsrKsCkmF95K8AEm8evWK6tWrK+e8fq3aPC8+Pp7g4GDl+akxadIkxo8fr3wfFhaGk5NTtm3WJmbMmEHp0qW5dOkSderUUfHCaDO5GUiYE96j3Ejjdq/sQMtK9pmuoJzZOiIpr5PbXhZtS/Xn/ftcybLKNlevws8/w7VrquM6OtCxI4waBZn8+5SblY6VMUc5XARQIMhNsiR25s6dy+rVq9XG7ezs+PLLL3NE7Dg7O2Nvb8/JkyeV4iYsLIzLly8rPTb169cnJCSEa9euUatWLUAWYomJidRNp5CWoaEhhh/BL2fyrajCRlY9QNl94OdWVldWRV5mBVxeZaVoVW2fmBjZmxOtZdlmt2/LqeLnzqkfy0Zrh1yvdJxLRQAFgtwkSz+pAQEBODs7q42XKlWKgIAAjdeJiIjg4cOHyvePHz/Gx8cHa2trSpYsydixY/nxxx8pW7asMvXc0dFRudVVsWJF3N3dGTZsGKtWrSIuLg4PDw969uyp9ZlYgqyT3a2R7DzwtTGrK689NpqgFan+kvRfOrk2eXP8/eWYnGPH1I9ls7VDrlY61teXvTgWFrleG0cgyGmyJHbs7Oy4efOmWiPQf/75Bxsbzf9wXb16laZNmyrfJ20tDRgwgI0bN/K///2PyMhIvvzyS0JCQmjUqBFHjx5VSXnftm0bHh4eNG/eHB0dHbp27cqyZcuycluCAkB+b41oa3NBbasjku+iMCpK9uZoU5bV8+ewYgXs3596a4fx46FOnWxdIlcqHRsZySInRV01gaAgkSWx06tXL77++mvMzc1p3LgxAGfOnGHMmDHKzB1NcHNzS7fyr0KhYObMmcycOTPNOdbW1lpbQFCQs2jD1ohoLqgZ+SYKExLkLKt8zLBU480bWLUKdu1SF18VKsienByqeJyjlY7zsQigQJDTZEnszJo1iydPntC8eXP0/t2zTUxMpH///syZMydHDRQIktCGrRHRXFAz8kUUhofLxQG1JZ08NBTWrYPNm9XjhUqXhjFjwN09R7eEsl3pWEdH3qYSRQAFhYwsiR0DAwN27drFrFmz+OeffzA2NqZKlSoZdu8WFAy0si4KWrA18i+iuWDG5KkojI+Xt6wiI7O/Vk4QGQlbtshCJ1y16jEODnJ2VS61dshypWM9vf+acurq5rhdAkF+k62PFOXKlaNbt260b99eCJ0cxsvLC4VCoXzZ2trStm1bbt26pTJv7ty5fPrpp5ibm2NnZ0enTp3U6g5pQnR0NKNGjcLCyhoDYxNatP0cj/Wn6bX2Eo3mn+Kob6DaOXv37qVVq1bY2NigUCjw8fFRm+Pm5qZyHwqFghEjRmTaPtCueBltaS6Y3SrOuUmSKLS3VP1+2Fsa5VxsVUgIPHmiHUInJgY2bYKWLeUsq+RCx9oaJk+Wg5K7dcu1LKZMVzo2NAR7e7kpp7W1EDqCQovGRQXHjx/PrFmzMDU1ValRkxo///xzjhiXV2halCgv8fLyomnTpvj5+WFhYcHLly+ZMGECDx484OHDh8ou5u7u7vTs2ZNPP/2U+Ph4Jk+ejK+vL3fu3MlUNeaRI0eyZ/8BdNxGoTA0Jfj4ShQKHez7LkyzWN6WLVt4/Pgxjo6ODBs2jBs3bijLBCTh5uZGuXLlVOKuTExMsvR1zmwRvcKO1hXsS4Nc8RTGxsqtHrQhnTw+Xu40vmIFBKb4UGBuDkOGQP/+eVpwL8M6OyYmsrhJVl1dICiI5HhRwRs3bhD3b3DdjRs30pyXk1WVCxOHDh2ib9++vHv3Dl1dXXx8fKhRowYTJ05k3rx5AAwdOpTo6Gi2bt2qPM/Ozg4rKyvs7e0ZO3Ysn3/+Offu3VN2FD969KjKdTZu3IidnR3Xrl1TBo9nRGhoKOvXr6f0F5OILVkNgKJtx/Jy3UhiXtzDsHiFVIN/+/XrB8CTJ0/SXd/ExCTdIo+akpWtEW3dkssu+Z2VlhlyNFNMkiA4WH7ldzp5YiIcPSqnkaf8HTAygn79YOhQeXsoj0m1u7pTEXQtLUQRQMFHicZi5/Tp06n+X6AZn332GeHh4dy4cYPatWtz5swZihYtipeXl3LOmTNnmDhxYqrnh4aGsnPnTgClVyeteYBK37KBAwfy5MkTlWsl59q1a8TFxRFtV0m5r6lv44SuhS0xL2Wxk53g323btrF161bs7e3p0KEDU6ZMUenXlRkyEy9TUDwfmSWzWWmFRvB9+CDH5sRqlnGUayS1dli8GO7eVT2mrw89esDw4Zlq7ZAbKCsd6+rKsTiiCKDgI0b85OcRlpaWVK9eHS8vL2rXro2Xlxfjxo1jxowZREREEBoaysOHD2nSpInKeSVKlAAg8t+YhM8//5wKFSqkeo3ExETGjh1Lw4YNqVy5snLcwcGBxJR1PZIRFBSEnr4BOkZmKuO6plYkRL5XGcts8G/v3r2VDWJv3rzJxIkT8fPzY+/evZlaJzmaFNHLac+HNgmGzGSlhX6ILfiCLzFRTt/+V8jnK3//Lbd2uH5ddTyptYOHB/z7O5vviCKAAoESjcVOly5dNF40Ow+ywkyTJk3w8vLim2++4dy5c8ydO5fffvuN8+fPExwcjKOjI2XLllU559y5c5iYmHDp0iXmzJnDqlWr0lx/1KhR+Pr6cv78eZXxuXPnZmibpo/tzAb/fvnll8r/V6lSBQcHB5o3b879Bw95p1Mky+Ihva2RnK7Hk5GHKK+FkKaC88SdIDZceFIgtrrSJCJCTiePj09zSm41vFTB11f25KT43QLk1g5jxoCLS85eM6sYG8six8ws47kCwUeCxmLH0tJS+X9Jkti3bx+WlpbUrl0bkLdCQkJCMiWKPjbc3NzYsGED//zzD/r6+lSoUAE3Nze8vLx4//69mlcH5B5hVlZWlC9fntevX9OjRw/Onj2rNs/Dw4NDhw5x9uxZpTdIU+zt7YmLi8XWIJ63sXrKh2NCZAi6pnKV1Zyqi5LUs6zjnN/5UOy/kvg56W3IyXo8GXmIvmzszIF/AvPUc6Kp4Nzn80I7elNlhfh4WeRERKQ7LTcbXgLpt3b47DO5IGAyL2q+ktSU0yj3sxEFgoKGxr5NT09P5atYsWJ0796dx48fs3fvXvbu3cujR4/o2bMnRYsWzU17CzRJcTuLFy9WCpsksePl5YWbm1u65yd5bvbt26cckyQJDw8P9u3bx6lTp1LtWZYRtWrVQl9fn9ZF3gDygzDu3XMSwt5g6FghR+uirNp7AoBQheqnziTxcPjmy2ynUudUPZ6MPEQSsPrsYzVhlXQvqaXr5wRJBfvS+k4okIvGBUem3SohueDTOkJD5YBfDYTO9AN31NojJDW8PHv/ddZteP4cJk2C9u3VhU7Nmv/V0clvoZPUlNPZWa7hI4SOQJAqWdrI3bBhA99++y26yWoy6OrqMn78eDZs2JBjxhU2ihQpQtWqVdm2bZtS2DRu3Jjr169z//79VD07yTExMWHYsGFMmzZN2WZj1KhRbN26le3bt2Nubk5QUBBBQUF8+PBBed6kSZPo379/mutaWloyZMgQtiyZxYhyHzCLCODd4SUYOlbAsHgFZV2UsV80VRFawcHB+Pj4cOfOHQD8/Pzw8fEhKCgIAH9/f2bNmsW1a9d48uQJ+/f/wfdjR2LoVBkDO1VRliQePHbcoNfaS4zZ6ZNujZ/0yKl6PBl5iNIi6V6+3+dLbHzasVJZJSkrDdKupdKxumaNcPOyYWmGxMbCs2dyEHI6MWaQccNLkBteZlosv3kDs2bJlY337lW1o2JFWLMGtm/Pdg+rbKOnB0WLQpkyciC0qHYsEKRLlsROfHw89+7dUxu/d+9euoGwAjluJyEhQSl2rK2tqVSpEvb29pQvXz7D8z08PLh79y67d+8GYOXKlYSGhuLm5oaDg4PytWvXLuU5gYGBGXajX7x4Me3bt2f+t1/yyPNb6lV2YdWm7SrF8vz8/JTZXgAHDhygRo0atGvXDoCePXtSo0YNZVyRgYEBJ06coFWrVlSoUIHRY8dh6Fofu65T07Qj5bMpK14STTwfDhpsyWVXCLyLjKXe3BO54uHJqGBfy0qapfrndcPSVElKJ3/6VM640oDMNLzUiJAQ+OknaNECtm5V7WHl7CzH6+zdC02a5EgPqyyTsgigCDwWCDRC46KCyRk/fjybN29m8uTJ1Pn3E87ly5eZN28e/fr1E0UFBanyh88Lxuz0yfR5WSkWmBRrA6nX49EkONfb/x291l7KtL0pUWh4vayQVnB0gSnAGB0te3Ji0hYuqXHi7itm/3k3w3nft6tIi4rF0p4QGSn3rlq/Xr21g6OjnF3VsWP+p2ybmsrxOKIIoECgQo4XFUzOokWLsLe356effiLw34qhDg4OTJgwgW+++SZrFgsKPVn1ImSlxk9O9K/KqJllZvh+ny8fYhOwtzTO0WyttLLStL5haWIivH0re1SyQLYbXsbEwM6dcjfy4BRxSzY2MGIE9OwJ6dS0ynUUiv+acuanHQJBISBLnp3khIWFARRoj4jw7OQNGXkbMmJpz+p0rF4809dM8nwUNTMECd5GxmicIn7UN5ARW6+nOyez5GWdG60srBgZKXtz0kknz4iERIleay9l2PByx7B6qt/jjFo7DB0qVz7Ow9YOaogigAItR5vqjuWqZwfkuB0vLy/8/f3p3bs3AC9fvsTCwgIzUd9BkArpeRs0ISueoSTPx1HfQL7d/U+qD/2MChTmNHlZ50aTAox5RkKCnE6ecrsoCyQ1vJx+4E6aniuVhpfptXYwNpZ7Vw0ZIouM/MLA4L/O46LtjkBL0coPUBqQJc/O06dPcXd3JyAggJiYGO7fv0+ZMmUYM2YMMTEx6Ra+00aEZydvSe2XRUehHpycRHbjS9KqlZP0kLQy0Sck6r+A1OQiqNH8U2lmZCkASxN9dBQKgiMz18JAa2Jm8oqwMDnTKSEhR5fNsM6OJMGZM3KAccqkCn19eatq+HCwtc1RuzKFKAIoKCCk97cUci82MT1y1bMzZswYateuzT///IONzX/xAp07d2bYsGFZWVKQC2iTqzE5qXkb3kfGMmp72gHFWY0vyahWDqAidOA/z8vYFmUzLE4YEhXHlkF1GPPbjXTr2qR2blZ7jRUo4uLkLauoqFxZvnE5O+qVKcofPi94GfIBRytjOlYvjoGeDly5Ird2SNm4WEcHOnWSg4+LZ25bNMdQKGRxI4oACgoIOV2ZPq/Jktg5d+4cFy9eVGtIWbp0aV68eJEjhgmyh7a7GlMLrF2pk72A4tTISq2cpF9czwtPNJof/CGWOZ2rpJr9lRFaVecmp3n/Xg5CzsXu5Kl5dq4ePs/Eu4exvn5F/YT8bu2goyNvUxUpIuJxBAWKnKxMnx9k6bctMTGRhFTc0c+fP8fc3DzbRgmyR043wcwrciO+JKtiQgJCPmjmqbEzN6K+i02q2V8Z8TY8hoRESSs/CWWZmBjZmxOdu0IuqYJy0s95qfcvGXz1AI2f3FCf3Lix3Nrhk09y1aY00dOTBY6lpaiNIyiQ5FRl+vwiS2KnVatWLFmyhDVr1gCgUCiIiIhg2rRptG3bNkcNFGSOgu5qTK/BZ1bIbtE8K2N9Qj/EpVurJqk4YXKxFhQWzaxDtzPc2pr1513WnX+sNR63bCFJ8O6d7NHJRW8OqFZQtg97y8DrB2nx8DK6Ka4r1aqFYvx4+LeHX55jaCiLHHNzEXQsKNDkVGX6/CLLdXbc3d2pVKkS0dHR9O7dmwcPHlC0aFF27NiR0zYKMkFBdzXmNNmtlTOooTNLTtzXuFZNcrFmrK+j0daWtnvcNCIqSvbmxGket5Qdbj4PIeHVa8bc+JN2986jn6jqab5v48S6TzvRa3xvapTKXvPaLCGKAAoKGRn9Lc2pZtG5RZbEjpOTE//88w+7du3in3/+ISIigiFDhtCnTx+MjY1z2kZBJijorsacJqvp7km/uB7NXClvb5alWKK0ChumpCB43NIkIUHOsvq33laeEBJCkZXL2PbHbowSVMVVgGUx1tfuyDnnGkgKHVpF5Y34AkQRQEGhJquFSrUlUSbTYicuLo4KFSpw6NAh+vTpQ58+fXLDLkEWKeiuxtwgSXRMP3CHoLD/REcRE33eR8Vl+IubnViipHM3XnjMrHTaGxRIj1t4uFw3J4fTydMkIkJu7bBhA6VT1OoJMrNmU832/FW2Hok6/zUo1rTScnZI0NHleqjESwyw09Gjjq0+uhmfJtAitOWBrO1ktjK9NiXKZFrs6OvrE53LgYeCrFPQXY25i+pXxFBPh+GNnTnwT2CGv7jZiSXS1VFQ1NxQo7kFwuMWHy9vWUVG5s31YmJgxw5YvVqttUOwsTlbq7flUMXPiNP9r/N3UgXlqiWscs8uQ0OOB8Yw9dRjAsP+ywbTpqxHQcZo0wO5IKDphz9tS5TJUlHBOXPmcP/+fdatW4deIUifLGxFBXOiCWZuk5efpDIqhLWid02KmBrkqi2aNhXdMayednt2QkLkdPLExNy/VlJrh+XLIShI9ZiFBY879mCUbhWi9Q1T/Tmf/nklubBgTvNvPM7RR6FaV2BNkDm0sUheYSCpNVB6BVlzqqhqrhYV/Pvvvzl58iR//fUXVapUwTRFH5m9e/dmZVlBDpETTTBzk7z8JKVJdtqsP+/keiXjAu9xi42VBUdeeHUTE+HIEVi2LPXWDgMGwODBOFta8l0qdXaKJq+gnFOkiMeRf668C2zWo6DgZ65qM9qYKJMlsWNlZUXXrl1z2hZBDqJVPZGSkVOuTU09Q9ryS6f1XcjTQpLkraPg4FxPJ0eSwMtLbu3g56d6LKm1w4gRULSocrhxOTsautpy83kI7yJjsTE1oGoJq5z7Ourp/devSve/SBxt+bkSZB3xPcw9tDFRJlNiJzExkYULF3L//n1iY2Np1qwZ06dPFxlYWkpO16zJLjn1SSozniFt+qXTdo+bGlFRcgBybOb6fmWJ9Fo7dO4Mo0al2dpBV0dBjZJFMnW5hEQpfYGUQX0cbfq5EmQN8T3MPbQxUSZTYmf27NlMnz6dFi1aYGxszLJly3jz5g0bNmzILfsEhYic+CSVWc+Qtv3SaavHTYW8TCe/dQuWLIHz59WPtWkDo0fneGuHdJuH1iwje3L+rY+TkChx5dE7te+Vtv1cCTKP+B7mHtq4bZ8psbN582Z+/fVXhg8fDsCJEydo164d69atQ0eUQBdkQHY/SWXFM6SNv3Ta5nFTIa/SyR8+hKVL4a+/1I999hmMG5crrR1StpgASFQoeBSrx9Azb1lW2gV3R1nopOdBbFnJPsNilQ7aHIMl0Mq/DYUFbdy2z5RCCQgIUGkH0aJFCxQKBS9fvsxxwwSFj+x+ksqMZyiJpF86+O+XLAmtjpVBFnfe/u/4w+cF3v7vSEjMxZiZ+Hh48QICA3NX6Dx7BhMnQvv26kKndm3Ytg3WrcsVoZO8xQRAvI4uwcYWPLMsxltTK+J09Zlx8A4JiZLSg5jy5y3Jg3j8TpDy5yotPq/moJU/VwKZgvy3oSCQtG1vb6n699ze0ihfstwy5dmJj4/HyEjVcH19feLyqES8oGCT3U9SWfUMZRQr07KSPd7+6lsV+Ume1v4IDZW3rXIznfz1a1i5EnbvVm8p8cknsienUaNc7R9183kIb8JjiNHTJ9TQjEgDY5XrJYnlS4/eaeRBPD+xGV82dmb12cepXm/N2cfUKFlE+2KxBEoKXBxdAUObtu0zJXYkSWLgwIEYGv5XIC06OpoRI0aopJ/nZOp56dKlefr0qdr4V199xYoVK3Bzc+PMmTMqx4YPH86qVatyzAZBzpBd12Z2PENp/dIdvxOkVg8irwqKpZVRllFcUo7VBYqNlUVIVFSO3E+qvH8ve2q2blVPWy9TRu5E3qpVnjTJfJ2ox0vzosTop1/g0dv/nUYexEv+7zjwT2C6a4nUZe1Hmx7IhRFt2bbPlNgZMGCA2ljfvn1zzJjU+Pvvv0lI5lb39fWlZcuWdOvWTTk2bNgwZs6cqXxvIprvaS3Z+SRVx9kaKxN9QtLpd2Rlop+mZyjlL93hmy/5avsNtXl5UeEzLc/NlHaVmPVn2l4FAI8d10m+o5VpcSZJsgh59y730skjImDTJtiwQf5/cooXBw8P+PxzObU7N9HRUdbHsdANI0Y/KONzNOyg5v3orUhdLiRoywNZkHtk6i+Np6dnbtmRJra2tirv582bh4uLC02aNFGOmZiYYG9vn9emZZqC1n8lt+zNzU9S8QkSCYlShmsdvhmIxw51oQO5X1AsPc/NV9uvZ3h+ytCdTImz6Gi51UNMTPrzskpSa4dVq2RBlZyiReU6OT165H6jzKT6OFZWsuBB823U+mWKsvy0vwYX0eznQqQuCwT5T4Hq9RAbG8vWrVsZP348imRu723btrF161bs7e3p0KEDU6ZMSde7ExMTQ0yyP/ZheZBiW9D6r+S2vVn5JHXlcXC6Xh2AiJh46s09wZzOVdK086hvYIaiIrc+lWeUUZYVNBJniYmyJyelAMkp4uLk1g4rVqTa2oFhw6BvX2VKd65hZCTXxzEzU9sa03QbtZ6LjWaiyMWG5acfZmhSVlOXC9qHI4FAmylQYmf//v2EhIQwcOBA5Vjv3r0pVaoUjo6O3Lx5k4kTJ+Ln55du3NDcuXOZMWNGHlgso20N0TJCW+3V9BNycGRcmnYmiY2cvqamZJRRllXSFWeRkXJsTm4kEiQmwuHDcmuHlLF1JibQvz8MGSILntzEzEwWORkUONV0G1UjUVRGM1GUldTlgvbhSCDQdrLUCDS/aN26NQYGBhw8eDDNOadOnaJ58+Y8fPgQlzSKkaXm2XFycsqVRqB52RAtJ9BmezVtpgn/2bnoi2q8jYxRfjK+8jhY4zUg5xtz/uHzgjE7fXJsvZQs7VmdjtX/rTSckCCLnPDwnL+QJMHp03JBwNRaO/TqBcOHq7R2yHGSxeOgr5/x/GRo4jXRRHDkRtNd0ZxSINCcXG0Emh88ffqUEydOZJjpVbduXYB0xY6hoaFKRlluUtD6r+SWvTnhks8o5iI1O/usv6wcc7A0om1lzWO7cqMoXGa2NFJ6FTK1fliYnE6eGzVzLl+W+1el1tqhSxe5tYOjY7Yvk2ZLBz09WeBYWirjcTKLJtuomsSW5XTqsmhOKRDkDgVG7Hh6emJnZ0e7du3Snefj4wOAg4N2fPIpaP1XcsPenHLJJ4+5yApBodGsv/BE4/m5UVBM0yDZKe0qMuvPuypfMx2FenByyvPqlDCH589zJ5385k3Zk3Phgvqxtm3l1g5lyuTIpVJr6WBZxIxRXT6lZR3nPElVh5wTRZpS0D4cCQQFhQIhdhITE/H09GTAgAHoJUtV9ff3Z/v27bRt2xYbGxtu3rzJuHHjaNy4MVWrVs1Hi/+joPRfSfK8PHil2ZaHpvbmdPxP0ifpyft8CY7MXIPKJBt0FPIuTFpeEx0FLO9VI1e2CjQNknWv7EDryg4qD9D3kTGM+jdVPrXzZjV2RDfgac6nk9+/L7d2OHFC/Zibm1wrp2LFHLtcypYOkfpGhBqZ8URhyJf77rPS1EzrtnFyKnW5oH04EggKCgVC7Jw4cYKAgAAGDx6sMm5gYMCJEydYsmQJkZGRODk50bVrV3744Yd8slSdgtB/JTXPS1pkxt7ccsm7V3agWYVi1Jt7guDIzAfdJnlH0tomsjDSQycXtwg03fpI7QG6Ukehdp6TqQ4z6xXFzVY3Z4XOs2fwyy9w4ID6up9+Klc9rlUr567Hfy0dEhQKwg1NCTM0JV73vz9ThX0bp6B8OBIIChoFKkA5t9A0wCmr5EYQY06RluclNTJrr6YBxVkNAk7r66oJgxuWZu+NF6mmsmd0nzmVEpzVdZTnhX3AMT6Kmhagm5PP/Vev/mvtEB+veuyTT2D8eGjYMFe2kq6/jGDg/geEG5ogKdKOx8npwHFtISlBIKMPR9qS0CAQ5DeFLkC5IKOt/Vf+396dhzdVpv0D/yZpkq5JWrqkhVKg7JQWWe2AiFAEWUXm98rivLi8eKGAAqPjMi7gvDO4vDOuqDOI4IyCO4KgzLAjTAEpFChghU7Zm2K3JN3SNnl+f4SE7DknOVm5P9fFddnkJDlPGnvuPM/93LenmRdX+J5voKfk3b2vXIzrl4HvTrou9e9p1knILcG+Ln1IxCIUZsYBYh3Qbv49HrvkIpGXr/p6YPVqc2sHx6KDgW7tcL0+zqVmHXSxnlswANG7jBOO3aIJiQYU7ARJOPZf4VrzZdEdPTGyZ6r1fLnOSARjSt72fdVoW/CHrWe85vKoFXKAARqd+yrCrhJBw6L+kNFo3mV1vRCmq0TetCQ5Ft2Ri9G907k9Z2MjsG6dubVDU5P9fZ07mxOPp00DJBKBBmEjKcm8s+p6g+H0JG55WNG8jBOuX44IiWQU7ARRuPVf4frtuFdGot0Fn+vMRiDyldwFWoW5nVBcUcspaXn28K6oaeLWLsHyHoXFlmC93lw35/p2csdEXosavQHLN5/G8mnwHPC0tppbO/z1r65bOzz6KPD//p/wrR3EYvO28eRkp95YkZDjFgzh+OWIkEhGwc5NjO/MC9+ZDaGn5L0FWlyDt26pCbzHHtItwR0d5iDHpqGmJZHXU/D1zu4KjOyZ5vz+trcDX39tbu1QXW1/n1IJ/M//BKa1g1RqDnAUCrf1cWgZ54Zw+3JESCTzrSIXiQqWb9HuLhsi3Cisx6Wn04pvT8PoUAjGMiWvVtoHF2plLK9lH0ug5RhwWAKtbWVVvAIYPmMHQrglWKsFzp936hx+4nKD3dKVIwbgF70BJy433LjRZAK+/dZcE+eFF+wDnfh4c5POHTuAhx8WNtCJjzcvh3XvbteY0x2hPjOEEGJBMzs3MT7foosran2e2fA0Jc8l/4frEtLeJ+/gvATCdwYh6FuC29vNDTVbWlzeXcuxxlBtU9uN1g6vv26umWNLKgXmzDG3dugk4CyCSHSjlYMPy2C0jEMIERIFOzc5rsmQ/s5suJqS55r/w3UJqeRCPa8Ahk8iaFBzSerrgZoajzVzOiVwCyByzp4Elq8GrlcWt5JIzK0dHn1UkNYOVjEx5tkbpdLvhGZaxiGECIWCnSjgb90XLt+ifZ3ZcHdufPJ/+ARa0wd15rWThesMQlBySQwG89JSq/fx5ndRIS1Jjhq9wWXw1e9aJRYc+xa9Vp9yvnPSJOCxx8zLSkKRy82zOElJQWvlQAghXFGwE+GE7Dvl6Vu0LzMb7s7N0veJ684mvoEW3yUQrjMIAdsSzBhQW2ue0eFY41MiFmHRHblYvvm0XfDVre4KHjqyGaMulDo/6I47gMcfF7S1AxISzEGOTY6PUEUXCSFEKFRBGYGvoBwo7mZHAlWZmU8laE/nxvUDZ6mSG25VZQW9mLe0mGdz2vj1+bKw1NmRXrmMeSXfoujcYYgd36Xhw82tHQYP9u0cHXnIxxGy6CIhhHhDFZSjXCjqvnCd2eCyc4sLy/JVuG1HFiSXxGg05+VotX49zWglw6gr2yD68guIrtffscrLM7d2+NWvhFlakkjM+Tgqlct8nFAVXaSZJEKINxTsRKhQ1X3hskTEtTKzN7bLV1FVVVanM1dBdgxO+KirM7d2+OQTiB1bO/TsaW7tUFQkTJAjk92oj+Pm+XwJvoUIUmgmiRDCBQU7ESpkdV/gfWbD39d0t7Mp4rcjt7WZiwM2N/v+HN5aOzz2GDB1qjCtHeLjzUFOQoLXQ/kG30IEKWHRvoMQEhEo2IlQQa/7wgOf1+S7LBWR25EZMycf19ZyTkB20toKrF9vbu3Q0GB/X1oa8MgjwrR2EIlu9KuSyzk/jE/wLUSQEhbtOwghEYMqKEcovhWAg4nrub075yaoktvaCly86LVujlvt7cCnnwLjxwOvvGIf6KhUwBNPANu3A3Pn+hfoSCRASop5O7pazSvQAbgHuKmJcp8qcTviM5NECCE0sxOhwi1p15dzm5iXiQl5Ebws5YnJdGM7uS+MRmDrVuDtt83Bkq34eOD++4EHHzTPwvhDJrtRBNCP/B6upQnAIEiuWSiXcQkhkYdmdiJYOPcQ4npulmWp6YM6ozC3U3QEOk1NwIULvgU6jJn7U919N/Dkk/aBjkxmDnJ27DDXy/En0LH0q+rWzRzs+JnIbAlwATjN6NkGuHy7zbsTzsu4hJDwQzM7ES6ck3bD+dwCwmg0JyDr9b49vrjY3L/q+HH72yUSYOZMc2uHTD8CWB/zcbjismOuuKKW03N5C1KC2r6DEBLxKNiJAuGctBvO5yYordacl+PLdvLjx81BTnGx831TpgCLF5tnYHwlkZiXqVQqc++qAPIW4AoVpITzMi4hJPxQsEM4o+JtLhgM5tkcN93JPSovB958E9i50/m+O+4w18rp29f3c+NQHycQPAW4QgYpUVV7iRASUNQuApHbLiKYqHibA0sCckMD/11WFy6YE4+3bHF+7PDh5qrHt9zi+7nxqI8TKkJ+nigIJ+TmxfX6TcEOKNjxJtg9uMJeY6N5Nqejg9/jqquBVauAr75yfuzAgeYgp7DQt1mYAOfjBAIFKYQQf1FvLCIIrsXbkuRS1DQZovui1d5uDnIcKxd7Y9PaAa5aOzz+uLmOji9BThDzcYR20+RzEUJCLrL+OpKg41q8be6aQ9bbom55izFzwFJXx2/JqrERWLvW3NrBsUVEly7mxGNfWzuEKB+HEEIiEQU7xCNfirJFVW+ipibzbE57O/fHtLaaZ3H+9jfhWztEQD4OIYSEGwp2Qizc8xZ8KcoWFb2JOC5ZGU0MJy43oLapDakyEfIP7YT4vXfNj7WlUgHz55vbOsTF8TuXCMzHIYSQcELBTghFwg4nb3VR3OFa9t8XAQ0QeSxZ7fv5Gt7ZXYFabQvGVhzG/SXfQqyvsT/In9YOEZyPQwgh4YT+goaIv52fgzUjZFsXxRdC9yYKaIDIY8lq38/XsHzTKfzqwnE8eGQTetRftbvfJJVBfN9c4OGHzQ02+aB8HEIIERQFOyHAdYeTuyWgYM8ITczLxMOju+Ov+yp5P1bI3kT+BohudXSYg5zGRk6HG40m7F23Gav2fYF+v5y3v08kxrbev8KW0TPwzrLJ/AJQyschhJCAoGAnBLjucHK1BBSwC74HRhPD5uNVvB+XKWBvIn8DRJcYMycQ19aaiwRyUVqK5pWv4vnSEqe7duYOw9ohU3FFmQEAOHG5Abd0Tfb8fCKReQYnOdm3hGVCCCFeUbATAlyXdhyPC8gFnwNvwZk70woyBTsPfwJEl1pazLM5jnVv3PnpJ3Nrh1274Jh5c6BrPj4cOh3/6dTF7vbapjb3zyeRmHNxVCrftp4TQgjhjIKdEOC6tON4nOAXfI58zbvZfLwKv5vYT5CAR6PzLUB0YjSaG3Zqtdxe+MIF4K23gK1bnRKWj2X2xgfD7sbpjFyXD+2U4GKmhvJxCCEk6CjYCQFfOz/7OiPkL1/zboQKvLaVVeEPW05xOtbjuer15tkcLp3JNRrg3XeBL790Ov5MWjd8MOxuHM3q6zJgEQFITZIjv4vqxo1xceYgJzGR0zgIIYQIRxzqE/Bk+fLlEIlEdv/62nSBbm1txcKFC9GpUyckJiZi5syZqK6uDuEZc2PZ4QTc6C9l4anzc2oCtxorQiYFAzeCM1/mIVwFXkYTQ3FFLTaVXkFxRS2MJvdbvC05SnVNnndIieAhR6ijA7hyBaiq8h7o1NUBL79sbt/w2Wd2x1cmZ+H5ogV4dPrTONq5n9tABwAW3ZFr/v0lJgJduwLZ2RToEEJIiIT9zM6AAQOwY8cO688xNvVGli5diq1bt+KLL76AUqnEokWLcM899+DAgQOhOFVeJuZl4r37BjvtqlK72VW1rawKyzef9vic7maE/GW7/VwE8Kq34xh48dlJ5ilHyZanABENDeZlK28JyHq9ubXD2rVOrR0MmZ3xf33uxK7c4TCJPX8/SE2SY9G4Xhg9tKd5Jkcq9XL2vrMtP5CaKAcYor8/GSGE+CDsg52YmBio1Wqn27VaLdasWYP169dj7NixAIC1a9eiX79+OHjwIG699dZgnypvE/MyMb6/2mu9HHc7sGx5vOALdK6ugjNP5+MYePHdScY1MTolQYY/zsizD5ZaW81LVq2uH2+pfFxfp0OfHZuR9fk/IHLM40lLAxYuxP4Bt2HHv855PY85v+qBB6fcAklKMuAlKPKXq6DRVrgVpySEkFAK+2Dn7NmzyMrKQmxsLAoLC7Fy5Up07doVJSUlaG9vR1FRkfXYvn37omvXriguLvYY7BgMBhhsduHodLqAjsETb52fuc5upCfJMGdEDgwdJhRX1Abkm71jcHa+phlv7PgZgP1sj6vAy5edZFxzj56b3O/GRZ1DAvK+n6/hvR3lGH5kF35zbCtSmx2OVanMxQDnzgViY5Fysd7j67dJpNDGJiJv1CBIUgPfxZtL8BtV/ckIIcRPYR3sjBgxAuvWrUOfPn1QVVWFFStW4LbbbkNZWRk0Gg1kMhlUKpXdYzIyMqDRaDw+78qVK7FixYoAnrlwuM5utJuA13ectf4cqG/2jsFZH3Uip6U4X3aScc09Uiuv95rSas2Bjoe8nH1nqnDgjY/w55JvkeXQ2qFZKscvv56DnCcW2eXX5HdRIS1Jjhq9wS7AaImRoyEuEQZprHkWq0fgAx2uwW9U9CcjhBCBhHWwc9ddd1n/Oz8/HyNGjEBOTg4+//xzxPFtpmjjmWeewbJly6w/63Q6ZGdn+3WugcJ1dqPOoaZLIL7Zu2pRwXUpzpedZJx3rWUlABcvul2yAgAwBuP2Hei2fCVG116xu6tNEoNv+o/BhoKJkKanYkN8Amwr30jEIiy6IxfLN5+GCIBeFgdtbBLaYqQBWz501w6ET82jQPYnI4SQSBLWwY4jlUqF3r1749y5cxg/fjza2trQ0NBgN7tTXV3tMsfHllwuhzxCukf7urOK6zd7rj22vCUWe7uY+lJbyFNitAiAiJnwvyMzILl00f0TMgb8+9/A669DcvIkutrcZRSJsbXPKPzjlkmoSbxe6VhvcFn5eHSfDDw3V4kXfriKXxo7rLe7Syj3h6f32tDBsdKzDaFLERBCSKSJqGCnsbERFRUV+M1vfoMhQ4ZAKpVi586dmDlzJgCgvLwcFy9eRGFhYYjPVDi+dh0HvH+z57ozSogWFb7WFnKXGN09luHFEWm4Xe2hxcKxY8Bf/gIcPmx3swki7Ow5DOsGT8VVZbrTw+wqH9tUOh7bS4Lbb8sLaANWb+/1kqJevJ9T6FIEhBASacI62HniiScwdepU5OTk4OrVq3jxxRchkUgwe/ZsKJVKPPTQQ1i2bBlSUlKgUCiwePFiFBYWhvVOLL7dyv3Z9m3h6ps91wCGa2Jxklzqcdszl+7p7paCxvdXIylWiuKKWoiM7RitAAanSt2/bz/9BLzxBrB7t9Nd+3MK8OHQ6ahM6ez2PDolyICYGHO3coXCbmeVt4Ryf3B5rzccvgi1IhbVOu/Bb6BKERBCSKQJ62Dn8uXLmD17Nmpra5GWloZRo0bh4MGDSEtLAwC8/vrrEIvFmDlzJgwGAyZMmIB33303xGftnq/dyt3NbqQkSL0W2wOcv9nz2RnFNbF47ppDXsdk6Z6++odK2NYRFIuA+bd1d/ke2L5nSYYmpDRrcShRhkV35GJ0b4dZmfPngbffBrZscT7RW2+FcclSvHW4BTV61/2wRAAUyYnIH9oXUAa/nQOX91qjM2BpUW+8seNnj8FvoEsREEJIJBExxnyZLIgqOp0OSqUSWq0WCoUiIK/hbibFchnishTkOCs0JCcZt7+22+vS0P6nxtpd8IorajF79UGv57xh/q24pm/F45+Wej3W8XUB5zHxfQ8sx8cY25Ha1IDYjja745dP628OeKqqzK0dvvrKeSdWfj6wbBlwfWlz38/XrMUZLefBADTL4qCNTcSb998asq3am0qvcHqv35w1CPIYMdXZIYTc9Lhev8N6ZidaCNWt3NUSiqcEXsv9/uyM8iXfw9WY+L4HRhPDis2noGzRQdWit2tVYTn+o63HMOrLEog3bADaHDqM9+4NPP44MG6c3QzN6N7pWD4NeGd3BTSN7dDFJkAvT0B6cgLeDHFwwCeJuzC3k90uOKqgTAgh7lGwEwSB7FbOt+0EwO+i6muCtOOY+L4HR05fhuTSRSQbO5yOTWhrwX+d2I5fl+2AuN1hSSo7G1i8GJgyxZxc7MLogdkYOTIPh2s7cK0xfIIDvkncgcwfIoSQaELBThAEuls511o3FlwCGFW8FKbriTX+JEhbxsT5PWhoAjTt0Fech9Qh0JF3tGHGqd2YfXwbFAb7/lVITwcWLgRmznTfjyohwdyvKj4eEgCFya4PCxVvW+0BysEhhBBfULATBL7UmOGLz7d8Lju8GprbMXfNIWvuB5++WLYsY+IytkRDM7K114AkhXlH1HUxxg5MLt+P3xzdik4t9q09OhRKxDyyAJgzB4h18RoiEZCUZA5yBKqtxHdHHR++zNQRQgjxjIKdIPC1xkwgcW3sabsVff9TY60X+ZR4GR779Bjqm13vBnMck6f3QHo9ATne2AZ9oxqAAvldVEhPkGLQsX2Yd3SLU2uHJmkstg6diJlvPQcokpxPQCwGlEpzkBMj3Mfc1x11fPCdqSOEEOIZ7cZCcHdjAa6XJ0LVsNFoYjj4n1os/OQoGlo8By6WXV3eOm5z3o3FGFStersEZBGA5VP7YfSF42h69c9IuHTe7rkNEim+GWBu7bDsv4ZjdO90awfz2qY2pKgSUTCwGyQqpeCdx4XYUUcIIUQ4tBsrzITr8oRELIJYJHIb6AD2ycPaljavHbfdjWliXiZWzRmMRRuOQtbWitRmrX1eDmMYeuUMsua/DFRXIsHmsR0iMb7rMwr/GDwJIrUay67X2dn38zW8s7sCl1oYGmKT0CLTIfPfdYK/p0LtqCOEEBJ8FOwEUbguT3BNHtboWvHqtp88BjopCVLsffIOyGJcz6okx0rQSV+PxDb7BOMB1RV46MdvcEvVz/YPEIlgmjIF5TP/G/GqDDybIEN+FxUkYhH2/XwNT31Xgbq4JBgUN/JxAtEENZA76gghhAQWBTtB5phIbDQxFFfUBjz48ZRUyzUxuq7R4DVBua6pHSUX6l1f8LVaNJaftQt0cmsv4cEjm/Criyedjx83DliyBOLevTEAwADb8cQn4PmjelQpUp0eFoiZlkDvqCOEEBI4FOyEkJDJrp6CGW+vY0ke9hbIXK5v9ni/hdMFv6UFuHYNMBiQGmf+yHVpqMb9R7/FuIofnR5/pHM/JD39BPrcOcr+DrHY3KsqORmHL+pwodl9B3ChZ1qCsaOOEEJIYFCwEyJ8O4l7C2aWbz4Njc4mF0gRi+XT+gMAp9d5fnI/PLr+mMdz3ny8itPYavQGbCq9gvS4GAxPNELS1Gi9L1/SgueKP8GYU/shYfbByun07lgz9G5c6luADUU2zVxjYsy7qpQ3ko6DPdMSjjvqCCGEcEPBTgjwTXb1NDMDAAtcdBLX6Fqx4OOjUMVLOb1OcoL3GjS1TW1ISZCivqndbd6OWAT8YctpKFsboWrVI8PStDM1BvjrXyFZvx7jHFo7/Cc5Cx8OnY5/5xQAIhGW35FrDuQsnceVSqemnMGeaaGCf4QQErko2AkBPsmu7nY/WWZm4mSuWyJYNLipg2P7OgcrarHjjIbTuc8Y1BkfHjjvthih3NCKTs0NkJrMDTmba+px4YWV6PhpN2JaWuzHoEzHB4OnYlfuUDCRGGlJcnNgNKCzOchRuO88HoqZFq476gJZdJAQQgh/FOyEAOfdT9oWvPrPcrczMwDQ3GZ0cS8/D370Iwwd7vNfbBX1V2NY9xSnC77c1A5Vkxbx13tVyTvaMKNsF2af+Kdza4eMDGDhQqTdPQOTq5twa1MbOiXIkN9TDUmnFHPFYy+4zrQAEDQB3NuOumAUHSTBRwEsIZGNgp0gsf1jWaM3eH8AgLqmNt7tGXzBJdCxnSmRiEU3LvjaZjRe0WD1t8cggrm1w5SffsB9x75zau3QmqhA7KJHYbx3Fk7UtKL2P/XolCDDHYO7m4Oc+Hhe5+1tpgUARr2yS/DAw11rDr55WCQyUABLSOSjYCcIvFUcdkUsAlTxMu8HBgmDfU6KRCxCYScJgDbsuNQKicmEonOHcH/Jt8hsrLV7bJM0Fp/nj8eWW8ZjTNce2Lm2BNqWDjTK4qCNTUKnHxuvXzj4BTuA80xLaqIcYMDOn6rx4YHzTscHKvCgooPRiQJYQqIDBTsB5u6PpTcmBjQ0t3k/MEgeHNntxh/15mbgl18AgwFgDD2OHsCar95Ctwb73VoGiRQbr7d20MUmAgC+OnYFTbJ4NChT0C4xdyf398JhmWnZVlaFJ7447jUfKhCBBxUdjD4UwBISPSjYCSBPfyy5SEmUe0zC5UIVL8Wf7h6IZzee9NgSwpvx/dVAe7s5yGlsBBgD9u8HXn8dPU6dsju2QyTG1r634R+3TEJtggqA+eKglydAG5uIDon9x06ICwefoDIQgQcVHYw+FMASEj0o2Akgb38svVErYt0m4XL1p7vzkJwgQ7fUeJRe0vp0HiJmQuPlKsBUZw5ySkqA118HfrQvCGiCCDt6Dse6IVNRpUgz3yYSQXc9yDGJ3e8c8+fC4WtQKWTgQUUHow8FsIREDwp2AmjHaW7buR05bpteUtQbaw9U8pqZSY6X4r+GdsEftp7xK+BStDZC1aLH6s11GDtKCclbbwJ79jgdx4rG47H0UTiVoAYAGEViaGMToYtNABNx7z7uy4XD16BSyMCDig5GHwpgCYkeFOwEyLayKqxxkSDrje226e2nNU6JzfFSCZrbvW8375WeiL/uq+T9+hZxba3o1GLuSt6loRoP7tgEyV9KnA8sLASWLYMoPx//9fM1/H5LuTnIkSe4rZHjieOFg8uWX74BUiACDyo6GH0ogCUkelCwEwCWZRUuxCJzMrKF7bZpVzkoXAIdADh8vp7TcY7kHW1IbtYhrsOA9MY6/PfRLZj4c7FTawcMGgQsWWIOdgBAJsPo2wbi9zk9sGLLGeh4zrS4unBw3fLL55t1IAMPrkUHSWSgAJaQ6CFijPma+xo1dDodlEoltFotFAqF389XXFGL2asPcjr23TmDkZwgs5u5MJoYbl25A3VNvicU8xVj7EByiw6JbS1Qtegwt/R7TDu9DzJTh/2BvXvDuGQJTvS8BTWNbajrECE+Mx2pmanWWRfLbMzV+mY88dUJcPmEiQC73VjuEo4tlxXbY40mhlGv7OKUyB2M+ihUgC66UJ0dQsIX1+s3zewEANdllQdHdsOkfPs/ltvKqvDsxrKgBTpikxHJLXokGZqQaGjGvSf+hV+X7UJch33hwypVBjKeexL7ew3DO3srcenkT2iITUKLLBaADsA5uwtAYW4nFFeAU6CTkiDFn2YMtAte+Gz59fQN3OKhkd1Q1F8dlMDDXdFBEpm8Vc0mhIQ/CnYCgOuySmdVHIwmZtdqwJeaPL4Qm4xQtTZC0dqIuHYDZpzajdnH/4mkNvvWDr8kqPDR4Ck4MPA2TFR0wbptlaiPS0SrwnmMVQ71crgGfc9PGWD3DdmXLb/ulpDoGzgRAgWwhEQ2CnYCwFtio8Uftp7B6h8qsXxaf4zvr/arJg9XYpMRytZGKAxNkHW0u23t0BCbiE8G3YVN/W5He4wULZDjLz+1wqBI9foallkXrkGf2iFw8nXLL30DJ4QQ4goFOwHAZVnFQqNrxYKPj2JpUW/e26f51N4RMROUrY1QtjYixmjE+HOHMM9Da4cv8orQIotFS4wc9fEKGGK4ta6wnXXxdTfL+ZpmF0c7cxVM0TdwQgghjijYCRB3yyru/G1fBa/nn39bd2w5UeX9uRmDwtAEVYseEpMRo88fwwNHNntt7dAaI0NdnAIGqZzXeVkcOFeD4d1TeO9mMZoYNhy+6PX51Qo5bfklhBDCCe3GgvC7sWwZTQwf7q/EH787I+jzZipj8fu7+qJab8De8mvYd85+hgaMIamtGaoWPWKMHRh2+TQeOvIN+tTYBxKOrR3aJFLUxSmuJx77f46WbfSOQV9KghQzBnV2Shp+c8fPeH3HWa/PfVdeBv67sDstUxFCyE2M6/Wbgh0ENtgBgP/7Zzne2X1O8Od1J8HQjORWPaTGDuRpzuF/fvwGBRr7AMLc2mEE1g2ZgipFGtolMaiPTUKTnH/ncW/ev2+wNZdm+2kNvim9irqmG01ObYOiBR8f5fXc4Z6ATNvQCSEkcGjreRhhQdlfBcS2tyKlRQd5Rzt61lzEQ0c24dZLZU7H7es2CGuHTMf5lCx0iCVoiEuCXhbvU8VjLp7++iTG91dD29KGtQfOO70blq7nyngp7+f2t2N6IFF9FkIICQ8U7ATYtrIqrD90IaCvEd/WAlWrHvKOdmQ3aPBAyWbc8R/n1g4/du6HNcPuRnlaN5hEIjTEJkEbmxiwIMeiobkdb+08i8+PXHJbO8dyHF9CdEwPBHdlBKq05oT0YNb9IYSQmx33Do0hsHLlSgwbNgxJSUlIT0/H3XffjfLycrtjxowZA5FIZPdvwYIFITpje5YLXn1zh/eDfZBgaEZn7TVkNNaha70Gv9v7EdZ+udwp0ClL74Glk5fhd5OW4Ke0btDGJuKSMgPauKSABzoWH+z/j18NST2x3QEWDrh0YV9z4Dxmrz6IUa/swrayKg9HEkII8VdYz+zs3bsXCxcuxLBhw9DR0YFnn30Wd955J06fPo2EhATrcfPnz8dLL71k/Tk+Xvi8E76MJoanvz4p/ALW9cRjZWsjpMYOJDebWztMPePc2qEipQvWDJ2G4q75gEgEvTwe9XEKGMUSoc/KqyYDt55e/vClY3og8OnCHs7LcIQQEi3COtjZtm2b3c/r1q1Deno6SkpKMHr0aOvt8fHxUKvVwT49j97ZddanZRl3RMyEJIM5yIkxGZFoaMK9J7ZjZtlOxHW02R17WZGOD4dOw54eQ8BEYjTK4tAQl4R2Cf+cmEjCpyFoIBOH+QRd4boMRwgh0SSsgx1HWq0WAJCSYl9f5ZNPPsHHH38MtVqNqVOn4vnnnw/p7I7RxLD2wHlBnstSDFDR2gQJMyG23YB7Tu3CrOP/cmrtcC0hGX8fPBnbev8KRrEEzVI56uOUaIsJjyAnUR6DRoPwS3ruihO6E+jEYT5BF+C6/UWko11ohJBwEjHBjslkwpIlSzBy5Ejk5eVZb58zZw5ycnKQlZWFEydO4KmnnkJ5eTm+/vprt89lMBhgMNxodKnT6dwe64vDlXVoaPFvVse2rYOYMUiN7Zhy5gfcV/o9Uly0dvh40F3YfL21g78FAYUmApChkKO13eT1OF+W/RicixPasr3wnq9pclnHx5I4vLSoFxaN7eXXhZlruxBH4bIM5y/ahUYICTcRE+wsXLgQZWVl2L9/v93tDz/8sPW/Bw4ciMzMTIwbNw4VFRXIzc11+VwrV67EihUrAnau209rfH6s5HqQk3Q9yBGbzK0d7i/ZArVDa4dGaSw+z78TX+aNQ4ssVtCCgEKbPbyr12KBvuY3qeKlMJkYiitqnWYSXF14PXl9x1lsOHwJy6f5fmHm0y7EFt8ZoXDkbhca5SYRQkIpIooKLlq0CJs2bcK+ffvQvXt3j8c2NTUhMTER27Ztw4QJE1we42pmJzs7W5CigtvKqngXxgMAqbEdqpZGJLQ1QwTz8tVtlcfwYMlm5DTYB0+tEik2DrgDnxZMgC42ER1iCerjFGgMQEFAf6nipHh55kAYOkx4/NNSr8c/NLIbvivT2AUnYhFg4vkpzVTGYlpBJv62r9KnIEoE+H1h5hpoWZbh9j81NqKXeowmhlGv7HI73mgZJyEkfERFUUHGGBYvXoyNGzdiz549XgMdACgtLQUAZGa6v0jJ5XLI5cIv8Vi2HPMhbzdA1apHfPv14IsxDLt8Cv9zZBN6O7R2aBdLsKXvbfj4lkmoi1cGtVaOr+7/VTdMzMtEcUWt94MBFPVX49nJ/e3yPYbkJOPH83VY+MlRzsuDVdpW/HVfpT+n7nfSsG0X9u2nNfjwwHnOPcIikbddaNGYm0QIiQxhHewsXLgQ69evx6ZNm5CUlASNxjzDoVQqERcXh4qKCqxfvx6TJk1Cp06dcOLECSxduhSjR49Gfn5+0M+Xz5bjuLZWqFr1iLXZSTWw6iweOvINCjT2rSWMIhG297wVHw2eAo0iFQCgl8ejLk4BUwi2kfPx2ZFLWDyuF+cO6CYTw5YTV5GeFIsp+VnWAEAsEvmdB8WHUBdmSxf2wtxOGN49xWmmRx1FuSxcc46iJTeJEBI5wjrYee+99wCYCwfaWrt2Le6//37IZDLs2LEDb7zxBpqampCdnY2ZM2fiueeeC8HZcvgjzhgSr1c7lhpv7Ery3NrhFnw4dBouJGcBANolMfglXhU2ycfe2AYMnjqgMwAt7UbMXXPIerttUmuoLpBCvq7tTE807lLimnMUDblJhJDIEtbBjrd0ouzsbOzduzdIZ+Oduz/ijjVyLLIbNHjwyGaMqfTc2gEwBwP1cYqwXrJyxxIwTMzLxHv3DXaa3VDFS1Hf3O5Ul8iS1Lpqzi2o0RsQCkJfmC0zPdGI6+wd1xIBhBAilLAOdiLNkJxkp2RaRWsjklt0ENsEbhn6Wsw7ugV3ni2GxCGgK8vIxQdDp+N4Vh/rbU3SWNTFK9Ehicxfl23A4Di7kZoox28/L3X5OMs7s2jDMd4Jyv4K9IU5GuvQeNqFFk25SYSQyBOZV88wVXKh3uminGRotgY6yc063Ff6Haac+cGptcO5lC74YNjdOJSdZ525aZNIURuvQKs0MNP+UokI7UZuUcTzk/shJUGGA+dq8eXRy5we4y5gsJ3dKK6ohUbnedbG30DH3YV3cn4mtpxw7ksV6AtzNNehcTd7F025SYSQyEPBjoDc5XckGpow68S/cE/ZLqfWDpcU6Vhr09oBAIwiMerjFdDLE1w9nWC4BjqqOCnuH9kdErEIMwZ3QXZKnNeaOYD3Yn9A4JJVLa/48Oju2Hy8yu2Fd0q+c+ARyAvzzVCHJtpzkwghkYeCHQE55nfEt7Xgv49uwdzS75HY1mJ3X3VCMv4+eAq29S6021HVKItHbXx47bJ6YGQ3uwtVt1RuQdiDI7t5vXD7mxOTFBuDWcOyseWE+4DmdxP7ub3w+nNh5rsU5akberT1yIrm3CRCSOShYEdAlgTNKm0r5pR+j6U/fIK05ga7Y+pjk/DJLXdhc9/RaLfpWdUulqAmQRWwJStfJcolWDS2l91tXAOU8f29N2f1tbWChb61Ax/8UIlVcwYjOUHmMvDwduH15cLsy1IU1aEhhJDQEIf6BKKJRCzCs5P6AQC61lfZBTqNsjh8MHQ65sz6X3yVN84a6DCYe1tdVmaEXaADAPcOzXaaZbAEKO7mHkQwX/i9JfdaZkYm5al9bhVh8YetpzG8ewqmD+qMwtxOAZ0ZsSxFOQYulqWobWXOeUAA1aEhhJBQoZkdgV3TmS9U79/6a8wt/R5SkxFfDRiLz/LvhD7WfvnHECNFTXxy2HQld6XIZnbGdtlm1rBsvL7jrM+7blzNjDjuZOPaJiKYMyL+LEVRHRpCCAkNCnYEdqGuGQDQEKfA4mlPoS5O4RTkmEQi1MUrA56A7A/HnVSughNVvDlIs62PwyW5112SrmUX/oMju2F8fzXqm9qwcL3zce4cOPdLwBNi/VmKojo0hBASGhTsCCwn5UYzzj25Q9FZew0y441goFEWh9p4ZVglIDtynJ1xF5xYgpwl43qhe1oCpyCjrcOEZzeWeZwZ+b5Mg99PNr/2e+LBeHbjSdQ1eW8V8c7uCut/B2ortz9LUVSH5uYUjTWVCIk0lLMjsHSF6yWIdkkMqpI64ZfElLAOdADz7IJlC7SnZRuLt3adhfR6kq+3patbV+5AXVOb22NsZ0YA826pg88UISVBxmsM3vJnfOXvUpSlDo1aaX+/7XtOose2siqMemUXZq8+iMc/LcXs1Qcx6pVdgn8uCSGe0cyOgIwmhj9uPWN3m0kkQn1cEhpik8K6zcOUgWqMH6B2+ubJpbmpiQGPrj+G98Uitxdrd7ND7tjOjMhixPjTjDw88vFRAOD0HIHayi3EUhTVobk53Aw1lQiJFDSzI6DDlXXQ6OwDg+rEFDTEKcI60AGAubd2c7mTic/OoBXfnobRRUYxl9khRzV6g91zuZsR8cRxlkgIlqUoAE670fgsRVm2uwdj9xgJPm+J7ID7/18IIcKjYEdArgKDcF+ysnAX1PDZGeQusOAyO+ToD1vPOE33T8zLxN4n7+C9pCX0Vm5aiiLe8ElkJ4QEHi1jCSiStww7nrslqVKjbUFKgsxjno0tV4GFr8GGRtuKBR8fte7OGt49BSUX6jmfi0Ugfi+0FEU8oZpKhIQXCnYENLx7ChJkEjS1GUN9KryIRUB9041mnK62mXPlKrDwNdiwTPB/eOA8PjxwHpnKWEzK816V2SLQW7mpJQJxh2oqERJeKNgRkEQswsDOShyMsKlpEwMWrj+G967PSvBJJLblrmqyvy0hLDTaVqw5cJ7XY2grN/GXL1vHqaYSIeGFgh0BGU0MpZcaQn0aPlu++RQAkc8ByaxhXV3e7qm+DB+Wx4lF5gKEnp4nUHV2yM3Flx5oANVUIiTcUIKygA5W1KK1wxTq0/AJA6DRGZx2k/Hx+o6f3dYQcZfUm5LAv1WGid3YWu7K0qJe2P/UWEEDHaOJobiiFptKr6C4opZ20dwEfO2BZkGJ7ISEDxFj7Kb/q63T6aBUKqHVaqFQKHx+nv/75092VXxvRpYAxN0fc8clgSE5ybj9td28l7geHNkN35dpeH/j5stoYnhn11msPXAeDS03qjjTzFF0M5oYRr2yy23emmUZav9TY73OzlAFZUICh+v1m5axBBX6P2BqhRwzB3fBqj2hCbq8FfNzldTryxLX+P5q/H5y/4BeRLaVVeHpr0/a9f6yoMJw0c2fHmiOKJGdkNCjZSwBhcMfNI3OwLsODXD9m6pCDrWbdhd88K0hwqdgoAg3EqEDWZjPsoThKtABqDBctKOt44REFwp2BHRrj06Qx4T+LU1JlCNTGct5nsly3PJpAzB9kHCzFHwuBBPzMrH/qbFYWtTL43EMgU/s5FrxmQrDRS/aOk5IdAn9lTmKSMQi9FMnhfo0oFbEum1p4PL46wmT4/ursfm4cA0KXRUq9Jbk++mPlzw+Z3K8FOP7c6+14wu+FZ/p2330sWwdd/f/j+0MIyEk/FHOjoCMJoZTV7Uhe33b2h0SsQjv3TfY5bbZ5yf3R3KCzCnXpbiiltNFPiVBBhNj0Da3c64hwmULL5cgo765nVOehD/4Bi/07T760NZxQqILBTsCOlhRi/YQ7jx3XOLh29KA60X++cn9ECeTcL4QcO3+HC55EnyCF/p2H70suWSOQbqaduIREnEo2BFQ8X9qQvr6rpZ4+OwE4XqRVyvjUJjbidOFwFv3Z9udW+GSJ8Gn4jN9u49u1AONkOhAwY6gQvsH0N8lHr4l7rlcCPhs4bW8vqfjgzGTwqXisypeipfvGUjf7m8CtHWckMhHCcoCCoc/iP4s8Vgu8oBz2OYuT8Hb9m8+S1MSsQjTCjwHD9MKMoPyrdrddnhVvBRLi3qj5LnxFOgQQkiEoJkdAd3aoxMSZGI0tYUuccffJR6h8xT4LE0ZTczrbrDNx6vwu4n9ghbw0BIGIYREPgp2BCQRi/Darwvw6PpjQX9tIbsoC3mR57M0xmU3FteqtUKhJQxCCIl8tIwlsEn5WZh/W7egvmYgtsIKVZ2Yz9JYuOzGIoQQEl1oZicAfj95AAARVv9QyetxyfFSMMCuRYFjXZzzNc3YcPiiXXfycN8Ky3VpLFx2YxFCCIku1PUcwnU9d/TdiSr87qsTaDR02N0uEQHTCzLxx3sKUHqpwW6pCIDX5aNI7aLs7bwtnaa9LXlx6TRNCCEk+nG9fkdNsLNq1Sq89tpr0Gg0KCgowNtvv43hw4dzemwggh3LhV2ja0WN3oD65jaIRUBhj1TcKnDTymhiKUAIuC5WSF3GCSGEWHC9fkfFMtZnn32GZcuW4f3338eIESPwxhtvYMKECSgvL0d6enrQz8dTa4SRvVKDfj6RhKrWEkIIEVpUzOyMGDECw4YNwzvvvAMAMJlMyM7OxuLFi/H00097fbyQMzvuWiPQzAQ/kbpURwghJHhumpmdtrY2lJSU4JlnnrHeJhaLUVRUhOLiYpePMRgMMBgM1p91Op0g58KnNQJduD2jLd+EEEKEEvFbz2tqamA0GpGRkWF3e0ZGBjQajcvHrFy5Ekql0vovOztbkHPh0xqBEEIIIcER8cGOL5555hlotVrrv0uXLgnyvFQnhhBCCAk/Eb+MlZqaColEgurqarvbq6uroVarXT5GLpdDLpcLfi5UJ4YQQggJPxE/syOTyTBkyBDs3LnTepvJZMLOnTtRWFgY1HOxtEZwl40jQnC6dhNCCCHkhogPdgBg2bJlWL16NT766COcOXMGjzzyCJqamvDAAw8E9Tx86RpOCCGEkMCK+GUsALj33nvxyy+/4IUXXoBGo8GgQYOwbds2p6TlYKA6MYQQQkh4iYo6O/4KZAVlqhNDCCGEBMZNU2cnXFGdGEIIISQ8REXODiGEEEKIOxTsEEIIISSqUbBDCCGEkKhGwQ4hhBBCohoFO4QQQgiJahTsEEIIISSqUbBDCCGEkKhGwQ4hhBBCohoFO4QQQgiJalRBGYClY4ZOpwvxmRBCCCGEK8t121vnKwp2AOj1egBAdnZ2iM+EEEIIIXzp9XoolUq391MjUAAmkwlXr15FUlISRCLhmnXqdDpkZ2fj0qVLgjUYjRQ389gBGj+Nn8ZP4785xx/ssTPGoNfrkZWVBbHYfWYOzewAEIvF6NKlS8CeX6FQ3HQfeIubeewAjZ/GT+On8d+c4w/m2D3N6FhQgjIhhBBCohoFO4QQQgiJahTsBJBcLseLL74IuVwe6lMJupt57ACNn8ZP46fx35zjD9exU4IyIYQQQqIazewQQgghJKpRsEMIIYSQqEbBDiGEEEKiGgU7hBBCCIlqFOwEyKpVq9CtWzfExsZixIgROHz4cKhPKSCWL18OkUhk969v377W+1tbW7Fw4UJ06tQJiYmJmDlzJqqrq0N4xv7Zt28fpk6diqysLIhEInzzzTd29zPG8MILLyAzMxNxcXEoKirC2bNn7Y6pq6vD3LlzoVAooFKp8NBDD6GxsTGIo/CNt7Hff//9Tp+FiRMn2h0TqWMHgJUrV2LYsGFISkpCeno67r77bpSXl9sdw+XzfvHiRUyePBnx8fFIT0/Hk08+iY6OjmAOxSdcxj9mzBinz8CCBQvsjonE8b/33nvIz8+3FsorLCzE999/b70/mn/vgPfxR8LvnYKdAPjss8+wbNkyvPjiizh69CgKCgowYcIEXLt2LdSnFhADBgxAVVWV9d/+/fut9y1duhTffvstvvjiC+zduxdXr17FPffcE8Kz9U9TUxMKCgqwatUql/e/+uqreOutt/D+++/j0KFDSEhIwIQJE9Da2mo9Zu7cuTh16hS2b9+OLVu2YN++fXj44YeDNQSfeRs7AEycONHus7Bhwwa7+yN17ACwd+9eLFy4EAcPHsT27dvR3t6OO++8E01NTdZjvH3ejUYjJk+ejLa2Nvz73//GRx99hHXr1uGFF14IxZB44TJ+AJg/f77dZ+DVV1+13hep4+/SpQtefvlllJSU4MiRIxg7diymT5+OU6dOAYju3zvgffxABPzeGRHc8OHD2cKFC60/G41GlpWVxVauXBnCswqMF198kRUUFLi8r6GhgUmlUvbFF19Ybztz5gwDwIqLi4N0hoEDgG3cuNH6s8lkYmq1mr322mvW2xoaGphcLmcbNmxgjDF2+vRpBoD9+OOP1mO+//57JhKJ2JUrV4J27v5yHDtjjM2bN49Nnz7d7WOiZewW165dYwDY3r17GWPcPu/fffcdE4vFTKPRWI957733mEKhYAaDIbgD8JPj+Blj7Pbbb2ePP/6428dE0/iTk5PZBx98cNP93i0s42csMn7vNLMjsLa2NpSUlKCoqMh6m1gsRlFREYqLi0N4ZoFz9uxZZGVloUePHpg7dy4uXrwIACgpKUF7e7vde9G3b1907do1Kt+LyspKaDQau/EqlUqMGDHCOt7i4mKoVCoMHTrUekxRURHEYjEOHToU9HMW2p49e5Ceno4+ffrgkUceQW1trfW+aBu7VqsFAKSkpADg9nkvLi7GwIEDkZGRYT1mwoQJ0Ol0dt+SI4Hj+C0++eQTpKamIi8vD8888wyam5ut90XD+I1GIz799FM0NTWhsLDwpvu9O47fItx/79QIVGA1NTUwGo12v1QAyMjIwE8//RSiswqcESNGYN26dejTpw+qqqqwYsUK3HbbbSgrK4NGo4FMJoNKpbJ7TEZGBjQaTWhOOIAsY3L1u7fcp9FokJ6ebnd/TEwMUlJSIv49mThxIu655x50794dFRUVePbZZ3HXXXehuLgYEokkqsZuMpmwZMkSjBw5Enl5eQDA6fOu0Whcfj4s90UKV+MHgDlz5iAnJwdZWVk4ceIEnnrqKZSXl+Prr78GENnjP3nyJAoLC9Ha2orExERs3LgR/fv3R2lp6U3xe3c3fiAyfu8U7BC/3HXXXdb/zs/Px4gRI5CTk4PPP/8ccXFxITwzEmyzZs2y/vfAgQORn5+P3Nxc7NmzB+PGjQvhmQlv4cKFKCsrs8tPu5m4G79t/tXAgQORmZmJcePGoaKiArm5ucE+TUH16dMHpaWl0Gq1+PLLLzFv3jzs3bs31KcVNO7G379//4j4vdMylsBSU1MhkUicMvGrq6uhVqtDdFbBo1Kp0Lt3b5w7dw5qtRptbW1oaGiwOyZa3wvLmDz97tVqtVOiekdHB+rq6qLuPenRowdSU1Nx7tw5ANEz9kWLFmHLli3YvXs3unTpYr2dy+ddrVa7/HxY7osE7sbvyogRIwDA7jMQqeOXyWTo2bMnhgwZgpUrV6KgoABvvvnmTfN7dzd+V8Lx907BjsBkMhmGDBmCnTt3Wm8zmUzYuXOn3fpmtGpsbERFRQUyMzMxZMgQSKVSu/eivLwcFy9ejMr3onv37lCr1Xbj1el0OHTokHW8hYWFaGhoQElJifWYXbt2wWQyWf9ARIvLly+jtrYWmZmZACJ/7IwxLFq0CBs3bsSuXbvQvXt3u/u5fN4LCwtx8uRJu6Bv+/btUCgU1iWBcOVt/K6UlpYCgN1nIFLH78hkMsFgMET9790dy/hdCcvfe1DSoG8yn376KZPL5WzdunXs9OnT7OGHH2YqlcouEz1a/Pa3v2V79uxhlZWV7MCBA6yoqIilpqaya9euMcYYW7BgAevatSvbtWsXO3LkCCssLGSFhYUhPmvf6fV6duzYMXbs2DEGgP3lL39hx44dYxcuXGCMMfbyyy8zlUrFNm3axE6cOMGmT5/OunfvzlpaWqzPMXHiRHbLLbewQ4cOsf3797NevXqx2bNnh2pInHkau16vZ0888QQrLi5mlZWVbMeOHWzw4MGsV69erLW11fockTp2xhh75JFHmFKpZHv27GFVVVXWf83NzdZjvH3eOzo6WF5eHrvzzjtZaWkp27ZtG0tLS2PPPPNMKIbEi7fxnzt3jr300kvsyJEjrLKykm3atIn16NGDjR492vockTr+p59+mu3du5dVVlayEydOsKeffpqJRCL2r3/9izEW3b93xjyPP1J+7xTsBMjbb7/NunbtymQyGRs+fDg7ePBgqE8pIO69916WmZnJZDIZ69y5M7v33nvZuXPnrPe3tLSwRx99lCUnJ7P4+Hg2Y8YMVlVVFcIz9s/u3bsZAKd/8+bNY4yZt58///zzLCMjg8nlcjZu3DhWXl5u9xy1tbVs9uzZLDExkSkUCvbAAw8wvV4fgtHw42nszc3N7M4772RpaWlMKpWynJwcNn/+fKcAP1LHzhhzOXYAbO3atdZjuHzez58/z+666y4WFxfHUlNT2W9/+1vW3t4e5NHw5238Fy9eZKNHj2YpKSlMLpeznj17sieffJJptVq754nE8T/44IMsJyeHyWQylpaWxsaNG2cNdBiL7t87Y57HHym/dxFjjAVnDokQQgghJPgoZ4cQQgghUY2CHUIIIYRENQp2CCGEEBLVKNghhBBCSFSjYIcQQgghUY2CHUIIIYRENQp2CCGEEBLVKNghhBCORCIRvvnmm1CfBiGEJwp2CCFhqbi4GBKJBJMnT+b1uG7duuGNN94IzEkRQiISBTuEkLC0Zs0aLF68GPv27cPVq1dDfTqEkAhGwQ4hJOw0Njbis88+wyOPPILJkydj3bp1dvd/++23GDZsGGJjY5GamooZM2YAAMaMGYMLFy5g6dKlEIlEEIlEAIDly5dj0KBBds/xxhtvoFu3btaff/zxR4wfPx6pqalQKpW4/fbbcfTo0UAOkxASJBTsEELCzueff46+ffuiT58+uO+++/Dhhx/C0sZv69atmDFjBiZNmoRjx45h586dGD58OADg66+/RpcuXfDSSy+hqqoKVVVVnF9Tr9dj3rx52L9/Pw4ePIhevXph0qRJ0Ov1ARkjISR4YkJ9AoQQ4mjNmjW47777AAATJ06EVqvF3r17MWbMGPzxj3/ErFmzsGLFCuvxBQUFAICUlBRIJBIkJSVBrVbzes2xY8fa/fy3v/0NKpUKe/fuxZQpU/wcESEklGhmhxASVsrLy3H48GHMnj0bABATE4N7770Xa9asAQCUlpZi3Lhxgr9udXU15s+fj169ekGpVEKhUKCxsREXL14U/LUIIcFFMzuEkLCyZs0adHR0ICsry3obYwxyuRzvvPMO4uLieD+nWCy2LoNZtLe32/08b9481NbW4s0330ROTg7kcjkKCwvR1tbm20AIIWGDZnYIIWGjo6MDf//73/HnP/8ZpaWl1n/Hjx9HVlYWNmzYgPz8fOzcudPtc8hkMhiNRrvb0tLSoNFo7AKe0tJSu2MOHDiAxx57DJMmTcKAAQMgl8tRU1Mj6PgIIaFBMzuEkLCxZcsW1NfX46GHHoJSqbS7b+bMmVizZg1ee+01jBs3Drm5uZg1axY6Ojrw3Xff4amnngJgrrOzb98+zJo1C3K5HKmpqRgzZgx++eUXvPrqq/j1r3+Nbdu24fvvv4dCobA+f69evfCPf/wDQ4cOhU6nw5NPPunTLBIhJPzQzA4hJGysWbMGRUVFToEOYA52jhw5gpSUFHzxxRfYvHkzBg0ahLFjx+Lw4cPW41566SWcP38eubm5SEtLAwD069cP7777LlatWoWCggIcPnwYTzzxhNNr19fXY/DgwfjNb36Dxx57DOnp6YEdMCEkKETMcSGbEEIIISSK0MwOIYQQQqIaBTuEEEIIiWoU7BBCCCEkqlGwQwghhJCoRsEOIYQQQqIaBTuEEEIIiWoU7BBCCCEkqlGwQwghhJCoRsEOIYQQQqIaBTuEEEIIiWoU7BBCCCEkqlGwQwghhJCo9v8BXqKVDlxhe9kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener predicciones del modelo\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.view(-1).tolist())  # Asegurarse de que las predicciones sean un vector\n",
    "    return predictions\n",
    "\n",
    "# Obtener predicciones del modelo en los datos X1\n",
    "y_pred_all_cnn = get_predictions(model, data_loader)\n",
    "\n",
    "# Convertir los datos reales y predichos a NumPy arrays de tipo flotante\n",
    "y1_numpy = np.array(y1_tensor.numpy().flatten(), dtype=float)\n",
    "y_pred_all_cnn = np.array(y_pred_all_cnn, dtype=float)\n",
    "\n",
    "# Verificar si hay valores NaN o no numÃ©ricos y tamaÃ±os inconsistentes\n",
    "if np.isnan(y1_numpy).any() or np.isnan(y_pred_all_cnn).any():\n",
    "    raise ValueError(\"Los datos contienen valores NaN o no numÃ©ricos. Por favor, lÃ­mpialos antes de continuar.\")\n",
    "if len(y1_numpy) != len(y_pred_all_cnn):\n",
    "    raise ValueError(f\"Los datos tienen tamaÃ±os incompatibles: {len(y1_numpy)} vs {len(y_pred_all_cnn)}.\")\n",
    "\n",
    "# Calcular las mÃ©tricas de evaluaciÃ³n\n",
    "r2_all_cnn = r2_score(y1_numpy, y_pred_all_cnn)\n",
    "mse_all_cnn = mean_squared_error(y1_numpy, y_pred_all_cnn)\n",
    "rmse_all_cnn = np.sqrt(mse_all_cnn)\n",
    "mae_all_cnn = mean_absolute_error(y1_numpy, y_pred_all_cnn)\n",
    "pearson_corr, _ = pearsonr(y1_numpy, y_pred_all_cnn)\n",
    "\n",
    "# Calcular BIAS\n",
    "bias_all_cnn = np.mean(y_pred_all_cnn - y1_numpy)\n",
    "\n",
    "# Calcular PBIAS\n",
    "pbias_all_cnn = 100 * np.sum(y_pred_all_cnn - y1_numpy) / np.sum(y1_numpy)\n",
    "\n",
    "# Calcular NRMSE (Normalized RMSE) usando la media de los valores reales\n",
    "nrmse_all_cnn = rmse_all_cnn / np.mean(y1_numpy)\n",
    "\n",
    "# Calcular RPIQ\n",
    "rpiq_all_cnn = calculate_rpiq(y1_numpy, y_pred_all_cnn)\n",
    "\n",
    "# Calcular wR2\n",
    "wr2_all_cnn = calculate_wr2(y1_numpy, y_pred_all_cnn)\n",
    "\n",
    "# Graficar el scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y1_numpy, y_pred_all_cnn)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title(\"Actual vs Predicted (CNN)\")\n",
    "\n",
    "# Agregar la lÃ­nea de regresiÃ³n\n",
    "sns.regplot(x=y1_numpy, y=y_pred_all_cnn, scatter=False, ax=ax, color='red')\n",
    "\n",
    "# Anotar las mÃ©tricas en la grÃ¡fica\n",
    "ax.annotate(f'R2 Score: {r2_all_cnn:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'MSE: {mse_all_cnn:.2f}', xy=(0.05, 0.90), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'RMSE: {rmse_all_cnn:.2f}', xy=(0.05, 0.85), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'MAE: {mae_all_cnn:.2f}', xy=(0.05, 0.80), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'Pearson Corr: {pearson_corr:.2f}', xy=(0.05, 0.75), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'BIAS: {bias_all_cnn:.2f}', xy=(0.05, 0.70), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'PBIAS: {pbias_all_cnn:.2f}%', xy=(0.05, 0.65), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'NRMSE: {nrmse_all_cnn:.2f}', xy=(0.05, 0.60), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'RPIQ: {rpiq_all_cnn:.2f}', xy=(0.05, 0.55), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "ax.annotate(f'wR2: {wr2_all_cnn:.2f}', xy=(0.05, 0.50), xycoords='axes fraction', ha='left', va='top', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try to find the best solution using gridsearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.88628357433882, Test Loss: 43.13532826807592\n",
      "Epoch [20/50], Train Loss: 41.389534240472514, Test Loss: 40.093665878494065\n",
      "Epoch [30/50], Train Loss: 37.985344896160186, Test Loss: 36.83358036388051\n",
      "Epoch [40/50], Train Loss: 34.23518797452333, Test Loss: 33.10520590745009\n",
      "Epoch [50/50], Train Loss: 31.041834246525998, Test Loss: 32.09937638121766\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.817010873263, Test Loss: 43.71157252324092\n",
      "Epoch [20/50], Train Loss: 41.31620135072802, Test Loss: 40.54435739888773\n",
      "Epoch [30/50], Train Loss: 37.586797645443774, Test Loss: 34.95192651624804\n",
      "Epoch [40/50], Train Loss: 34.205221119865044, Test Loss: 34.79860501475149\n",
      "Epoch [50/50], Train Loss: 31.513725874853915, Test Loss: 31.46710274436257\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.839284928118595, Test Loss: 43.87594807612432\n",
      "Epoch [20/50], Train Loss: 42.65181105566806, Test Loss: 38.569103984089644\n",
      "Epoch [30/50], Train Loss: 38.704103438580624, Test Loss: 40.588974742146284\n",
      "Epoch [40/50], Train Loss: 35.90200964505555, Test Loss: 33.884951901126215\n",
      "Epoch [50/50], Train Loss: 32.60685047087122, Test Loss: 32.62175671465985\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.0605480881988, Test Loss: 44.40055733222466\n",
      "Epoch [20/50], Train Loss: 41.598904937994284, Test Loss: 39.746384608281126\n",
      "Epoch [30/50], Train Loss: 37.397014711723955, Test Loss: 36.24359579210157\n",
      "Epoch [40/50], Train Loss: 34.10930001931112, Test Loss: 34.31745306237951\n",
      "Epoch [50/50], Train Loss: 32.227812357417875, Test Loss: 33.49367614845177\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.749697012979475, Test Loss: 43.74326205563236\n",
      "Epoch [20/50], Train Loss: 41.29783929293273, Test Loss: 40.92928616412274\n",
      "Epoch [30/50], Train Loss: 37.30643972803335, Test Loss: 36.742060748013586\n",
      "Epoch [40/50], Train Loss: 33.793415457303404, Test Loss: 35.15289083703772\n",
      "Epoch [50/50], Train Loss: 31.88518710527264, Test Loss: 32.72511440128475\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.668583704213624, Test Loss: 43.44491289807605\n",
      "Epoch [20/50], Train Loss: 42.354150465668226, Test Loss: 38.65526669985288\n",
      "Epoch [30/50], Train Loss: 38.48629072845959, Test Loss: 39.671096058634966\n",
      "Epoch [40/50], Train Loss: 35.21739461930072, Test Loss: 35.8902219549402\n",
      "Epoch [50/50], Train Loss: 32.347593776515275, Test Loss: 35.413136296458056\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.08229642148878, Test Loss: 42.298920866730924\n",
      "Epoch [20/50], Train Loss: 41.50209263660869, Test Loss: 39.46387447010387\n",
      "Epoch [30/50], Train Loss: 37.602385449018634, Test Loss: 37.73875258804916\n",
      "Epoch [40/50], Train Loss: 34.08741390666024, Test Loss: 36.72268850153143\n",
      "Epoch [50/50], Train Loss: 31.500788585475235, Test Loss: 32.73207743756183\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.8761492869893, Test Loss: 41.98129401268897\n",
      "Epoch [20/50], Train Loss: 41.22676420368132, Test Loss: 40.653206267914214\n",
      "Epoch [30/50], Train Loss: 37.864429786557054, Test Loss: 35.82691821804294\n",
      "Epoch [40/50], Train Loss: 34.215909764024076, Test Loss: 35.45805460446841\n",
      "Epoch [50/50], Train Loss: 31.879290715201957, Test Loss: 33.654077703302555\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.603578673816116, Test Loss: 43.79314140220741\n",
      "Epoch [20/50], Train Loss: 42.438455662961864, Test Loss: 36.698340725589105\n",
      "Epoch [30/50], Train Loss: 38.28804541415855, Test Loss: 38.43667448960341\n",
      "Epoch [40/50], Train Loss: 35.1180362763952, Test Loss: 34.60377002072025\n",
      "Epoch [50/50], Train Loss: 32.32022286086786, Test Loss: 35.30138421987559\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.817415393766808, Test Loss: 29.777396065848215\n",
      "Epoch [20/50], Train Loss: 25.608587884121253, Test Loss: 29.81972345129236\n",
      "Epoch [30/50], Train Loss: 25.549485334802846, Test Loss: 28.548399640368178\n",
      "Epoch [40/50], Train Loss: 24.85202579810971, Test Loss: 27.62591379339045\n",
      "Epoch [50/50], Train Loss: 25.931166458129884, Test Loss: 28.10342838237812\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.461356991627177, Test Loss: 30.842351393266156\n",
      "Epoch [20/50], Train Loss: 27.877360678500818, Test Loss: 28.814875070150798\n",
      "Epoch [30/50], Train Loss: 26.23117566968574, Test Loss: 28.62349829735694\n",
      "Epoch [40/50], Train Loss: 25.25094418760206, Test Loss: 27.577717669598467\n",
      "Epoch [50/50], Train Loss: 25.72840540526343, Test Loss: 31.434504620440595\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.470938685682952, Test Loss: 33.113188632122885\n",
      "Epoch [20/50], Train Loss: 27.48099362107574, Test Loss: 29.915739084219005\n",
      "Epoch [30/50], Train Loss: 26.639026979540215, Test Loss: 28.365676285384538\n",
      "Epoch [40/50], Train Loss: 26.577678717941534, Test Loss: 28.010465052220727\n",
      "Epoch [50/50], Train Loss: 26.374113658217134, Test Loss: 27.13812119620187\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.89290479441158, Test Loss: 66.16018924465428\n",
      "Epoch [20/50], Train Loss: 25.60868585305136, Test Loss: 30.057559843187207\n",
      "Epoch [30/50], Train Loss: 24.393097048900167, Test Loss: 29.12647935941622\n",
      "Epoch [40/50], Train Loss: 27.112261550152887, Test Loss: 27.922852825808835\n",
      "Epoch [50/50], Train Loss: 26.553130096685692, Test Loss: 29.685367708082325\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.666461425531107, Test Loss: 28.333239691598074\n",
      "Epoch [20/50], Train Loss: 26.391315810406795, Test Loss: 28.582917077200754\n",
      "Epoch [30/50], Train Loss: 25.76437370425365, Test Loss: 28.875004706444678\n",
      "Epoch [40/50], Train Loss: 25.125360107421876, Test Loss: 28.26174963913955\n",
      "Epoch [50/50], Train Loss: 24.967320876825053, Test Loss: 27.762020656040736\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.43126796034516, Test Loss: 28.847984165340275\n",
      "Epoch [20/50], Train Loss: 26.220075726118242, Test Loss: 28.509116333800478\n",
      "Epoch [30/50], Train Loss: 25.78261134663566, Test Loss: 27.77433410248199\n",
      "Epoch [40/50], Train Loss: 26.380899241713227, Test Loss: 28.530841753080292\n",
      "Epoch [50/50], Train Loss: 25.828091624525726, Test Loss: 28.422212377771153\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.861130873883358, Test Loss: 31.014080072378185\n",
      "Epoch [20/50], Train Loss: 26.1291063840272, Test Loss: 31.142246816065406\n",
      "Epoch [30/50], Train Loss: 25.797736264838548, Test Loss: 29.373581849135363\n",
      "Epoch [40/50], Train Loss: 25.911777333744237, Test Loss: 28.698444440767364\n",
      "Epoch [50/50], Train Loss: 25.95492232901151, Test Loss: 28.41514166299399\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.460489563863785, Test Loss: 31.866384679620918\n",
      "Epoch [20/50], Train Loss: 26.69601734348985, Test Loss: 29.385033842805143\n",
      "Epoch [30/50], Train Loss: 25.78930444561067, Test Loss: 30.463204668713853\n",
      "Epoch [40/50], Train Loss: 26.703528210374177, Test Loss: 28.694791967218574\n",
      "Epoch [50/50], Train Loss: 26.73279077498639, Test Loss: 27.45091636459549\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.602990334932922, Test Loss: 30.035591001634472\n",
      "Epoch [20/50], Train Loss: 26.606719063930825, Test Loss: 29.367221188235593\n",
      "Epoch [30/50], Train Loss: 25.513011801047405, Test Loss: 29.289618975156312\n",
      "Epoch [40/50], Train Loss: 25.926582524033844, Test Loss: 27.65228794147442\n",
      "Epoch [50/50], Train Loss: 25.832141976278336, Test Loss: 29.487734162962283\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.110403986446194, Test Loss: 32.454999923706055\n",
      "Epoch [20/50], Train Loss: 26.15634380403112, Test Loss: 28.780463206303583\n",
      "Epoch [30/50], Train Loss: 26.82130587843598, Test Loss: 32.61131338639693\n",
      "Epoch [40/50], Train Loss: 27.53082293526071, Test Loss: 27.936571294611152\n",
      "Epoch [50/50], Train Loss: 26.44792498604196, Test Loss: 28.793265082619406\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.921336702440605, Test Loss: 37.600792079776916\n",
      "Epoch [20/50], Train Loss: 27.29185791015625, Test Loss: 29.605355522849344\n",
      "Epoch [30/50], Train Loss: 25.915724732445888, Test Loss: 31.839379570700906\n",
      "Epoch [40/50], Train Loss: 25.367020503810195, Test Loss: 30.058588944472277\n",
      "Epoch [50/50], Train Loss: 25.91200491483094, Test Loss: 29.99118926308372\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.78691261166432, Test Loss: 36.373046156647916\n",
      "Epoch [20/50], Train Loss: 27.265613043112833, Test Loss: 32.297584385066834\n",
      "Epoch [30/50], Train Loss: 27.01796010126833, Test Loss: 28.427768905441482\n",
      "Epoch [40/50], Train Loss: 27.96746678586866, Test Loss: 27.977158434979327\n",
      "Epoch [50/50], Train Loss: 26.152808267562115, Test Loss: 28.170204162597656\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.96658386480613, Test Loss: 29.34427766676073\n",
      "Epoch [20/50], Train Loss: 26.57637852528056, Test Loss: 34.589333769562955\n",
      "Epoch [30/50], Train Loss: 25.867360061895653, Test Loss: 29.31538249919941\n",
      "Epoch [40/50], Train Loss: 26.176262596005298, Test Loss: 29.56731196812221\n",
      "Epoch [50/50], Train Loss: 25.337197763411726, Test Loss: 27.94154070569323\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.518710827436603, Test Loss: 31.509255149147727\n",
      "Epoch [20/50], Train Loss: 28.638481734228915, Test Loss: 29.827484279483944\n",
      "Epoch [30/50], Train Loss: 26.357798835879468, Test Loss: 29.243794329754717\n",
      "Epoch [40/50], Train Loss: 26.671571368858462, Test Loss: 29.326789905498554\n",
      "Epoch [50/50], Train Loss: 25.998072965027855, Test Loss: 27.90760359826026\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.658093755753313, Test Loss: 33.39881406511579\n",
      "Epoch [20/50], Train Loss: 26.82075565525743, Test Loss: 32.518198013305664\n",
      "Epoch [30/50], Train Loss: 25.971167010948307, Test Loss: 28.765945087779652\n",
      "Epoch [40/50], Train Loss: 25.50592122312452, Test Loss: 28.213746008934912\n",
      "Epoch [50/50], Train Loss: 27.69176294295514, Test Loss: 30.76794180931983\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.196670751102634, Test Loss: 33.86159371710443\n",
      "Epoch [20/50], Train Loss: 28.12742777652428, Test Loss: 30.1838064565287\n",
      "Epoch [30/50], Train Loss: 26.38196152859047, Test Loss: 28.115429915391005\n",
      "Epoch [40/50], Train Loss: 26.774514964369477, Test Loss: 27.85171243741915\n",
      "Epoch [50/50], Train Loss: 26.39869393520668, Test Loss: 28.757323351773348\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.017102838735113, Test Loss: 37.476077141699854\n",
      "Epoch [20/50], Train Loss: 27.83552899595167, Test Loss: 29.519192039192497\n",
      "Epoch [30/50], Train Loss: 26.359988503377945, Test Loss: 34.64054821063946\n",
      "Epoch [40/50], Train Loss: 26.023846035316343, Test Loss: 30.351147688828505\n",
      "Epoch [50/50], Train Loss: 25.97810764625424, Test Loss: 28.885610530902813\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.850390975201716, Test Loss: 36.68868057449143\n",
      "Epoch [20/50], Train Loss: 28.347513361446193, Test Loss: 31.145660400390625\n",
      "Epoch [30/50], Train Loss: 26.630328087728532, Test Loss: 30.181411123895025\n",
      "Epoch [40/50], Train Loss: 26.90281524658203, Test Loss: 28.825480300110655\n",
      "Epoch [50/50], Train Loss: 27.002536511030353, Test Loss: 29.382293676401112\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.04579701658155, Test Loss: 47.3234869721648\n",
      "Epoch [20/50], Train Loss: 45.58292111256083, Test Loss: 43.325873585490434\n",
      "Epoch [30/50], Train Loss: 43.14492193753602, Test Loss: 41.69556189202643\n",
      "Epoch [40/50], Train Loss: 40.80823142880299, Test Loss: 40.18033203521332\n",
      "Epoch [50/50], Train Loss: 38.739797448330236, Test Loss: 39.29519068730342\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.974372488553406, Test Loss: 47.80378004792449\n",
      "Epoch [20/50], Train Loss: 45.6121799156314, Test Loss: 44.409773888526026\n",
      "Epoch [30/50], Train Loss: 43.002246381415695, Test Loss: 42.06726173301796\n",
      "Epoch [40/50], Train Loss: 40.716643711777984, Test Loss: 39.75273013424564\n",
      "Epoch [50/50], Train Loss: 38.48276618582303, Test Loss: 38.15282048807516\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.2379285968718, Test Loss: 47.17664550186752\n",
      "Epoch [20/50], Train Loss: 45.93111996259846, Test Loss: 42.6164743002359\n",
      "Epoch [30/50], Train Loss: 43.54767237178615, Test Loss: 42.8831590429529\n",
      "Epoch [40/50], Train Loss: 41.28964373479124, Test Loss: 38.83956884408926\n",
      "Epoch [50/50], Train Loss: 39.2220504135382, Test Loss: 39.21436919175185\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.99032453943472, Test Loss: 46.68268208689504\n",
      "Epoch [20/50], Train Loss: 45.46195247212394, Test Loss: 43.44093139450271\n",
      "Epoch [30/50], Train Loss: 43.13915295210041, Test Loss: 42.436914121949826\n",
      "Epoch [40/50], Train Loss: 40.90925176026391, Test Loss: 40.37592394940265\n",
      "Epoch [50/50], Train Loss: 38.594778992699794, Test Loss: 38.289465024873806\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.89332765673028, Test Loss: 47.762059347970144\n",
      "Epoch [20/50], Train Loss: 45.35271366306993, Test Loss: 43.06994782484971\n",
      "Epoch [30/50], Train Loss: 43.00695430568007, Test Loss: 42.28463408234832\n",
      "Epoch [40/50], Train Loss: 40.619569872246416, Test Loss: 39.673595230300705\n",
      "Epoch [50/50], Train Loss: 38.3644685088611, Test Loss: 38.97043490719486\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.29683197521773, Test Loss: 46.235439399620155\n",
      "Epoch [20/50], Train Loss: 46.02436741062852, Test Loss: 43.11513316166865\n",
      "Epoch [30/50], Train Loss: 43.74857853123399, Test Loss: 41.940837562858285\n",
      "Epoch [40/50], Train Loss: 41.63607605480757, Test Loss: 40.427308862859554\n",
      "Epoch [50/50], Train Loss: 39.8497453095483, Test Loss: 36.46216243892521\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.99454458267962, Test Loss: 47.024189788025694\n",
      "Epoch [20/50], Train Loss: 45.54381381175557, Test Loss: 42.15875531481458\n",
      "Epoch [30/50], Train Loss: 43.053939256511754, Test Loss: 38.151748805851135\n",
      "Epoch [40/50], Train Loss: 40.88252324588963, Test Loss: 38.57091006984958\n",
      "Epoch [50/50], Train Loss: 38.55918277677942, Test Loss: 37.85782831365412\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.68998435598905, Test Loss: 46.208106152423014\n",
      "Epoch [20/50], Train Loss: 45.08028541940158, Test Loss: 43.33544451230532\n",
      "Epoch [30/50], Train Loss: 42.74704362212634, Test Loss: 38.86174140038428\n",
      "Epoch [40/50], Train Loss: 40.28543701171875, Test Loss: 39.881010575727984\n",
      "Epoch [50/50], Train Loss: 38.13006109018795, Test Loss: 38.67972049465427\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.45642148627609, Test Loss: 47.10568336387733\n",
      "Epoch [20/50], Train Loss: 46.187874828401156, Test Loss: 44.030814282305826\n",
      "Epoch [30/50], Train Loss: 43.89971130871382, Test Loss: 40.52558259840136\n",
      "Epoch [40/50], Train Loss: 41.70829727923284, Test Loss: 40.55661159366756\n",
      "Epoch [50/50], Train Loss: 39.40650477174853, Test Loss: 37.79271341299082\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.489872904292874, Test Loss: 31.268089121038262\n",
      "Epoch [20/50], Train Loss: 25.354017939333055, Test Loss: 31.099969492330178\n",
      "Epoch [30/50], Train Loss: 26.194122552090004, Test Loss: 27.74117593641405\n",
      "Epoch [40/50], Train Loss: 24.232524371538005, Test Loss: 28.622954430518213\n",
      "Epoch [50/50], Train Loss: 24.154088279849194, Test Loss: 29.273200741061917\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.30794762783363, Test Loss: 31.41515466764376\n",
      "Epoch [20/50], Train Loss: 25.846522872174372, Test Loss: 30.788030178515942\n",
      "Epoch [30/50], Train Loss: 24.569910193271323, Test Loss: 31.512266951721983\n",
      "Epoch [40/50], Train Loss: 24.43263524790279, Test Loss: 27.137437448873147\n",
      "Epoch [50/50], Train Loss: 23.994134515230773, Test Loss: 28.86067244294402\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.24514112003514, Test Loss: 31.250758332091493\n",
      "Epoch [20/50], Train Loss: 24.88697235232494, Test Loss: 27.16211854018174\n",
      "Epoch [30/50], Train Loss: 24.868227949298795, Test Loss: 28.455048821189187\n",
      "Epoch [40/50], Train Loss: 25.899462596705703, Test Loss: 28.385312563413148\n",
      "Epoch [50/50], Train Loss: 26.31096826772221, Test Loss: 28.243937603839033\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.352745224999598, Test Loss: 31.582807342727463\n",
      "Epoch [20/50], Train Loss: 26.221351348376665, Test Loss: 27.938614560412123\n",
      "Epoch [30/50], Train Loss: 25.961830032848923, Test Loss: 28.14734040297471\n",
      "Epoch [40/50], Train Loss: 24.941036443241305, Test Loss: 28.083866812966086\n",
      "Epoch [50/50], Train Loss: 23.77007570735744, Test Loss: 28.32581762834029\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.83806389980629, Test Loss: 35.01158545853256\n",
      "Epoch [20/50], Train Loss: 25.376006711115604, Test Loss: 29.182449662840213\n",
      "Epoch [30/50], Train Loss: 24.732474749205544, Test Loss: 28.653394476159825\n",
      "Epoch [40/50], Train Loss: 24.444004921834978, Test Loss: 28.73589599906624\n",
      "Epoch [50/50], Train Loss: 25.684506125528305, Test Loss: 28.83578397082044\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.363359201149862, Test Loss: 32.79462395705186\n",
      "Epoch [20/50], Train Loss: 26.6394971315978, Test Loss: 27.752675985361073\n",
      "Epoch [30/50], Train Loss: 24.764673326836256, Test Loss: 30.980066076501622\n",
      "Epoch [40/50], Train Loss: 24.61320364279825, Test Loss: 28.097235791094892\n",
      "Epoch [50/50], Train Loss: 23.86171383466877, Test Loss: 28.538546921370866\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.163126648449506, Test Loss: 32.29212936797699\n",
      "Epoch [20/50], Train Loss: 26.037655420772364, Test Loss: 30.24865014212472\n",
      "Epoch [30/50], Train Loss: 25.742161116052845, Test Loss: 29.46898470296488\n",
      "Epoch [40/50], Train Loss: 24.618745641239354, Test Loss: 31.964233844311206\n",
      "Epoch [50/50], Train Loss: 24.68143551935915, Test Loss: 28.09089403028612\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.49879758866107, Test Loss: 33.99390864681888\n",
      "Epoch [20/50], Train Loss: 26.553185960113026, Test Loss: 30.04699692168793\n",
      "Epoch [30/50], Train Loss: 25.41593117010398, Test Loss: 29.746779057886695\n",
      "Epoch [40/50], Train Loss: 24.87069782194544, Test Loss: 28.930061538498123\n",
      "Epoch [50/50], Train Loss: 25.679189338058723, Test Loss: 31.910288674490793\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.678342737917042, Test Loss: 33.41907345164906\n",
      "Epoch [20/50], Train Loss: 27.195286347436124, Test Loss: 29.43673309722504\n",
      "Epoch [30/50], Train Loss: 25.71221616150903, Test Loss: 31.914835422069995\n",
      "Epoch [40/50], Train Loss: 25.981812617817862, Test Loss: 28.658538471568715\n",
      "Epoch [50/50], Train Loss: 25.32369089595607, Test Loss: 28.47483169258415\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.971310143392593, Test Loss: 28.91955601085316\n",
      "Epoch [20/50], Train Loss: 25.12199141080262, Test Loss: 27.79485286365856\n",
      "Epoch [30/50], Train Loss: 25.555902887563235, Test Loss: 30.43619443224622\n",
      "Epoch [40/50], Train Loss: 25.354623894613297, Test Loss: 28.787917496322038\n",
      "Epoch [50/50], Train Loss: 24.45154445210441, Test Loss: 30.716815874174042\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.093610582195346, Test Loss: 28.89122928272594\n",
      "Epoch [20/50], Train Loss: 25.087397584758822, Test Loss: 28.46537057455484\n",
      "Epoch [30/50], Train Loss: 25.790367138972048, Test Loss: 32.3811775058895\n",
      "Epoch [40/50], Train Loss: 25.545491102875257, Test Loss: 28.60278553157658\n",
      "Epoch [50/50], Train Loss: 25.835745845857215, Test Loss: 28.739763136033886\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.937262682054865, Test Loss: 34.43602021948084\n",
      "Epoch [20/50], Train Loss: 26.838283551325564, Test Loss: 31.12474077398127\n",
      "Epoch [30/50], Train Loss: 25.3475209157975, Test Loss: 28.862676521400353\n",
      "Epoch [40/50], Train Loss: 25.504593964873767, Test Loss: 28.126277725417893\n",
      "Epoch [50/50], Train Loss: 25.755431960058996, Test Loss: 27.215657073181944\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.859139307991402, Test Loss: 31.334335921646712\n",
      "Epoch [20/50], Train Loss: 27.371388063274445, Test Loss: 32.09470434312696\n",
      "Epoch [30/50], Train Loss: 26.010081406890368, Test Loss: 29.615299720268744\n",
      "Epoch [40/50], Train Loss: 26.615598059482263, Test Loss: 29.035088402884348\n",
      "Epoch [50/50], Train Loss: 25.305445711729956, Test Loss: 31.881386917906923\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.374787214935804, Test Loss: 36.0534901309323\n",
      "Epoch [20/50], Train Loss: 26.73987434262135, Test Loss: 36.43148149762835\n",
      "Epoch [30/50], Train Loss: 26.194381157296604, Test Loss: 28.87119503764363\n",
      "Epoch [40/50], Train Loss: 26.404109717197105, Test Loss: 29.050544367208108\n",
      "Epoch [50/50], Train Loss: 25.830066655893795, Test Loss: 28.68540773763285\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.754821289562788, Test Loss: 30.423768675172482\n",
      "Epoch [20/50], Train Loss: 26.287352708910333, Test Loss: 29.86198747312868\n",
      "Epoch [30/50], Train Loss: 25.732266685610913, Test Loss: 28.92234133435534\n",
      "Epoch [40/50], Train Loss: 26.763350139680455, Test Loss: 31.69268764149059\n",
      "Epoch [50/50], Train Loss: 25.190357277041574, Test Loss: 29.631113795491007\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.902583231691455, Test Loss: 36.94995047829368\n",
      "Epoch [20/50], Train Loss: 27.68368034362793, Test Loss: 30.36896970674589\n",
      "Epoch [30/50], Train Loss: 27.713527398031268, Test Loss: 30.383642122342987\n",
      "Epoch [40/50], Train Loss: 26.604595440723855, Test Loss: 32.7657221261557\n",
      "Epoch [50/50], Train Loss: 26.1063720703125, Test Loss: 29.317729727014317\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.378125875504292, Test Loss: 35.397415557464996\n",
      "Epoch [20/50], Train Loss: 26.09954378096784, Test Loss: 29.42467285750748\n",
      "Epoch [30/50], Train Loss: 26.680612839245406, Test Loss: 30.60634043309596\n",
      "Epoch [40/50], Train Loss: 26.69864812131788, Test Loss: 29.012566380686575\n",
      "Epoch [50/50], Train Loss: 26.025032875186106, Test Loss: 28.521736367956386\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.759609184890497, Test Loss: 34.60345072560496\n",
      "Epoch [20/50], Train Loss: 27.754718980632845, Test Loss: 29.617679075761274\n",
      "Epoch [30/50], Train Loss: 27.078424378692127, Test Loss: 31.529563482705647\n",
      "Epoch [40/50], Train Loss: 25.826415965596183, Test Loss: 29.697365575022513\n",
      "Epoch [50/50], Train Loss: 25.97857118200083, Test Loss: 28.298134816157354\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.81314454625865, Test Loss: 47.82607650756836\n",
      "Epoch [20/50], Train Loss: 47.27966137245053, Test Loss: 45.94845962524414\n",
      "Epoch [30/50], Train Loss: 45.75633896374312, Test Loss: 43.339107513427734\n",
      "Epoch [40/50], Train Loss: 44.34912367023406, Test Loss: 42.342529296875\n",
      "Epoch [50/50], Train Loss: 42.87793565343638, Test Loss: 40.08090591430664\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.55469400374616, Test Loss: 48.66355895996094\n",
      "Epoch [20/50], Train Loss: 46.976561224265176, Test Loss: 46.3008918762207\n",
      "Epoch [30/50], Train Loss: 45.4216682183938, Test Loss: 41.49652099609375\n",
      "Epoch [40/50], Train Loss: 43.8873200588539, Test Loss: 41.52388381958008\n",
      "Epoch [50/50], Train Loss: 42.44642592883501, Test Loss: 42.263389587402344\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 49.14668306444512, Test Loss: 47.276668548583984\n",
      "Epoch [20/50], Train Loss: 47.83368680359887, Test Loss: 45.626399993896484\n",
      "Epoch [30/50], Train Loss: 46.50028749059458, Test Loss: 42.749839782714844\n",
      "Epoch [40/50], Train Loss: 44.8394489100722, Test Loss: 43.268280029296875\n",
      "Epoch [50/50], Train Loss: 43.50311589475538, Test Loss: 41.74345016479492\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.773611375152086, Test Loss: 47.41579818725586\n",
      "Epoch [20/50], Train Loss: 47.347007713943235, Test Loss: 45.027801513671875\n",
      "Epoch [30/50], Train Loss: 45.76691129090356, Test Loss: 43.37981033325195\n",
      "Epoch [40/50], Train Loss: 44.25053123098905, Test Loss: 42.253726959228516\n",
      "Epoch [50/50], Train Loss: 42.84563786240875, Test Loss: 41.92698669433594\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.71181538065926, Test Loss: 49.12338638305664\n",
      "Epoch [20/50], Train Loss: 47.15783828985496, Test Loss: 45.85566711425781\n",
      "Epoch [30/50], Train Loss: 45.60426109188893, Test Loss: 42.120689392089844\n",
      "Epoch [40/50], Train Loss: 44.07158376975138, Test Loss: 42.11579513549805\n",
      "Epoch [50/50], Train Loss: 42.57026460991531, Test Loss: 42.183162689208984\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 49.15945264472336, Test Loss: 47.98558807373047\n",
      "Epoch [20/50], Train Loss: 47.858836552354155, Test Loss: 45.923465728759766\n",
      "Epoch [30/50], Train Loss: 46.34487205880587, Test Loss: 43.2412223815918\n",
      "Epoch [40/50], Train Loss: 44.90086772480949, Test Loss: 41.76840591430664\n",
      "Epoch [50/50], Train Loss: 43.41137746592037, Test Loss: 41.679134368896484\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.83589133590949, Test Loss: 48.42774200439453\n",
      "Epoch [20/50], Train Loss: 47.29191559338179, Test Loss: 46.5611572265625\n",
      "Epoch [30/50], Train Loss: 45.76162005565205, Test Loss: 42.316593170166016\n",
      "Epoch [40/50], Train Loss: 44.29406668240907, Test Loss: 37.9398078918457\n",
      "Epoch [50/50], Train Loss: 42.77113059622342, Test Loss: 41.45757293701172\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.78529089005267, Test Loss: 47.32156753540039\n",
      "Epoch [20/50], Train Loss: 47.28726714087314, Test Loss: 46.147911071777344\n",
      "Epoch [30/50], Train Loss: 45.79474956324843, Test Loss: 42.973201751708984\n",
      "Epoch [40/50], Train Loss: 44.26140369352747, Test Loss: 41.42581558227539\n",
      "Epoch [50/50], Train Loss: 42.8068606517354, Test Loss: 39.350830078125\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 49.037083497594615, Test Loss: 47.2896728515625\n",
      "Epoch [20/50], Train Loss: 47.778271196709305, Test Loss: 44.682132720947266\n",
      "Epoch [30/50], Train Loss: 46.31742987710921, Test Loss: 44.56214141845703\n",
      "Epoch [40/50], Train Loss: 44.890053245669506, Test Loss: 42.22146987915039\n",
      "Epoch [50/50], Train Loss: 43.45052540263192, Test Loss: 41.08066940307617\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.014186577718768, Test Loss: 35.55079650878906\n",
      "Epoch [20/50], Train Loss: 25.782150837632475, Test Loss: 29.05000114440918\n",
      "Epoch [30/50], Train Loss: 25.23822154060739, Test Loss: 30.334983825683594\n",
      "Epoch [40/50], Train Loss: 24.641789420706328, Test Loss: 28.333534240722656\n",
      "Epoch [50/50], Train Loss: 23.43501456213779, Test Loss: 28.928951263427734\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.30516526894491, Test Loss: 33.06317138671875\n",
      "Epoch [20/50], Train Loss: 25.665416792572522, Test Loss: 31.716419219970703\n",
      "Epoch [30/50], Train Loss: 24.00472363331279, Test Loss: 28.280670166015625\n",
      "Epoch [40/50], Train Loss: 24.630987548828124, Test Loss: 29.87016487121582\n",
      "Epoch [50/50], Train Loss: 24.08753294397573, Test Loss: 28.594938278198242\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.641025430648053, Test Loss: 33.64637756347656\n",
      "Epoch [20/50], Train Loss: 25.342659309262135, Test Loss: 31.129743576049805\n",
      "Epoch [30/50], Train Loss: 24.700023882506322, Test Loss: 31.775846481323242\n",
      "Epoch [40/50], Train Loss: 24.263337938902808, Test Loss: 28.702129364013672\n",
      "Epoch [50/50], Train Loss: 24.121683989978226, Test Loss: 27.721742630004883\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.857580216204532, Test Loss: 35.58740234375\n",
      "Epoch [20/50], Train Loss: 25.751060742237527, Test Loss: 30.87786293029785\n",
      "Epoch [30/50], Train Loss: 25.793357548948194, Test Loss: 32.15329360961914\n",
      "Epoch [40/50], Train Loss: 24.837469113459353, Test Loss: 28.86317253112793\n",
      "Epoch [50/50], Train Loss: 24.47206542843678, Test Loss: 29.69132423400879\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.047534936373353, Test Loss: 39.66838073730469\n",
      "Epoch [20/50], Train Loss: 25.001812725379818, Test Loss: 32.87015914916992\n",
      "Epoch [30/50], Train Loss: 24.783552438704692, Test Loss: 28.28955078125\n",
      "Epoch [40/50], Train Loss: 25.52237982828109, Test Loss: 29.500627517700195\n",
      "Epoch [50/50], Train Loss: 24.654746684090036, Test Loss: 28.63534927368164\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.652137537471585, Test Loss: 35.73741912841797\n",
      "Epoch [20/50], Train Loss: 25.18132875786453, Test Loss: 28.648380279541016\n",
      "Epoch [30/50], Train Loss: 25.11058286447994, Test Loss: 32.39130783081055\n",
      "Epoch [40/50], Train Loss: 24.625839558585746, Test Loss: 27.697778701782227\n",
      "Epoch [50/50], Train Loss: 24.730814123935385, Test Loss: 30.127887725830078\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.669128124049454, Test Loss: 35.96461486816406\n",
      "Epoch [20/50], Train Loss: 26.061517721707702, Test Loss: 32.126216888427734\n",
      "Epoch [30/50], Train Loss: 25.196539462980677, Test Loss: 30.468791961669922\n",
      "Epoch [40/50], Train Loss: 25.731309484262937, Test Loss: 30.384273529052734\n",
      "Epoch [50/50], Train Loss: 25.746010933547723, Test Loss: 32.01103210449219\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.8638313168385, Test Loss: 38.549503326416016\n",
      "Epoch [20/50], Train Loss: 26.20009497970831, Test Loss: 34.767879486083984\n",
      "Epoch [30/50], Train Loss: 26.542801184732404, Test Loss: 42.836509704589844\n",
      "Epoch [40/50], Train Loss: 24.840441882024045, Test Loss: 29.304161071777344\n",
      "Epoch [50/50], Train Loss: 26.020312012219037, Test Loss: 28.834674835205078\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.228163803600875, Test Loss: 36.095401763916016\n",
      "Epoch [20/50], Train Loss: 27.109236401417217, Test Loss: 32.29149627685547\n",
      "Epoch [30/50], Train Loss: 25.886076874029442, Test Loss: 33.278167724609375\n",
      "Epoch [40/50], Train Loss: 25.133608445964875, Test Loss: 32.72223663330078\n",
      "Epoch [50/50], Train Loss: 24.964054370317303, Test Loss: 30.14268684387207\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.711840144923475, Test Loss: 32.69559860229492\n",
      "Epoch [20/50], Train Loss: 25.93418806107318, Test Loss: 28.104177474975586\n",
      "Epoch [30/50], Train Loss: 26.072008245499408, Test Loss: 28.61334800720215\n",
      "Epoch [40/50], Train Loss: 25.37245146954646, Test Loss: 27.91292381286621\n",
      "Epoch [50/50], Train Loss: 25.34799285638528, Test Loss: 32.618133544921875\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.856681779955256, Test Loss: 32.73698425292969\n",
      "Epoch [20/50], Train Loss: 26.399846367757828, Test Loss: 29.077425003051758\n",
      "Epoch [30/50], Train Loss: 25.356591784367797, Test Loss: 29.507413864135742\n",
      "Epoch [40/50], Train Loss: 25.087725379818774, Test Loss: 28.409189224243164\n",
      "Epoch [50/50], Train Loss: 24.62958737357718, Test Loss: 28.103214263916016\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.168668503057763, Test Loss: 35.36872100830078\n",
      "Epoch [20/50], Train Loss: 25.49966893430616, Test Loss: 31.76075553894043\n",
      "Epoch [30/50], Train Loss: 24.827101347876376, Test Loss: 28.11094093322754\n",
      "Epoch [40/50], Train Loss: 25.10100525402632, Test Loss: 28.45662498474121\n",
      "Epoch [50/50], Train Loss: 25.66107590472112, Test Loss: 28.342998504638672\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.51994595762159, Test Loss: 31.63051414489746\n",
      "Epoch [20/50], Train Loss: 26.68345084893899, Test Loss: 33.316383361816406\n",
      "Epoch [30/50], Train Loss: 26.050111964491546, Test Loss: 35.15697479248047\n",
      "Epoch [40/50], Train Loss: 25.576153176729797, Test Loss: 28.804861068725586\n",
      "Epoch [50/50], Train Loss: 25.948144012201027, Test Loss: 29.723129272460938\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.934814947159563, Test Loss: 33.903987884521484\n",
      "Epoch [20/50], Train Loss: 27.124837543925302, Test Loss: 31.835765838623047\n",
      "Epoch [30/50], Train Loss: 25.793588944732164, Test Loss: 39.7107048034668\n",
      "Epoch [40/50], Train Loss: 25.309463138267642, Test Loss: 31.833961486816406\n",
      "Epoch [50/50], Train Loss: 25.596001071617252, Test Loss: 30.062328338623047\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.830940715602186, Test Loss: 31.154876708984375\n",
      "Epoch [20/50], Train Loss: 29.06802945996894, Test Loss: 30.985107421875\n",
      "Epoch [30/50], Train Loss: 27.888351853167425, Test Loss: 30.879182815551758\n",
      "Epoch [40/50], Train Loss: 26.53318757229164, Test Loss: 29.516605377197266\n",
      "Epoch [50/50], Train Loss: 25.840728334520684, Test Loss: 29.693296432495117\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.839029818675556, Test Loss: 36.263370513916016\n",
      "Epoch [20/50], Train Loss: 29.191495151207096, Test Loss: 31.782150268554688\n",
      "Epoch [30/50], Train Loss: 27.187085636326525, Test Loss: 33.84532165527344\n",
      "Epoch [40/50], Train Loss: 28.361283836990108, Test Loss: 33.36534118652344\n",
      "Epoch [50/50], Train Loss: 25.689383547423315, Test Loss: 35.951324462890625\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.419917910216284, Test Loss: 34.578182220458984\n",
      "Epoch [20/50], Train Loss: 28.371312682355036, Test Loss: 34.30588150024414\n",
      "Epoch [30/50], Train Loss: 26.655172854564228, Test Loss: 30.97542381286621\n",
      "Epoch [40/50], Train Loss: 26.53432815426686, Test Loss: 30.002277374267578\n",
      "Epoch [50/50], Train Loss: 26.253891066254162, Test Loss: 32.234825134277344\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.6275294194456, Test Loss: 36.3856201171875\n",
      "Epoch [20/50], Train Loss: 28.59362634127257, Test Loss: 32.60105514526367\n",
      "Epoch [30/50], Train Loss: 26.39604648527552, Test Loss: 32.1761474609375\n",
      "Epoch [40/50], Train Loss: 26.892498804311284, Test Loss: 35.82444381713867\n",
      "Epoch [50/50], Train Loss: 25.866201669661724, Test Loss: 30.734718322753906\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.19435526113041, Test Loss: 41.63598647674957\n",
      "Epoch [20/50], Train Loss: 36.18887262813381, Test Loss: 34.594064539129086\n",
      "Epoch [30/50], Train Loss: 30.610937287377528, Test Loss: 32.29707160553375\n",
      "Epoch [40/50], Train Loss: 28.237867405375496, Test Loss: 29.342874502206776\n",
      "Epoch [50/50], Train Loss: 26.409297123893364, Test Loss: 28.92034590089476\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 42.609486210932495, Test Loss: 39.786288471965044\n",
      "Epoch [20/50], Train Loss: 35.565903210249104, Test Loss: 35.44409571065531\n",
      "Epoch [30/50], Train Loss: 30.988950779398934, Test Loss: 32.26023448597301\n",
      "Epoch [40/50], Train Loss: 29.14369481196169, Test Loss: 29.75799795869109\n",
      "Epoch [50/50], Train Loss: 26.51397788876393, Test Loss: 28.87618907086261\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.14784638451748, Test Loss: 45.26263130485238\n",
      "Epoch [20/50], Train Loss: 37.424148959800846, Test Loss: 36.797671825854806\n",
      "Epoch [30/50], Train Loss: 32.48974716311596, Test Loss: 32.228742599487305\n",
      "Epoch [40/50], Train Loss: 28.537493083516104, Test Loss: 29.868344690892602\n",
      "Epoch [50/50], Train Loss: 27.127403596971856, Test Loss: 28.274231477217242\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 42.99501151413214, Test Loss: 38.81336415278447\n",
      "Epoch [20/50], Train Loss: 35.767339919043366, Test Loss: 35.1539909560959\n",
      "Epoch [30/50], Train Loss: 31.233490377957704, Test Loss: 33.28732862100973\n",
      "Epoch [40/50], Train Loss: 27.899923843633932, Test Loss: 30.763342077081855\n",
      "Epoch [50/50], Train Loss: 26.639763041011623, Test Loss: 28.381305149623326\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 42.89511151548292, Test Loss: 40.448049916849506\n",
      "Epoch [20/50], Train Loss: 36.05006965887351, Test Loss: 34.92914692767255\n",
      "Epoch [30/50], Train Loss: 31.640421620353322, Test Loss: 33.49028661653593\n",
      "Epoch [40/50], Train Loss: 28.930769360651734, Test Loss: 29.64298597558752\n",
      "Epoch [50/50], Train Loss: 27.73927182682225, Test Loss: 28.3640681180087\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.880664612816986, Test Loss: 42.142598635190495\n",
      "Epoch [20/50], Train Loss: 37.14874398903768, Test Loss: 38.98817037607168\n",
      "Epoch [30/50], Train Loss: 32.98492344090196, Test Loss: 32.445612795941244\n",
      "Epoch [40/50], Train Loss: 29.384148775944944, Test Loss: 30.710129353907202\n",
      "Epoch [50/50], Train Loss: 27.58740093043593, Test Loss: 30.512877154659915\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.01250710409196, Test Loss: 41.92787418117771\n",
      "Epoch [20/50], Train Loss: 35.747367633757044, Test Loss: 35.46413223464768\n",
      "Epoch [30/50], Train Loss: 31.7478726621534, Test Loss: 31.60304745760831\n",
      "Epoch [40/50], Train Loss: 28.723761442841077, Test Loss: 29.251062715208377\n",
      "Epoch [50/50], Train Loss: 26.5393349694424, Test Loss: 28.938933087633803\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.13914444720159, Test Loss: 41.45710531457678\n",
      "Epoch [20/50], Train Loss: 35.91227948548364, Test Loss: 37.533624178403386\n",
      "Epoch [30/50], Train Loss: 31.367122106083105, Test Loss: 33.02731848382331\n",
      "Epoch [40/50], Train Loss: 28.327298398877755, Test Loss: 30.694469427133534\n",
      "Epoch [50/50], Train Loss: 26.798911460501248, Test Loss: 29.855940509152102\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.15736139016073, Test Loss: 40.267710054075565\n",
      "Epoch [20/50], Train Loss: 37.41986381655834, Test Loss: 37.237135156408534\n",
      "Epoch [30/50], Train Loss: 33.14475980664863, Test Loss: 34.6135314346908\n",
      "Epoch [40/50], Train Loss: 29.408415997614625, Test Loss: 29.65100887843541\n",
      "Epoch [50/50], Train Loss: 27.034212356317237, Test Loss: 29.103442699878247\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.52454236765377, Test Loss: 29.059131325065316\n",
      "Epoch [20/50], Train Loss: 26.429244376010583, Test Loss: 28.65630461952903\n",
      "Epoch [30/50], Train Loss: 26.204396063382507, Test Loss: 28.846106937953405\n",
      "Epoch [40/50], Train Loss: 24.977073694448002, Test Loss: 28.85032975828493\n",
      "Epoch [50/50], Train Loss: 24.290468897585008, Test Loss: 29.265206151194388\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.597339261164432, Test Loss: 30.01505980553565\n",
      "Epoch [20/50], Train Loss: 26.003906393832846, Test Loss: 28.735742643282013\n",
      "Epoch [30/50], Train Loss: 26.331206524958375, Test Loss: 28.194086520702808\n",
      "Epoch [40/50], Train Loss: 24.3862859319468, Test Loss: 29.87706637692142\n",
      "Epoch [50/50], Train Loss: 25.620558435408796, Test Loss: 28.260034387761895\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.46126751508869, Test Loss: 30.309271775282824\n",
      "Epoch [20/50], Train Loss: 25.862282996881202, Test Loss: 28.553539028415432\n",
      "Epoch [30/50], Train Loss: 26.418939183969968, Test Loss: 30.498912117697976\n",
      "Epoch [40/50], Train Loss: 26.11061610237497, Test Loss: 28.351727275105265\n",
      "Epoch [50/50], Train Loss: 25.610618472490156, Test Loss: 30.979164396013534\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.14430483208328, Test Loss: 31.625395514748313\n",
      "Epoch [20/50], Train Loss: 26.070140707297405, Test Loss: 29.099903329626308\n",
      "Epoch [30/50], Train Loss: 24.408620015128715, Test Loss: 28.7334941517223\n",
      "Epoch [40/50], Train Loss: 24.54495577577685, Test Loss: 28.07799852049196\n",
      "Epoch [50/50], Train Loss: 25.271846083344006, Test Loss: 27.573866608855013\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.399177607551948, Test Loss: 29.292816434587753\n",
      "Epoch [20/50], Train Loss: 26.143391199580957, Test Loss: 29.592900313340223\n",
      "Epoch [30/50], Train Loss: 26.291743575549518, Test Loss: 28.67931583949498\n",
      "Epoch [40/50], Train Loss: 25.203864225794057, Test Loss: 29.08788438276811\n",
      "Epoch [50/50], Train Loss: 26.11534010465028, Test Loss: 30.696930823388037\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.793119749475697, Test Loss: 29.47032220023019\n",
      "Epoch [20/50], Train Loss: 27.019906303530835, Test Loss: 30.20237248903745\n",
      "Epoch [30/50], Train Loss: 26.55211606260206, Test Loss: 29.430470206520774\n",
      "Epoch [40/50], Train Loss: 26.44239151751409, Test Loss: 28.10149165562221\n",
      "Epoch [50/50], Train Loss: 24.891289345162814, Test Loss: 28.256278199034853\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.623918989838145, Test Loss: 31.735495356770304\n",
      "Epoch [20/50], Train Loss: 28.792867103951878, Test Loss: 31.365856517444957\n",
      "Epoch [30/50], Train Loss: 26.79934370947666, Test Loss: 29.929237316181133\n",
      "Epoch [40/50], Train Loss: 25.3750111564261, Test Loss: 29.419525988690264\n",
      "Epoch [50/50], Train Loss: 26.146428180131757, Test Loss: 28.48826294440728\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.012329139084112, Test Loss: 31.9174744246842\n",
      "Epoch [20/50], Train Loss: 26.61252580236216, Test Loss: 31.932898286101107\n",
      "Epoch [30/50], Train Loss: 26.474376828553247, Test Loss: 32.59717525135387\n",
      "Epoch [40/50], Train Loss: 26.57057288748319, Test Loss: 28.909732001168386\n",
      "Epoch [50/50], Train Loss: 26.765789926247518, Test Loss: 28.83720742262803\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.44990698392274, Test Loss: 34.00480235706676\n",
      "Epoch [20/50], Train Loss: 27.581698633412845, Test Loss: 31.286218915666854\n",
      "Epoch [30/50], Train Loss: 26.917608311137215, Test Loss: 28.71850481900302\n",
      "Epoch [40/50], Train Loss: 26.05100058258557, Test Loss: 29.73625017141367\n",
      "Epoch [50/50], Train Loss: 25.711064973424694, Test Loss: 27.96058887630314\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.783874836906058, Test Loss: 29.2060983335817\n",
      "Epoch [20/50], Train Loss: 28.09923951195889, Test Loss: 29.94171803957456\n",
      "Epoch [30/50], Train Loss: 26.75683618139048, Test Loss: 31.26555749967501\n",
      "Epoch [40/50], Train Loss: 28.491061576468045, Test Loss: 28.141381474284383\n",
      "Epoch [50/50], Train Loss: 26.505314686259286, Test Loss: 27.84930372857428\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.422361730356684, Test Loss: 31.309096175354796\n",
      "Epoch [20/50], Train Loss: 27.016988485367573, Test Loss: 31.04743628068404\n",
      "Epoch [30/50], Train Loss: 26.82502676541688, Test Loss: 29.57215594006823\n",
      "Epoch [40/50], Train Loss: 26.875676014384286, Test Loss: 32.95394030484286\n",
      "Epoch [50/50], Train Loss: 26.276486856429305, Test Loss: 28.55450741656415\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.245239658043033, Test Loss: 32.08006891027674\n",
      "Epoch [20/50], Train Loss: 27.60533691781466, Test Loss: 29.954344093025504\n",
      "Epoch [30/50], Train Loss: 28.21009750366211, Test Loss: 28.82676280628551\n",
      "Epoch [40/50], Train Loss: 26.31076805239818, Test Loss: 28.164066116531174\n",
      "Epoch [50/50], Train Loss: 25.188105898997822, Test Loss: 28.841054445737367\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.65115008745037, Test Loss: 30.40035225509049\n",
      "Epoch [20/50], Train Loss: 27.980510874263576, Test Loss: 30.04363270549031\n",
      "Epoch [30/50], Train Loss: 26.361782921337692, Test Loss: 28.80409015308727\n",
      "Epoch [40/50], Train Loss: 27.771713087988683, Test Loss: 28.120649709329978\n",
      "Epoch [50/50], Train Loss: 26.97588879319488, Test Loss: 28.223419139911602\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.00350999050453, Test Loss: 32.835459820635904\n",
      "Epoch [20/50], Train Loss: 27.943599163117955, Test Loss: 30.118387990183646\n",
      "Epoch [30/50], Train Loss: 29.218028596971855, Test Loss: 36.587432365912896\n",
      "Epoch [40/50], Train Loss: 27.983985169207465, Test Loss: 28.703313901826935\n",
      "Epoch [50/50], Train Loss: 27.07756211953085, Test Loss: 30.51993719323889\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.729031234491067, Test Loss: 32.610841131829595\n",
      "Epoch [20/50], Train Loss: 28.467611969494428, Test Loss: 29.53697630646941\n",
      "Epoch [30/50], Train Loss: 26.796845564295033, Test Loss: 31.80641154499797\n",
      "Epoch [40/50], Train Loss: 26.048017045318105, Test Loss: 28.382911335338246\n",
      "Epoch [50/50], Train Loss: 27.345871922227204, Test Loss: 28.286546261279614\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.656058683551727, Test Loss: 34.534067153930664\n",
      "Epoch [20/50], Train Loss: 27.970512465179944, Test Loss: 43.228699324967025\n",
      "Epoch [30/50], Train Loss: 27.686967593333762, Test Loss: 34.05738265793045\n",
      "Epoch [40/50], Train Loss: 27.51567596685691, Test Loss: 30.354706305962104\n",
      "Epoch [50/50], Train Loss: 26.764399544137422, Test Loss: 29.2730635110434\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.355869574624982, Test Loss: 42.29778468144404\n",
      "Epoch [20/50], Train Loss: 28.35137656164951, Test Loss: 33.115483271611204\n",
      "Epoch [30/50], Train Loss: 26.598650134977746, Test Loss: 30.522541343391715\n",
      "Epoch [40/50], Train Loss: 27.57500536559058, Test Loss: 29.262955876139852\n",
      "Epoch [50/50], Train Loss: 26.911380705286245, Test Loss: 32.02023382310743\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 31.218709176485657, Test Loss: 33.60400757232269\n",
      "Epoch [20/50], Train Loss: 28.073417175793256, Test Loss: 31.44782284327916\n",
      "Epoch [30/50], Train Loss: 27.77074990819712, Test Loss: 32.052093827879276\n",
      "Epoch [40/50], Train Loss: 26.434576653652503, Test Loss: 31.52039247983462\n",
      "Epoch [50/50], Train Loss: 26.687953079723922, Test Loss: 29.420483477703936\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.48051662757749, Test Loss: 47.216399502444574\n",
      "Epoch [20/50], Train Loss: 41.79498745027136, Test Loss: 39.602533414766384\n",
      "Epoch [30/50], Train Loss: 37.698186017646165, Test Loss: 38.65466100519354\n",
      "Epoch [40/50], Train Loss: 34.68268287533619, Test Loss: 36.35879211921196\n",
      "Epoch [50/50], Train Loss: 31.38101855418721, Test Loss: 33.502331523152144\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.2527401158067, Test Loss: 47.541475915289546\n",
      "Epoch [20/50], Train Loss: 41.77740833720223, Test Loss: 39.79034384194907\n",
      "Epoch [30/50], Train Loss: 37.82035512455174, Test Loss: 38.64961926348798\n",
      "Epoch [40/50], Train Loss: 34.19578857421875, Test Loss: 34.5977373742438\n",
      "Epoch [50/50], Train Loss: 31.30583192168689, Test Loss: 32.07872053864715\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.02609900802862, Test Loss: 45.04623908501167\n",
      "Epoch [20/50], Train Loss: 42.63736460951508, Test Loss: 40.87502779279436\n",
      "Epoch [30/50], Train Loss: 38.88071464163358, Test Loss: 39.15828204464603\n",
      "Epoch [40/50], Train Loss: 35.399216798876154, Test Loss: 35.1720437384271\n",
      "Epoch [50/50], Train Loss: 32.12968411054768, Test Loss: 34.21994216720779\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.936310602406984, Test Loss: 46.53807157045835\n",
      "Epoch [20/50], Train Loss: 41.43391365926774, Test Loss: 39.87643561425147\n",
      "Epoch [30/50], Train Loss: 37.25325254846792, Test Loss: 40.06291704054003\n",
      "Epoch [40/50], Train Loss: 33.87171158087058, Test Loss: 36.828466564029846\n",
      "Epoch [50/50], Train Loss: 31.84730220857214, Test Loss: 31.92433706506506\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.3458969241283, Test Loss: 45.87957114677925\n",
      "Epoch [20/50], Train Loss: 41.76592493526271, Test Loss: 40.94004390766094\n",
      "Epoch [30/50], Train Loss: 37.89803666912142, Test Loss: 37.159324720308376\n",
      "Epoch [40/50], Train Loss: 34.599324523425494, Test Loss: 32.925966287588146\n",
      "Epoch [50/50], Train Loss: 31.62283895523822, Test Loss: 33.52071831443093\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.2076702305528, Test Loss: 44.55473877547623\n",
      "Epoch [20/50], Train Loss: 43.08582013239626, Test Loss: 41.158154475224485\n",
      "Epoch [30/50], Train Loss: 38.873131986524236, Test Loss: 38.89189202444894\n",
      "Epoch [40/50], Train Loss: 35.539139081611005, Test Loss: 35.42583420988801\n",
      "Epoch [50/50], Train Loss: 32.48396771540407, Test Loss: 32.51437526554256\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.77399434574315, Test Loss: 45.6896996436181\n",
      "Epoch [20/50], Train Loss: 42.69596306222384, Test Loss: 40.370802198137554\n",
      "Epoch [30/50], Train Loss: 38.330602326940316, Test Loss: 36.138147477979786\n",
      "Epoch [40/50], Train Loss: 34.70740731661437, Test Loss: 36.945533925836735\n",
      "Epoch [50/50], Train Loss: 32.23696331586994, Test Loss: 32.92008865653695\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.423990468509864, Test Loss: 47.787044525146484\n",
      "Epoch [20/50], Train Loss: 41.973431033775455, Test Loss: 38.208590173102046\n",
      "Epoch [30/50], Train Loss: 38.20155955455342, Test Loss: 39.50960208843281\n",
      "Epoch [40/50], Train Loss: 34.830051372090324, Test Loss: 33.13044954275156\n",
      "Epoch [50/50], Train Loss: 32.24311983702613, Test Loss: 31.67504835748053\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.92297951119845, Test Loss: 44.827858466606635\n",
      "Epoch [20/50], Train Loss: 42.61392999867924, Test Loss: 40.34211067100624\n",
      "Epoch [30/50], Train Loss: 38.72896741022829, Test Loss: 36.01508953044941\n",
      "Epoch [40/50], Train Loss: 35.15042625802462, Test Loss: 35.74300389475637\n",
      "Epoch [50/50], Train Loss: 32.202498839331454, Test Loss: 32.08082337812944\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.1013644734367, Test Loss: 31.582727506563263\n",
      "Epoch [20/50], Train Loss: 25.37694502033171, Test Loss: 28.443260935993937\n",
      "Epoch [30/50], Train Loss: 24.314795809886494, Test Loss: 32.26852538368919\n",
      "Epoch [40/50], Train Loss: 23.924424230856975, Test Loss: 27.79582571054434\n",
      "Epoch [50/50], Train Loss: 23.89510402366763, Test Loss: 28.359283744514762\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.59920934458248, Test Loss: 29.31408285165762\n",
      "Epoch [20/50], Train Loss: 26.026000026014984, Test Loss: 29.744908791083795\n",
      "Epoch [30/50], Train Loss: 23.94585277369765, Test Loss: 29.637424667160232\n",
      "Epoch [40/50], Train Loss: 24.895440042214315, Test Loss: 29.233883993966238\n",
      "Epoch [50/50], Train Loss: 23.940324714535574, Test Loss: 28.889640064982625\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.365611091988985, Test Loss: 30.852423432585482\n",
      "Epoch [20/50], Train Loss: 25.23330448338243, Test Loss: 29.865615597018948\n",
      "Epoch [30/50], Train Loss: 25.04201175502089, Test Loss: 27.794947785216493\n",
      "Epoch [40/50], Train Loss: 23.713296127319335, Test Loss: 29.862261784541143\n",
      "Epoch [50/50], Train Loss: 24.181538847626232, Test Loss: 28.33313154245352\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.347649833804272, Test Loss: 30.629304984947304\n",
      "Epoch [20/50], Train Loss: 25.72533940799901, Test Loss: 28.37265007217209\n",
      "Epoch [30/50], Train Loss: 25.471707453493213, Test Loss: 30.237711175695644\n",
      "Epoch [40/50], Train Loss: 24.75139600409836, Test Loss: 28.158802156324512\n",
      "Epoch [50/50], Train Loss: 25.761585773405482, Test Loss: 28.432472179462383\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.64599474297195, Test Loss: 34.429259015368174\n",
      "Epoch [20/50], Train Loss: 26.147712538672277, Test Loss: 30.888153744982436\n",
      "Epoch [30/50], Train Loss: 26.105731438808753, Test Loss: 29.011708494904752\n",
      "Epoch [40/50], Train Loss: 24.31656153944672, Test Loss: 28.010646423736176\n",
      "Epoch [50/50], Train Loss: 23.723196567472865, Test Loss: 28.689257213047572\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.359775893414607, Test Loss: 31.170702005361584\n",
      "Epoch [20/50], Train Loss: 25.628728034848073, Test Loss: 30.433061896980583\n",
      "Epoch [30/50], Train Loss: 24.856402825527503, Test Loss: 28.26534684911951\n",
      "Epoch [40/50], Train Loss: 24.898699294543658, Test Loss: 28.937175255317193\n",
      "Epoch [50/50], Train Loss: 24.44941150477675, Test Loss: 28.958466814709947\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.043452278512422, Test Loss: 32.769257087212104\n",
      "Epoch [20/50], Train Loss: 26.20762025801862, Test Loss: 30.35179363597523\n",
      "Epoch [30/50], Train Loss: 25.911649410060196, Test Loss: 31.249860094739244\n",
      "Epoch [40/50], Train Loss: 26.330996922977636, Test Loss: 29.771536393599078\n",
      "Epoch [50/50], Train Loss: 25.188296452506645, Test Loss: 29.291491297932414\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.19699583835289, Test Loss: 34.82329415655755\n",
      "Epoch [20/50], Train Loss: 26.7171757432281, Test Loss: 32.58342423377099\n",
      "Epoch [30/50], Train Loss: 26.312905020791977, Test Loss: 30.196895475511425\n",
      "Epoch [40/50], Train Loss: 25.606840802802413, Test Loss: 28.623344000283772\n",
      "Epoch [50/50], Train Loss: 26.037789629326493, Test Loss: 29.536658225121435\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.405887428658907, Test Loss: 36.005776392949095\n",
      "Epoch [20/50], Train Loss: 26.567873001098633, Test Loss: 32.440767263437245\n",
      "Epoch [30/50], Train Loss: 25.849890067929127, Test Loss: 29.313725706818815\n",
      "Epoch [40/50], Train Loss: 26.990022946967454, Test Loss: 29.313806187022816\n",
      "Epoch [50/50], Train Loss: 26.793980807945378, Test Loss: 29.016343822726956\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.219018379586643, Test Loss: 30.544139837289784\n",
      "Epoch [20/50], Train Loss: 26.443316556586595, Test Loss: 32.096670150756836\n",
      "Epoch [30/50], Train Loss: 25.83091456303831, Test Loss: 31.253487302111342\n",
      "Epoch [40/50], Train Loss: 25.528966547231207, Test Loss: 28.244151400281236\n",
      "Epoch [50/50], Train Loss: 25.262545226050204, Test Loss: 28.49701589113706\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.722288225517897, Test Loss: 28.95934382351962\n",
      "Epoch [20/50], Train Loss: 26.330286989055697, Test Loss: 31.54961635540058\n",
      "Epoch [30/50], Train Loss: 25.91752938442543, Test Loss: 29.674812564602146\n",
      "Epoch [40/50], Train Loss: 25.707904221581632, Test Loss: 27.73595460668787\n",
      "Epoch [50/50], Train Loss: 26.380891043240908, Test Loss: 28.10908607383827\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.043573448306223, Test Loss: 30.336490135688287\n",
      "Epoch [20/50], Train Loss: 27.570860740786692, Test Loss: 29.02174627626097\n",
      "Epoch [30/50], Train Loss: 26.741372317955143, Test Loss: 28.976706071333453\n",
      "Epoch [40/50], Train Loss: 25.03758728777776, Test Loss: 28.63142134926536\n",
      "Epoch [50/50], Train Loss: 25.575682405565605, Test Loss: 27.990353943465593\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.965846371259847, Test Loss: 32.53551381594175\n",
      "Epoch [20/50], Train Loss: 29.39001569904265, Test Loss: 35.71894841379934\n",
      "Epoch [30/50], Train Loss: 26.483663008642978, Test Loss: 30.087758695924435\n",
      "Epoch [40/50], Train Loss: 26.939472648745678, Test Loss: 31.61937280134721\n",
      "Epoch [50/50], Train Loss: 26.006744165889554, Test Loss: 29.9904112134661\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.459396143428616, Test Loss: 31.296377305860645\n",
      "Epoch [20/50], Train Loss: 27.070497662903833, Test Loss: 31.688388527213753\n",
      "Epoch [30/50], Train Loss: 26.48563238675477, Test Loss: 29.353901206672965\n",
      "Epoch [40/50], Train Loss: 26.036225984917312, Test Loss: 29.808402222472353\n",
      "Epoch [50/50], Train Loss: 27.047690000690398, Test Loss: 29.62950206112552\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.47791058274566, Test Loss: 31.456699123630276\n",
      "Epoch [20/50], Train Loss: 26.76137525839884, Test Loss: 30.676653056949764\n",
      "Epoch [30/50], Train Loss: 25.93775293944312, Test Loss: 33.75361214674913\n",
      "Epoch [40/50], Train Loss: 26.05743938508581, Test Loss: 30.517481816279425\n",
      "Epoch [50/50], Train Loss: 25.41908296678887, Test Loss: 29.854404870565837\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.881391644086992, Test Loss: 37.02492929433848\n",
      "Epoch [20/50], Train Loss: 28.32259825409436, Test Loss: 31.753004049325916\n",
      "Epoch [30/50], Train Loss: 26.934216446172996, Test Loss: 29.765915239012088\n",
      "Epoch [40/50], Train Loss: 26.819243209088434, Test Loss: 29.182336435689553\n",
      "Epoch [50/50], Train Loss: 26.062617286306914, Test Loss: 29.17738163935674\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.10608696859391, Test Loss: 33.089342662266326\n",
      "Epoch [20/50], Train Loss: 28.52763663119957, Test Loss: 37.450614532867036\n",
      "Epoch [30/50], Train Loss: 27.879635488791543, Test Loss: 29.385260544814074\n",
      "Epoch [40/50], Train Loss: 27.254990361948483, Test Loss: 33.50878435605532\n",
      "Epoch [50/50], Train Loss: 27.789545953469197, Test Loss: 28.377606478604402\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.442446549212345, Test Loss: 33.67381460016424\n",
      "Epoch [20/50], Train Loss: 26.98910327348553, Test Loss: 32.540286448094754\n",
      "Epoch [30/50], Train Loss: 27.68104546343694, Test Loss: 31.433899668904093\n",
      "Epoch [40/50], Train Loss: 26.309997396000096, Test Loss: 30.546779434402268\n",
      "Epoch [50/50], Train Loss: 26.412875510043786, Test Loss: 33.18048531668527\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.889276185582894, Test Loss: 47.77728271484375\n",
      "Epoch [20/50], Train Loss: 45.03220242359599, Test Loss: 44.4508056640625\n",
      "Epoch [30/50], Train Loss: 42.35697603069368, Test Loss: 40.67306900024414\n",
      "Epoch [40/50], Train Loss: 39.475235385582096, Test Loss: 39.8646240234375\n",
      "Epoch [50/50], Train Loss: 37.22260850374816, Test Loss: 34.435768127441406\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.03842624601771, Test Loss: 48.106014251708984\n",
      "Epoch [20/50], Train Loss: 45.20139740490522, Test Loss: 44.33224868774414\n",
      "Epoch [30/50], Train Loss: 42.41783158349209, Test Loss: 42.33525848388672\n",
      "Epoch [40/50], Train Loss: 39.67135177362161, Test Loss: 37.50913619995117\n",
      "Epoch [50/50], Train Loss: 37.58685727979316, Test Loss: 36.36967468261719\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.37031244997118, Test Loss: 48.27052688598633\n",
      "Epoch [20/50], Train Loss: 45.79760031778304, Test Loss: 43.80375671386719\n",
      "Epoch [30/50], Train Loss: 43.09399597918401, Test Loss: 39.827938079833984\n",
      "Epoch [40/50], Train Loss: 40.72638658617364, Test Loss: 36.54180145263672\n",
      "Epoch [50/50], Train Loss: 38.15059594326332, Test Loss: 37.973304748535156\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.1910623394075, Test Loss: 50.49711608886719\n",
      "Epoch [20/50], Train Loss: 45.461433673295815, Test Loss: 43.34611511230469\n",
      "Epoch [30/50], Train Loss: 42.68861796895011, Test Loss: 41.74982452392578\n",
      "Epoch [40/50], Train Loss: 40.01753399958376, Test Loss: 34.7714958190918\n",
      "Epoch [50/50], Train Loss: 37.53234329223633, Test Loss: 37.19673156738281\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.882512527215674, Test Loss: 48.747047424316406\n",
      "Epoch [20/50], Train Loss: 44.987748380567204, Test Loss: 43.94870376586914\n",
      "Epoch [30/50], Train Loss: 42.25624258322794, Test Loss: 40.79680252075195\n",
      "Epoch [40/50], Train Loss: 39.59219324080671, Test Loss: 39.36143493652344\n",
      "Epoch [50/50], Train Loss: 37.17153400358607, Test Loss: 39.07029342651367\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.15634708091861, Test Loss: 48.18739318847656\n",
      "Epoch [20/50], Train Loss: 45.66205762644283, Test Loss: 42.91399383544922\n",
      "Epoch [30/50], Train Loss: 43.15270878526031, Test Loss: 37.183719635009766\n",
      "Epoch [40/50], Train Loss: 40.5222182977395, Test Loss: 39.486934661865234\n",
      "Epoch [50/50], Train Loss: 38.317868467237126, Test Loss: 36.614044189453125\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.51979724071065, Test Loss: 46.98247528076172\n",
      "Epoch [20/50], Train Loss: 45.924139104123974, Test Loss: 42.598426818847656\n",
      "Epoch [30/50], Train Loss: 43.17091178268683, Test Loss: 41.427162170410156\n",
      "Epoch [40/50], Train Loss: 40.875809566310195, Test Loss: 36.34365463256836\n",
      "Epoch [50/50], Train Loss: 38.45029155543593, Test Loss: 40.07980728149414\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.08182084130459, Test Loss: 47.80705642700195\n",
      "Epoch [20/50], Train Loss: 45.343682486112, Test Loss: 48.94602966308594\n",
      "Epoch [30/50], Train Loss: 42.5046350698002, Test Loss: 41.78634262084961\n",
      "Epoch [40/50], Train Loss: 39.937439677754384, Test Loss: 39.143428802490234\n",
      "Epoch [50/50], Train Loss: 37.45055763369701, Test Loss: 35.90766525268555\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 48.260525575231334, Test Loss: 46.633888244628906\n",
      "Epoch [20/50], Train Loss: 45.72570129144387, Test Loss: 43.74001693725586\n",
      "Epoch [30/50], Train Loss: 42.97743193829646, Test Loss: 41.92060852050781\n",
      "Epoch [40/50], Train Loss: 40.377346263948034, Test Loss: 38.905948638916016\n",
      "Epoch [50/50], Train Loss: 37.967458868808436, Test Loss: 37.42781066894531\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.89724123595191, Test Loss: 36.31739044189453\n",
      "Epoch [20/50], Train Loss: 26.96789379432553, Test Loss: 29.861652374267578\n",
      "Epoch [30/50], Train Loss: 24.265212349813492, Test Loss: 29.843341827392578\n",
      "Epoch [40/50], Train Loss: 24.639154265356847, Test Loss: 29.451108932495117\n",
      "Epoch [50/50], Train Loss: 23.674058363867587, Test Loss: 29.61265754699707\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.675030548846134, Test Loss: 33.964195251464844\n",
      "Epoch [20/50], Train Loss: 26.34679639222192, Test Loss: 31.6049861907959\n",
      "Epoch [30/50], Train Loss: 24.51900188883797, Test Loss: 27.771770477294922\n",
      "Epoch [40/50], Train Loss: 24.305127841136496, Test Loss: 27.511817932128906\n",
      "Epoch [50/50], Train Loss: 24.330128660358366, Test Loss: 28.785400390625\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.33916223869949, Test Loss: 35.807586669921875\n",
      "Epoch [20/50], Train Loss: 25.144110920390144, Test Loss: 32.191184997558594\n",
      "Epoch [30/50], Train Loss: 24.81211399015833, Test Loss: 29.77947425842285\n",
      "Epoch [40/50], Train Loss: 23.993566694415982, Test Loss: 31.14925193786621\n",
      "Epoch [50/50], Train Loss: 23.469838902207673, Test Loss: 28.449573516845703\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.03030837012119, Test Loss: 35.4825325012207\n",
      "Epoch [20/50], Train Loss: 25.699171529050734, Test Loss: 30.03055191040039\n",
      "Epoch [30/50], Train Loss: 24.828281746536003, Test Loss: 37.46516036987305\n",
      "Epoch [40/50], Train Loss: 25.247592156832336, Test Loss: 29.171016693115234\n",
      "Epoch [50/50], Train Loss: 24.202763979552223, Test Loss: 33.0027961730957\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.511059545298092, Test Loss: 36.25983810424805\n",
      "Epoch [20/50], Train Loss: 25.800410717823468, Test Loss: 30.66717529296875\n",
      "Epoch [30/50], Train Loss: 26.00581676295546, Test Loss: 32.9126091003418\n",
      "Epoch [40/50], Train Loss: 25.595031850846087, Test Loss: 29.02119255065918\n",
      "Epoch [50/50], Train Loss: 24.236349781223986, Test Loss: 28.080873489379883\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.22860899753258, Test Loss: 34.40629196166992\n",
      "Epoch [20/50], Train Loss: 24.975837232245773, Test Loss: 30.093351364135742\n",
      "Epoch [30/50], Train Loss: 24.579835966766858, Test Loss: 28.577180862426758\n",
      "Epoch [40/50], Train Loss: 24.604022723338645, Test Loss: 29.286008834838867\n",
      "Epoch [50/50], Train Loss: 24.159414810430807, Test Loss: 27.703384399414062\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.804865615094293, Test Loss: 36.77373123168945\n",
      "Epoch [20/50], Train Loss: 29.030649366535123, Test Loss: 35.79451370239258\n",
      "Epoch [30/50], Train Loss: 25.458313451047804, Test Loss: 30.919410705566406\n",
      "Epoch [40/50], Train Loss: 25.837268654244845, Test Loss: 29.24376106262207\n",
      "Epoch [50/50], Train Loss: 25.987693961721952, Test Loss: 29.51627540588379\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.352838184794443, Test Loss: 36.97837448120117\n",
      "Epoch [20/50], Train Loss: 26.947070237456774, Test Loss: 34.990726470947266\n",
      "Epoch [30/50], Train Loss: 25.483279418945312, Test Loss: 29.844196319580078\n",
      "Epoch [40/50], Train Loss: 25.722681339451523, Test Loss: 29.917325973510742\n",
      "Epoch [50/50], Train Loss: 25.188215674728642, Test Loss: 29.198577880859375\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.683516242855884, Test Loss: 38.36518859863281\n",
      "Epoch [20/50], Train Loss: 28.355096573126122, Test Loss: 32.976314544677734\n",
      "Epoch [30/50], Train Loss: 26.088840734763224, Test Loss: 35.91468811035156\n",
      "Epoch [40/50], Train Loss: 26.398858917736618, Test Loss: 29.920997619628906\n",
      "Epoch [50/50], Train Loss: 25.244330822053502, Test Loss: 33.35272216796875\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.999040147124745, Test Loss: 33.03115463256836\n",
      "Epoch [20/50], Train Loss: 25.459287005565205, Test Loss: 33.90794372558594\n",
      "Epoch [30/50], Train Loss: 26.22298318206287, Test Loss: 33.91585922241211\n",
      "Epoch [40/50], Train Loss: 26.193918484547098, Test Loss: 29.903257369995117\n",
      "Epoch [50/50], Train Loss: 25.24473746878202, Test Loss: 32.28720474243164\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.57870940536749, Test Loss: 30.81532859802246\n",
      "Epoch [20/50], Train Loss: 26.5655186012143, Test Loss: 29.90486717224121\n",
      "Epoch [30/50], Train Loss: 25.660268264520365, Test Loss: 28.275136947631836\n",
      "Epoch [40/50], Train Loss: 27.90075719864642, Test Loss: 29.01639175415039\n",
      "Epoch [50/50], Train Loss: 24.168649066862514, Test Loss: 34.04136657714844\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.306250137579244, Test Loss: 31.981477737426758\n",
      "Epoch [20/50], Train Loss: 26.157991847053903, Test Loss: 28.207242965698242\n",
      "Epoch [30/50], Train Loss: 25.53047167043217, Test Loss: 34.3030891418457\n",
      "Epoch [40/50], Train Loss: 25.540453032196545, Test Loss: 28.930936813354492\n",
      "Epoch [50/50], Train Loss: 24.70781283769451, Test Loss: 28.592798233032227\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.180877022665054, Test Loss: 34.11863327026367\n",
      "Epoch [20/50], Train Loss: 27.24419954643875, Test Loss: 32.359405517578125\n",
      "Epoch [30/50], Train Loss: 27.42898986065974, Test Loss: 31.467266082763672\n",
      "Epoch [40/50], Train Loss: 26.38632092085041, Test Loss: 28.994115829467773\n",
      "Epoch [50/50], Train Loss: 25.195124941966572, Test Loss: 29.980867385864258\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.284999647296843, Test Loss: 35.038047790527344\n",
      "Epoch [20/50], Train Loss: 27.001380676519677, Test Loss: 30.835641860961914\n",
      "Epoch [30/50], Train Loss: 27.076490846227426, Test Loss: 28.607120513916016\n",
      "Epoch [40/50], Train Loss: 26.032030299452483, Test Loss: 31.70138168334961\n",
      "Epoch [50/50], Train Loss: 25.488747249665806, Test Loss: 31.193819046020508\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.22597035892674, Test Loss: 32.67738723754883\n",
      "Epoch [20/50], Train Loss: 27.563487274920355, Test Loss: 32.63529968261719\n",
      "Epoch [30/50], Train Loss: 26.984548356103115, Test Loss: 32.646812438964844\n",
      "Epoch [40/50], Train Loss: 26.820986194297916, Test Loss: 30.997957229614258\n",
      "Epoch [50/50], Train Loss: 24.755247510065796, Test Loss: 29.957759857177734\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.622404461219663, Test Loss: 37.4932861328125\n",
      "Epoch [20/50], Train Loss: 27.82069303168625, Test Loss: 32.03342056274414\n",
      "Epoch [30/50], Train Loss: 27.975757148617603, Test Loss: 32.856876373291016\n",
      "Epoch [40/50], Train Loss: 28.16194803206647, Test Loss: 29.63442611694336\n",
      "Epoch [50/50], Train Loss: 26.13080343027584, Test Loss: 30.628345489501953\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.37114788117956, Test Loss: 36.189579010009766\n",
      "Epoch [20/50], Train Loss: 27.559225732772077, Test Loss: 43.791481018066406\n",
      "Epoch [30/50], Train Loss: 26.59480091782867, Test Loss: 32.2066764831543\n",
      "Epoch [40/50], Train Loss: 26.112095892233928, Test Loss: 30.828441619873047\n",
      "Epoch [50/50], Train Loss: 26.394349820496608, Test Loss: 31.58344268798828\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 31.948138377705558, Test Loss: 40.89833450317383\n",
      "Epoch [20/50], Train Loss: 27.621484869034564, Test Loss: 35.186405181884766\n",
      "Epoch [30/50], Train Loss: 27.937500143832846, Test Loss: 35.114158630371094\n",
      "Epoch [40/50], Train Loss: 26.7234763848977, Test Loss: 34.002418518066406\n",
      "Epoch [50/50], Train Loss: 26.28204096184402, Test Loss: 30.804569244384766\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.43340723006452, Test Loss: 40.02863524796127\n",
      "Epoch [20/50], Train Loss: 31.18215244480821, Test Loss: 33.46233898014217\n",
      "Epoch [30/50], Train Loss: 26.902556372470542, Test Loss: 29.654393134179053\n",
      "Epoch [40/50], Train Loss: 25.770212142193905, Test Loss: 27.819953274417234\n",
      "Epoch [50/50], Train Loss: 26.052989209284547, Test Loss: 28.190615418669466\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.41130254151391, Test Loss: 44.96579638394442\n",
      "Epoch [20/50], Train Loss: 30.484072194333937, Test Loss: 31.729328750015853\n",
      "Epoch [30/50], Train Loss: 27.099218759380403, Test Loss: 28.21642087961172\n",
      "Epoch [40/50], Train Loss: 25.403536074278783, Test Loss: 28.794402432132078\n",
      "Epoch [50/50], Train Loss: 25.469863091140496, Test Loss: 28.381093260529752\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.77276323662429, Test Loss: 38.43150324635691\n",
      "Epoch [20/50], Train Loss: 32.27593811535444, Test Loss: 33.34504100254604\n",
      "Epoch [30/50], Train Loss: 26.62978159169682, Test Loss: 29.478323775452452\n",
      "Epoch [40/50], Train Loss: 26.092435524111888, Test Loss: 28.646445534446023\n",
      "Epoch [50/50], Train Loss: 25.36902235062396, Test Loss: 28.01052576535708\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 38.58024290741467, Test Loss: 40.095735029740766\n",
      "Epoch [20/50], Train Loss: 30.70910208029825, Test Loss: 31.082379972779904\n",
      "Epoch [30/50], Train Loss: 26.40458336501825, Test Loss: 29.679659533810305\n",
      "Epoch [40/50], Train Loss: 25.569420498707256, Test Loss: 27.74240473957805\n",
      "Epoch [50/50], Train Loss: 25.62466743344166, Test Loss: 28.4545134754924\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.25746550012807, Test Loss: 38.913471271465355\n",
      "Epoch [20/50], Train Loss: 31.418127816622373, Test Loss: 29.74562523581765\n",
      "Epoch [30/50], Train Loss: 27.41214129338499, Test Loss: 28.108990607323584\n",
      "Epoch [40/50], Train Loss: 25.90566463157779, Test Loss: 29.265126116864092\n",
      "Epoch [50/50], Train Loss: 25.162821260045785, Test Loss: 26.891702131791547\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.596476095230855, Test Loss: 38.44900116363129\n",
      "Epoch [20/50], Train Loss: 31.287367185999134, Test Loss: 31.964949298214602\n",
      "Epoch [30/50], Train Loss: 27.973097022635038, Test Loss: 29.06269308808562\n",
      "Epoch [40/50], Train Loss: 25.726558647781122, Test Loss: 29.092339305134562\n",
      "Epoch [50/50], Train Loss: 25.70576138105549, Test Loss: 27.984011018431033\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.38325714361472, Test Loss: 36.33610475218141\n",
      "Epoch [20/50], Train Loss: 31.111811966192526, Test Loss: 32.719787647197776\n",
      "Epoch [30/50], Train Loss: 27.41324926595219, Test Loss: 31.068548821783686\n",
      "Epoch [40/50], Train Loss: 27.088431398985815, Test Loss: 28.998832454929104\n",
      "Epoch [50/50], Train Loss: 25.603460874713836, Test Loss: 27.26248538029658\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.04727569329934, Test Loss: 39.56913990169377\n",
      "Epoch [20/50], Train Loss: 31.249660629522605, Test Loss: 33.18132071061568\n",
      "Epoch [30/50], Train Loss: 26.866608604055937, Test Loss: 29.80519889237045\n",
      "Epoch [40/50], Train Loss: 25.769078851918707, Test Loss: 28.959975626561548\n",
      "Epoch [50/50], Train Loss: 25.26937304012111, Test Loss: 29.623843973333184\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 39.60510040033059, Test Loss: 38.40957606922496\n",
      "Epoch [20/50], Train Loss: 32.08938803750961, Test Loss: 31.09182253750888\n",
      "Epoch [30/50], Train Loss: 27.520854862400743, Test Loss: 29.409854566896115\n",
      "Epoch [40/50], Train Loss: 26.358030994602892, Test Loss: 27.947352718997312\n",
      "Epoch [50/50], Train Loss: 26.521533903528432, Test Loss: 28.075465710132153\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.166229673291816, Test Loss: 38.267122689779704\n",
      "Epoch [20/50], Train Loss: 26.25537919841829, Test Loss: 27.594471572281478\n",
      "Epoch [30/50], Train Loss: 25.75872775218526, Test Loss: 27.877028205178\n",
      "Epoch [40/50], Train Loss: 25.70295172519371, Test Loss: 28.24586380302132\n",
      "Epoch [50/50], Train Loss: 24.90131434456247, Test Loss: 27.484005667946555\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.10907072473745, Test Loss: 28.576021615560954\n",
      "Epoch [20/50], Train Loss: 25.7857165727459, Test Loss: 28.604211113669656\n",
      "Epoch [30/50], Train Loss: 26.18067719506436, Test Loss: 28.950022957541726\n",
      "Epoch [40/50], Train Loss: 25.682819991815286, Test Loss: 27.990776086782482\n",
      "Epoch [50/50], Train Loss: 25.811824923656026, Test Loss: 29.28216448697177\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 25.91344298691046, Test Loss: 30.451826244205623\n",
      "Epoch [20/50], Train Loss: 27.21436342958544, Test Loss: 28.75448142708122\n",
      "Epoch [30/50], Train Loss: 27.036910247802734, Test Loss: 27.90438634699041\n",
      "Epoch [40/50], Train Loss: 26.164783878013736, Test Loss: 28.115155207646357\n",
      "Epoch [50/50], Train Loss: 26.249128435478834, Test Loss: 28.114733832223074\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.21451497312452, Test Loss: 32.163270702609765\n",
      "Epoch [20/50], Train Loss: 26.82726010181865, Test Loss: 31.443182214514003\n",
      "Epoch [30/50], Train Loss: 26.08449778947674, Test Loss: 28.25879106893168\n",
      "Epoch [40/50], Train Loss: 26.50523777946097, Test Loss: 30.532007688051696\n",
      "Epoch [50/50], Train Loss: 25.943366773011256, Test Loss: 27.552268288352273\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.85278408488289, Test Loss: 32.315963670804905\n",
      "Epoch [20/50], Train Loss: 27.265184846471566, Test Loss: 32.74106474046583\n",
      "Epoch [30/50], Train Loss: 25.297440703970487, Test Loss: 29.15019775985123\n",
      "Epoch [40/50], Train Loss: 26.56487082184338, Test Loss: 27.95299933792709\n",
      "Epoch [50/50], Train Loss: 25.734520658899527, Test Loss: 28.660099846976145\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.32389992260542, Test Loss: 36.79354028577929\n",
      "Epoch [20/50], Train Loss: 26.64147056829734, Test Loss: 28.42423287924234\n",
      "Epoch [30/50], Train Loss: 26.113938578621287, Test Loss: 30.92617634364537\n",
      "Epoch [40/50], Train Loss: 25.316141757027047, Test Loss: 28.827636693979237\n",
      "Epoch [50/50], Train Loss: 26.248114288830365, Test Loss: 28.67641258239746\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.10396235106421, Test Loss: 42.63533391580953\n",
      "Epoch [20/50], Train Loss: 26.406147422165166, Test Loss: 35.27401805233646\n",
      "Epoch [30/50], Train Loss: 27.31953200043225, Test Loss: 30.346632053325703\n",
      "Epoch [40/50], Train Loss: 28.00035367871894, Test Loss: 28.659412062013303\n",
      "Epoch [50/50], Train Loss: 25.862027971861792, Test Loss: 29.08130915753253\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.59990674003226, Test Loss: 32.90319749906465\n",
      "Epoch [20/50], Train Loss: 27.161356378774173, Test Loss: 35.892274807025856\n",
      "Epoch [30/50], Train Loss: 26.90114300211922, Test Loss: 29.285198954792765\n",
      "Epoch [40/50], Train Loss: 27.41638603210449, Test Loss: 30.374463316682096\n",
      "Epoch [50/50], Train Loss: 25.627740143947914, Test Loss: 28.533452120694246\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.118324454886015, Test Loss: 32.46622073185908\n",
      "Epoch [20/50], Train Loss: 27.24455903475402, Test Loss: 32.91304026021586\n",
      "Epoch [30/50], Train Loss: 28.092471438548603, Test Loss: 28.868891332056617\n",
      "Epoch [40/50], Train Loss: 26.56578236564261, Test Loss: 29.773954961207007\n",
      "Epoch [50/50], Train Loss: 26.194089989584, Test Loss: 31.033023561750138\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.99003839336458, Test Loss: 30.844924257947252\n",
      "Epoch [20/50], Train Loss: 25.72014781138936, Test Loss: 29.637608986396295\n",
      "Epoch [30/50], Train Loss: 28.155576862272667, Test Loss: 30.03648312060864\n",
      "Epoch [40/50], Train Loss: 27.942005194992316, Test Loss: 27.937352935989182\n",
      "Epoch [50/50], Train Loss: 26.708048904919234, Test Loss: 30.228501901998147\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.319646159938124, Test Loss: 30.572396563245103\n",
      "Epoch [20/50], Train Loss: 28.042375295670308, Test Loss: 30.818265072711103\n",
      "Epoch [30/50], Train Loss: 26.22160768352571, Test Loss: 28.721536537269493\n",
      "Epoch [40/50], Train Loss: 26.69432963386911, Test Loss: 29.289141915061258\n",
      "Epoch [50/50], Train Loss: 26.000601921706902, Test Loss: 28.764577023394697\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.71422170420162, Test Loss: 30.5474488394601\n",
      "Epoch [20/50], Train Loss: 26.7168375671887, Test Loss: 30.79941595994033\n",
      "Epoch [30/50], Train Loss: 27.28562575793657, Test Loss: 29.174314498901367\n",
      "Epoch [40/50], Train Loss: 26.541144524245965, Test Loss: 28.198307532768744\n",
      "Epoch [50/50], Train Loss: 26.35049879980869, Test Loss: 29.03487403671463\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.06994363753522, Test Loss: 32.888374229530235\n",
      "Epoch [20/50], Train Loss: 28.61872220899238, Test Loss: 30.726453260941938\n",
      "Epoch [30/50], Train Loss: 27.302508876362786, Test Loss: 30.99713439445991\n",
      "Epoch [40/50], Train Loss: 26.225638392714202, Test Loss: 28.41149022981718\n",
      "Epoch [50/50], Train Loss: 26.092651135804225, Test Loss: 29.816678183419363\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.261345084768827, Test Loss: 32.737403671462815\n",
      "Epoch [20/50], Train Loss: 27.80890063926822, Test Loss: 31.203186481029956\n",
      "Epoch [30/50], Train Loss: 26.781243558789864, Test Loss: 30.81993667800705\n",
      "Epoch [40/50], Train Loss: 25.925699940665822, Test Loss: 31.89958500552487\n",
      "Epoch [50/50], Train Loss: 26.353022959974947, Test Loss: 30.362947117198598\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.680049596067335, Test Loss: 37.95019461891868\n",
      "Epoch [20/50], Train Loss: 26.372465127413392, Test Loss: 31.172622333873402\n",
      "Epoch [30/50], Train Loss: 26.622503762167007, Test Loss: 29.360682499873175\n",
      "Epoch [40/50], Train Loss: 25.752575420942463, Test Loss: 28.996015028520063\n",
      "Epoch [50/50], Train Loss: 27.525802749883933, Test Loss: 30.594605260081106\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.74818904438957, Test Loss: 33.74017844262061\n",
      "Epoch [20/50], Train Loss: 27.653632711191648, Test Loss: 33.47631635294332\n",
      "Epoch [30/50], Train Loss: 27.503057736256082, Test Loss: 30.46396186135032\n",
      "Epoch [40/50], Train Loss: 27.57933702312532, Test Loss: 29.723394369150135\n",
      "Epoch [50/50], Train Loss: 28.022443421160588, Test Loss: 30.141219745982777\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 31.58768840852331, Test Loss: 31.58534537971794\n",
      "Epoch [20/50], Train Loss: 27.202057910356366, Test Loss: 31.444815202192828\n",
      "Epoch [30/50], Train Loss: 28.90736626797035, Test Loss: 29.3508946802709\n",
      "Epoch [40/50], Train Loss: 27.16026819573074, Test Loss: 31.211879680683086\n",
      "Epoch [50/50], Train Loss: 27.016506357662013, Test Loss: 31.199613348230137\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.705399860319545, Test Loss: 33.12318512061974\n",
      "Epoch [20/50], Train Loss: 29.15512261312516, Test Loss: 32.635003424310064\n",
      "Epoch [30/50], Train Loss: 27.960110748791305, Test Loss: 31.54462621119115\n",
      "Epoch [40/50], Train Loss: 27.184281008360816, Test Loss: 31.200716637945796\n",
      "Epoch [50/50], Train Loss: 27.441612481289223, Test Loss: 29.93731677067744\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.192112944556065, Test Loss: 44.83862289825043\n",
      "Epoch [20/50], Train Loss: 37.14122048675037, Test Loss: 40.18389615145597\n",
      "Epoch [30/50], Train Loss: 31.3604561477411, Test Loss: 35.5057001485453\n",
      "Epoch [40/50], Train Loss: 27.974454404486984, Test Loss: 31.262274382950423\n",
      "Epoch [50/50], Train Loss: 26.12245946164991, Test Loss: 28.649075495732294\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.840107051661754, Test Loss: 45.064423945042996\n",
      "Epoch [20/50], Train Loss: 36.59660289326652, Test Loss: 35.628292083740234\n",
      "Epoch [30/50], Train Loss: 31.221396987164606, Test Loss: 33.23185717595088\n",
      "Epoch [40/50], Train Loss: 28.075968657946976, Test Loss: 29.35613302751021\n",
      "Epoch [50/50], Train Loss: 25.585568587506405, Test Loss: 29.111553093055626\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.60188037059346, Test Loss: 44.60126222882952\n",
      "Epoch [20/50], Train Loss: 37.97136045362129, Test Loss: 36.02041157809171\n",
      "Epoch [30/50], Train Loss: 33.23778347578205, Test Loss: 33.418091612976866\n",
      "Epoch [40/50], Train Loss: 29.28500653876633, Test Loss: 29.05744978669402\n",
      "Epoch [50/50], Train Loss: 27.847972801083426, Test Loss: 29.647692345953608\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.60591923447906, Test Loss: 44.12488977011148\n",
      "Epoch [20/50], Train Loss: 36.788757399261975, Test Loss: 36.90218888319932\n",
      "Epoch [30/50], Train Loss: 31.467475540911565, Test Loss: 35.51417167465408\n",
      "Epoch [40/50], Train Loss: 28.519666246507988, Test Loss: 31.224105934043983\n",
      "Epoch [50/50], Train Loss: 25.90782880939421, Test Loss: 28.61006414735472\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.029965197453734, Test Loss: 42.261092792857774\n",
      "Epoch [20/50], Train Loss: 36.772170220046746, Test Loss: 35.06005908916523\n",
      "Epoch [30/50], Train Loss: 30.958868833448065, Test Loss: 31.662587797486935\n",
      "Epoch [40/50], Train Loss: 28.87106316050545, Test Loss: 29.402102036909625\n",
      "Epoch [50/50], Train Loss: 27.15014108751641, Test Loss: 28.25059053495333\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.72784093637935, Test Loss: 43.37103033685065\n",
      "Epoch [20/50], Train Loss: 37.745440736364145, Test Loss: 35.453557943368885\n",
      "Epoch [30/50], Train Loss: 32.41249944968302, Test Loss: 34.06688660460633\n",
      "Epoch [40/50], Train Loss: 29.116445259969744, Test Loss: 29.797309900259044\n",
      "Epoch [50/50], Train Loss: 27.020950867699796, Test Loss: 29.295721029306385\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 43.97720325657579, Test Loss: 45.820756540670025\n",
      "Epoch [20/50], Train Loss: 36.89165436791592, Test Loss: 42.84511427445845\n",
      "Epoch [30/50], Train Loss: 32.328985420602265, Test Loss: 33.724965727174435\n",
      "Epoch [40/50], Train Loss: 29.147931933793867, Test Loss: 29.14230832186612\n",
      "Epoch [50/50], Train Loss: 26.96391381435707, Test Loss: 30.32554884080763\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 44.90984435785012, Test Loss: 47.05992537659484\n",
      "Epoch [20/50], Train Loss: 37.74622407506724, Test Loss: 37.7095551924272\n",
      "Epoch [30/50], Train Loss: 32.63888168334961, Test Loss: 35.60330680748085\n",
      "Epoch [40/50], Train Loss: 28.51646837328301, Test Loss: 30.00832072171298\n",
      "Epoch [50/50], Train Loss: 26.429865852731172, Test Loss: 29.638940068034383\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 45.21364408399238, Test Loss: 45.714922966895166\n",
      "Epoch [20/50], Train Loss: 38.7644437696113, Test Loss: 37.199356277267654\n",
      "Epoch [30/50], Train Loss: 33.10718870319304, Test Loss: 34.12343158969632\n",
      "Epoch [40/50], Train Loss: 29.820804802316133, Test Loss: 29.25043886358088\n",
      "Epoch [50/50], Train Loss: 26.129613163432136, Test Loss: 30.10881096976144\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.980044024107887, Test Loss: 33.07132458377194\n",
      "Epoch [20/50], Train Loss: 25.56294511263488, Test Loss: 28.96252154065417\n",
      "Epoch [30/50], Train Loss: 24.947288513183594, Test Loss: 28.227868315461393\n",
      "Epoch [40/50], Train Loss: 24.619119287709722, Test Loss: 28.77807582508434\n",
      "Epoch [50/50], Train Loss: 24.295416916393844, Test Loss: 27.91584297279259\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.56779079749936, Test Loss: 31.23501735538631\n",
      "Epoch [20/50], Train Loss: 26.66994074837106, Test Loss: 28.254328863961355\n",
      "Epoch [30/50], Train Loss: 25.164454375720414, Test Loss: 33.53852299281529\n",
      "Epoch [40/50], Train Loss: 24.936233007712442, Test Loss: 30.356089505282316\n",
      "Epoch [50/50], Train Loss: 24.564029687349915, Test Loss: 29.38404861053863\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.099419146678486, Test Loss: 30.577164786202566\n",
      "Epoch [20/50], Train Loss: 26.05729805367892, Test Loss: 28.915274013172496\n",
      "Epoch [30/50], Train Loss: 25.10388678253674, Test Loss: 28.45386195492435\n",
      "Epoch [40/50], Train Loss: 24.58622874900943, Test Loss: 27.995113645281112\n",
      "Epoch [50/50], Train Loss: 24.274582203098984, Test Loss: 28.378128968275988\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.378326634891696, Test Loss: 32.19342801478002\n",
      "Epoch [20/50], Train Loss: 27.230521943139248, Test Loss: 30.744231880485238\n",
      "Epoch [30/50], Train Loss: 25.65049253369941, Test Loss: 28.50208398893282\n",
      "Epoch [40/50], Train Loss: 25.18284062870213, Test Loss: 28.98970921008618\n",
      "Epoch [50/50], Train Loss: 24.685931271412333, Test Loss: 28.67876501207228\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.87594446901415, Test Loss: 33.00695134447766\n",
      "Epoch [20/50], Train Loss: 26.876105861976498, Test Loss: 33.70016313528086\n",
      "Epoch [30/50], Train Loss: 25.54581084954934, Test Loss: 28.332164268989068\n",
      "Epoch [40/50], Train Loss: 26.1344461347236, Test Loss: 35.176240153126905\n",
      "Epoch [50/50], Train Loss: 25.264496331136733, Test Loss: 32.27842479557186\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.100592766433465, Test Loss: 32.16827788910309\n",
      "Epoch [20/50], Train Loss: 25.850217356447313, Test Loss: 29.25903726552988\n",
      "Epoch [30/50], Train Loss: 25.842609674422466, Test Loss: 28.24245289393834\n",
      "Epoch [40/50], Train Loss: 24.792490374455685, Test Loss: 30.723088301621473\n",
      "Epoch [50/50], Train Loss: 24.613506686101193, Test Loss: 28.588791562365248\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.115688361496222, Test Loss: 36.0534603069355\n",
      "Epoch [20/50], Train Loss: 27.242523768690766, Test Loss: 32.2717416936701\n",
      "Epoch [30/50], Train Loss: 26.746450455462345, Test Loss: 31.187143350576427\n",
      "Epoch [40/50], Train Loss: 26.338281656484135, Test Loss: 32.645906547447304\n",
      "Epoch [50/50], Train Loss: 25.936980963534996, Test Loss: 30.147910303883737\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.165664772909196, Test Loss: 35.111467534845524\n",
      "Epoch [20/50], Train Loss: 27.432101402908074, Test Loss: 31.58660264448686\n",
      "Epoch [30/50], Train Loss: 25.45886775782851, Test Loss: 29.910982478748668\n",
      "Epoch [40/50], Train Loss: 26.807355336673925, Test Loss: 30.39315248464609\n",
      "Epoch [50/50], Train Loss: 26.138634353387552, Test Loss: 31.863122717126622\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.417479762092967, Test Loss: 36.33575806060394\n",
      "Epoch [20/50], Train Loss: 27.019356418046794, Test Loss: 31.396460248278334\n",
      "Epoch [30/50], Train Loss: 26.83745825720615, Test Loss: 38.51003867310363\n",
      "Epoch [40/50], Train Loss: 26.174130818101226, Test Loss: 35.79191103848544\n",
      "Epoch [50/50], Train Loss: 25.364465025604748, Test Loss: 31.49629040507527\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.910692527645924, Test Loss: 29.919138623522475\n",
      "Epoch [20/50], Train Loss: 26.888358494492827, Test Loss: 30.731875654938932\n",
      "Epoch [30/50], Train Loss: 27.10032784508877, Test Loss: 29.56268414584073\n",
      "Epoch [40/50], Train Loss: 26.393356460821433, Test Loss: 29.17969072019899\n",
      "Epoch [50/50], Train Loss: 25.53037555256828, Test Loss: 29.905781931691354\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.479965810306737, Test Loss: 31.947479768232867\n",
      "Epoch [20/50], Train Loss: 27.481373971407532, Test Loss: 31.97857906292011\n",
      "Epoch [30/50], Train Loss: 27.875495647993244, Test Loss: 29.413565920544908\n",
      "Epoch [40/50], Train Loss: 27.134419350545915, Test Loss: 28.678419757198977\n",
      "Epoch [50/50], Train Loss: 25.65448829775951, Test Loss: 27.698828610506926\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.07214286679127, Test Loss: 33.12353825259518\n",
      "Epoch [20/50], Train Loss: 27.02640106951604, Test Loss: 33.11964025125875\n",
      "Epoch [30/50], Train Loss: 25.260733882716444, Test Loss: 31.117351705377754\n",
      "Epoch [40/50], Train Loss: 26.69981861427182, Test Loss: 27.852220931610503\n",
      "Epoch [50/50], Train Loss: 25.829452864850154, Test Loss: 31.34568147535448\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.99088845800181, Test Loss: 30.98492991459834\n",
      "Epoch [20/50], Train Loss: 27.850471509089235, Test Loss: 31.221776516406567\n",
      "Epoch [30/50], Train Loss: 27.147202876356783, Test Loss: 29.46480570211039\n",
      "Epoch [40/50], Train Loss: 25.714516742893906, Test Loss: 29.92238188409186\n",
      "Epoch [50/50], Train Loss: 25.697376382546345, Test Loss: 29.389589879419898\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.629770078815397, Test Loss: 32.61562709065227\n",
      "Epoch [20/50], Train Loss: 26.604229073446305, Test Loss: 32.10478200540914\n",
      "Epoch [30/50], Train Loss: 27.03826027541864, Test Loss: 29.788026388589437\n",
      "Epoch [40/50], Train Loss: 26.41461344234279, Test Loss: 28.3777166589514\n",
      "Epoch [50/50], Train Loss: 25.355359987352717, Test Loss: 27.862682144363205\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.87858316390241, Test Loss: 33.6117406820322\n",
      "Epoch [20/50], Train Loss: 27.156413944431993, Test Loss: 30.57806329603319\n",
      "Epoch [30/50], Train Loss: 26.567268321553215, Test Loss: 30.924878281432314\n",
      "Epoch [40/50], Train Loss: 25.821935559882494, Test Loss: 28.892266929923714\n",
      "Epoch [50/50], Train Loss: 25.985669301767818, Test Loss: 29.636824892712877\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.110762480438733, Test Loss: 38.84594795920632\n",
      "Epoch [20/50], Train Loss: 28.023342363951635, Test Loss: 30.66155366773729\n",
      "Epoch [30/50], Train Loss: 28.742857848620805, Test Loss: 35.25705966701755\n",
      "Epoch [40/50], Train Loss: 26.73024613427334, Test Loss: 32.851721330122515\n",
      "Epoch [50/50], Train Loss: 26.35174701878282, Test Loss: 31.79642759050642\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.377479540715452, Test Loss: 37.87199590113256\n",
      "Epoch [20/50], Train Loss: 27.47422420939461, Test Loss: 35.36127566052722\n",
      "Epoch [30/50], Train Loss: 27.396566347215998, Test Loss: 33.531108757118126\n",
      "Epoch [40/50], Train Loss: 27.18992220143803, Test Loss: 29.699582929735058\n",
      "Epoch [50/50], Train Loss: 26.967438369500833, Test Loss: 33.843871351960416\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.569168322203588, Test Loss: 38.45095627029221\n",
      "Epoch [20/50], Train Loss: 28.342679295774367, Test Loss: 34.679626563926796\n",
      "Epoch [30/50], Train Loss: 30.20443164012471, Test Loss: 33.240311015735976\n",
      "Epoch [40/50], Train Loss: 27.73138249506716, Test Loss: 29.70556080805791\n",
      "Epoch [50/50], Train Loss: 28.015667368154055, Test Loss: 31.175818034580775\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.72660236045962, Test Loss: 48.989742279052734\n",
      "Epoch [20/50], Train Loss: 41.79918020279681, Test Loss: 39.689857482910156\n",
      "Epoch [30/50], Train Loss: 37.37657864680056, Test Loss: 39.737640380859375\n",
      "Epoch [40/50], Train Loss: 33.70437634577517, Test Loss: 34.15289306640625\n",
      "Epoch [50/50], Train Loss: 30.836538314819336, Test Loss: 31.264781951904297\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.627426960429204, Test Loss: 49.0462646484375\n",
      "Epoch [20/50], Train Loss: 41.573748116415054, Test Loss: 40.39185333251953\n",
      "Epoch [30/50], Train Loss: 36.61819663125961, Test Loss: 37.25044631958008\n",
      "Epoch [40/50], Train Loss: 33.34623568800629, Test Loss: 34.04616928100586\n",
      "Epoch [50/50], Train Loss: 30.414177866451077, Test Loss: 31.403560638427734\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.111690196052926, Test Loss: 50.43000030517578\n",
      "Epoch [20/50], Train Loss: 42.50665702194464, Test Loss: 39.940879821777344\n",
      "Epoch [30/50], Train Loss: 38.33840034359791, Test Loss: 35.49091339111328\n",
      "Epoch [40/50], Train Loss: 34.64125741427062, Test Loss: 34.64144515991211\n",
      "Epoch [50/50], Train Loss: 31.89399242713803, Test Loss: 36.64317321777344\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.24727855744909, Test Loss: 48.792423248291016\n",
      "Epoch [20/50], Train Loss: 42.62003946773341, Test Loss: 42.655006408691406\n",
      "Epoch [30/50], Train Loss: 38.0425916953165, Test Loss: 37.17343521118164\n",
      "Epoch [40/50], Train Loss: 33.993496597790326, Test Loss: 33.97175216674805\n",
      "Epoch [50/50], Train Loss: 30.825459727302928, Test Loss: 32.295841217041016\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.226102022264826, Test Loss: 49.19727325439453\n",
      "Epoch [20/50], Train Loss: 42.66291288782339, Test Loss: 41.76753616333008\n",
      "Epoch [30/50], Train Loss: 37.80997220649094, Test Loss: 38.36071014404297\n",
      "Epoch [40/50], Train Loss: 35.051019162037335, Test Loss: 35.57502746582031\n",
      "Epoch [50/50], Train Loss: 31.621266649590165, Test Loss: 32.18682098388672\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.12616568393395, Test Loss: 50.413368225097656\n",
      "Epoch [20/50], Train Loss: 42.549882619889054, Test Loss: 41.756736755371094\n",
      "Epoch [30/50], Train Loss: 38.41383878363938, Test Loss: 35.39870071411133\n",
      "Epoch [40/50], Train Loss: 34.768538428134605, Test Loss: 35.93770980834961\n",
      "Epoch [50/50], Train Loss: 31.45422350461366, Test Loss: 33.101070404052734\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.49355052260102, Test Loss: 43.81298828125\n",
      "Epoch [20/50], Train Loss: 42.549406308033426, Test Loss: 40.908180236816406\n",
      "Epoch [30/50], Train Loss: 38.32940517488073, Test Loss: 42.52861404418945\n",
      "Epoch [40/50], Train Loss: 34.70019311123207, Test Loss: 35.97719192504883\n",
      "Epoch [50/50], Train Loss: 31.208636630949428, Test Loss: 35.083946228027344\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 46.642221907318614, Test Loss: 50.858741760253906\n",
      "Epoch [20/50], Train Loss: 41.5023494782995, Test Loss: 43.069705963134766\n",
      "Epoch [30/50], Train Loss: 37.488865761678724, Test Loss: 37.14186096191406\n",
      "Epoch [40/50], Train Loss: 34.111873176449635, Test Loss: 35.326534271240234\n",
      "Epoch [50/50], Train Loss: 30.51864894179047, Test Loss: 35.2413215637207\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 47.20292417182297, Test Loss: 42.26561737060547\n",
      "Epoch [20/50], Train Loss: 42.87903601224305, Test Loss: 42.06105422973633\n",
      "Epoch [30/50], Train Loss: 38.375185444315925, Test Loss: 36.38020706176758\n",
      "Epoch [40/50], Train Loss: 35.44000728169426, Test Loss: 36.43973922729492\n",
      "Epoch [50/50], Train Loss: 32.344587444868246, Test Loss: 32.0028190612793\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.545697590562163, Test Loss: 35.5399169921875\n",
      "Epoch [20/50], Train Loss: 25.242223620805586, Test Loss: 29.253976821899414\n",
      "Epoch [30/50], Train Loss: 24.84843156533163, Test Loss: 31.16754150390625\n",
      "Epoch [40/50], Train Loss: 24.155523318931703, Test Loss: 27.761157989501953\n",
      "Epoch [50/50], Train Loss: 23.452576183881916, Test Loss: 28.33763313293457\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.53834140339836, Test Loss: 36.28965759277344\n",
      "Epoch [20/50], Train Loss: 25.37168818614522, Test Loss: 30.96272850036621\n",
      "Epoch [30/50], Train Loss: 25.09903810845047, Test Loss: 31.7187442779541\n",
      "Epoch [40/50], Train Loss: 24.745562919241483, Test Loss: 30.734025955200195\n",
      "Epoch [50/50], Train Loss: 24.544807471603644, Test Loss: 30.35047149658203\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.550050360257508, Test Loss: 33.13507080078125\n",
      "Epoch [20/50], Train Loss: 25.274672874075467, Test Loss: 30.610084533691406\n",
      "Epoch [30/50], Train Loss: 25.47423961201652, Test Loss: 29.018922805786133\n",
      "Epoch [40/50], Train Loss: 24.937549516021228, Test Loss: 30.6648006439209\n",
      "Epoch [50/50], Train Loss: 24.43731080977643, Test Loss: 28.726219177246094\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.447549938764727, Test Loss: 44.61380386352539\n",
      "Epoch [20/50], Train Loss: 25.89083801019387, Test Loss: 30.156211853027344\n",
      "Epoch [30/50], Train Loss: 25.39565583526111, Test Loss: 38.84282684326172\n",
      "Epoch [40/50], Train Loss: 26.46323679314285, Test Loss: 32.003536224365234\n",
      "Epoch [50/50], Train Loss: 25.128506788660268, Test Loss: 30.79357147216797\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 26.72644573899566, Test Loss: 36.13145446777344\n",
      "Epoch [20/50], Train Loss: 26.750564037385534, Test Loss: 30.24418830871582\n",
      "Epoch [30/50], Train Loss: 25.20601438928823, Test Loss: 29.949970245361328\n",
      "Epoch [40/50], Train Loss: 25.591926912401544, Test Loss: 29.25742530822754\n",
      "Epoch [50/50], Train Loss: 24.499542298864146, Test Loss: 29.908580780029297\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.8077052006956, Test Loss: 34.21371078491211\n",
      "Epoch [20/50], Train Loss: 26.4984182076376, Test Loss: 29.570796966552734\n",
      "Epoch [30/50], Train Loss: 25.34746684090036, Test Loss: 28.663909912109375\n",
      "Epoch [40/50], Train Loss: 27.04390603987897, Test Loss: 27.691999435424805\n",
      "Epoch [50/50], Train Loss: 24.51843390542953, Test Loss: 28.19782066345215\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.941632342729413, Test Loss: 38.43980026245117\n",
      "Epoch [20/50], Train Loss: 27.294059028000127, Test Loss: 32.75712966918945\n",
      "Epoch [30/50], Train Loss: 26.41361536119805, Test Loss: 31.45685386657715\n",
      "Epoch [40/50], Train Loss: 26.202460592301165, Test Loss: 29.67107582092285\n",
      "Epoch [50/50], Train Loss: 25.533274059608335, Test Loss: 30.971939086914062\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.601925321485176, Test Loss: 38.35307693481445\n",
      "Epoch [20/50], Train Loss: 27.035076178879034, Test Loss: 34.351844787597656\n",
      "Epoch [30/50], Train Loss: 26.513809272891184, Test Loss: 35.01201248168945\n",
      "Epoch [40/50], Train Loss: 26.18952684246126, Test Loss: 29.619192123413086\n",
      "Epoch [50/50], Train Loss: 25.217704691652393, Test Loss: 30.78457260131836\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 28.943618874471696, Test Loss: 39.149024963378906\n",
      "Epoch [20/50], Train Loss: 27.1670358939249, Test Loss: 33.17612838745117\n",
      "Epoch [30/50], Train Loss: 26.663987344210266, Test Loss: 33.98638916015625\n",
      "Epoch [40/50], Train Loss: 27.12744213166784, Test Loss: 43.320255279541016\n",
      "Epoch [50/50], Train Loss: 25.312107286296907, Test Loss: 32.22517013549805\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.048001980390705, Test Loss: 62.74637222290039\n",
      "Epoch [20/50], Train Loss: 26.405889711223665, Test Loss: 30.383386611938477\n",
      "Epoch [30/50], Train Loss: 25.92176943919698, Test Loss: 29.231042861938477\n",
      "Epoch [40/50], Train Loss: 25.31474892663174, Test Loss: 29.933551788330078\n",
      "Epoch [50/50], Train Loss: 25.586759998759284, Test Loss: 29.41938018798828\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.339452324538936, Test Loss: 34.265464782714844\n",
      "Epoch [20/50], Train Loss: 25.863397767113856, Test Loss: 30.23395347595215\n",
      "Epoch [30/50], Train Loss: 27.006944312423958, Test Loss: 29.13877296447754\n",
      "Epoch [40/50], Train Loss: 25.41106993878474, Test Loss: 29.332317352294922\n",
      "Epoch [50/50], Train Loss: 26.174260517808257, Test Loss: 28.27404022216797\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.60541820838803, Test Loss: 31.606937408447266\n",
      "Epoch [20/50], Train Loss: 27.00280719194256, Test Loss: 31.539243698120117\n",
      "Epoch [30/50], Train Loss: 25.937454229886413, Test Loss: 33.10731887817383\n",
      "Epoch [40/50], Train Loss: 25.882453605776927, Test Loss: 31.16689109802246\n",
      "Epoch [50/50], Train Loss: 24.61928626513872, Test Loss: 34.58405685424805\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 27.50722417987761, Test Loss: 34.977996826171875\n",
      "Epoch [20/50], Train Loss: 26.61985953909452, Test Loss: 33.14236831665039\n",
      "Epoch [30/50], Train Loss: 27.574785945454583, Test Loss: 29.270532608032227\n",
      "Epoch [40/50], Train Loss: 26.57769031211978, Test Loss: 35.30512237548828\n",
      "Epoch [50/50], Train Loss: 25.71235595452981, Test Loss: 28.294343948364258\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.055903544191455, Test Loss: 33.78492736816406\n",
      "Epoch [20/50], Train Loss: 28.50501106137135, Test Loss: 29.846961975097656\n",
      "Epoch [30/50], Train Loss: 25.76449680015689, Test Loss: 33.3171501159668\n",
      "Epoch [40/50], Train Loss: 28.5075318883677, Test Loss: 31.870553970336914\n",
      "Epoch [50/50], Train Loss: 25.72400981090108, Test Loss: 30.664522171020508\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 29.147683928442785, Test Loss: 35.90805435180664\n",
      "Epoch [20/50], Train Loss: 28.906244621902214, Test Loss: 31.521703720092773\n",
      "Epoch [30/50], Train Loss: 27.831185700463468, Test Loss: 32.12309265136719\n",
      "Epoch [40/50], Train Loss: 26.45612464029281, Test Loss: 46.62483215332031\n",
      "Epoch [50/50], Train Loss: 27.933547823546363, Test Loss: 29.329843521118164\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 31.1420304657983, Test Loss: 35.12177276611328\n",
      "Epoch [20/50], Train Loss: 28.826018299040246, Test Loss: 35.38302993774414\n",
      "Epoch [30/50], Train Loss: 27.132503503267884, Test Loss: 32.84490203857422\n",
      "Epoch [40/50], Train Loss: 27.615423346347495, Test Loss: 33.10197830200195\n",
      "Epoch [50/50], Train Loss: 26.407514841048442, Test Loss: 30.838632583618164\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 30.701847432871332, Test Loss: 37.567726135253906\n",
      "Epoch [20/50], Train Loss: 30.093160579243644, Test Loss: 39.63268280029297\n",
      "Epoch [30/50], Train Loss: 28.78482459021396, Test Loss: 31.28641128540039\n",
      "Epoch [40/50], Train Loss: 28.01914128788182, Test Loss: 37.23094940185547\n",
      "Epoch [50/50], Train Loss: 26.402723030965838, Test Loss: 43.60469055175781\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 50\n",
      "Epoch [10/50], Train Loss: 31.466193202284515, Test Loss: 36.56254577636719\n",
      "Epoch [20/50], Train Loss: 29.89404981019067, Test Loss: 42.501991271972656\n",
      "Epoch [30/50], Train Loss: 26.57711853277488, Test Loss: 32.06052780151367\n",
      "Epoch [40/50], Train Loss: 27.69681098187556, Test Loss: 43.83053970336914\n",
      "Epoch [50/50], Train Loss: 27.006829627615506, Test Loss: 31.376646041870117\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 45.944720208840295, Test Loss: 44.25435995126699\n",
      "Epoch [20/100], Train Loss: 41.55904140785092, Test Loss: 41.58321692726829\n",
      "Epoch [30/100], Train Loss: 38.18358359415023, Test Loss: 38.28470839463271\n",
      "Epoch [40/100], Train Loss: 33.78333522609023, Test Loss: 34.484889711652485\n",
      "Epoch [50/100], Train Loss: 31.84607752815622, Test Loss: 33.334303372866145\n",
      "Epoch [60/100], Train Loss: 29.979923223276607, Test Loss: 31.43258587725751\n",
      "Epoch [70/100], Train Loss: 28.04886121906218, Test Loss: 29.748021039095793\n",
      "Epoch [80/100], Train Loss: 27.03471836652912, Test Loss: 28.744604853840617\n",
      "Epoch [90/100], Train Loss: 26.179044401450234, Test Loss: 29.681204263266032\n",
      "Epoch [100/100], Train Loss: 25.1059847659752, Test Loss: 28.41509707562335\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 45.443605691878524, Test Loss: 43.635038450166775\n",
      "Epoch [20/100], Train Loss: 40.81433500696401, Test Loss: 37.8429039298714\n",
      "Epoch [30/100], Train Loss: 37.41224013781938, Test Loss: 34.853805021806195\n",
      "Epoch [40/100], Train Loss: 34.18516854458168, Test Loss: 34.92877529193829\n",
      "Epoch [50/100], Train Loss: 31.476550755735303, Test Loss: 33.616240761496805\n",
      "Epoch [60/100], Train Loss: 29.450209820856813, Test Loss: 30.312650259439046\n",
      "Epoch [70/100], Train Loss: 27.7723852814221, Test Loss: 30.352207704023883\n",
      "Epoch [80/100], Train Loss: 27.26532012439165, Test Loss: 29.25286981656954\n",
      "Epoch [90/100], Train Loss: 25.945749401655352, Test Loss: 28.213610958743406\n",
      "Epoch [100/100], Train Loss: 24.996399182178934, Test Loss: 27.832223198630594\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.815363449346826, Test Loss: 43.49445694762391\n",
      "Epoch [20/100], Train Loss: 42.565397518970926, Test Loss: 38.96988643299449\n",
      "Epoch [30/100], Train Loss: 38.474890330580415, Test Loss: 37.887064648913096\n",
      "Epoch [40/100], Train Loss: 35.1516077197966, Test Loss: 37.827439840737874\n",
      "Epoch [50/100], Train Loss: 32.55171598215572, Test Loss: 33.45405159987413\n",
      "Epoch [60/100], Train Loss: 30.830702297023084, Test Loss: 29.648002030013444\n",
      "Epoch [70/100], Train Loss: 28.336427970010725, Test Loss: 29.97276477070598\n",
      "Epoch [80/100], Train Loss: 27.07845933007412, Test Loss: 29.354372024536133\n",
      "Epoch [90/100], Train Loss: 26.052587721777744, Test Loss: 28.577590967153576\n",
      "Epoch [100/100], Train Loss: 25.583781283019018, Test Loss: 27.910728033486897\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.08661176337571, Test Loss: 43.95736193966556\n",
      "Epoch [20/100], Train Loss: 41.60193106229188, Test Loss: 40.29893924663593\n",
      "Epoch [30/100], Train Loss: 38.0085663342085, Test Loss: 39.131452436570996\n",
      "Epoch [40/100], Train Loss: 34.54199884133261, Test Loss: 34.71283261187665\n",
      "Epoch [50/100], Train Loss: 31.793409678975088, Test Loss: 33.09619809435559\n",
      "Epoch [60/100], Train Loss: 30.574397315353643, Test Loss: 30.356508849503157\n",
      "Epoch [70/100], Train Loss: 27.629884363393316, Test Loss: 30.685803648713346\n",
      "Epoch [80/100], Train Loss: 27.614401545290086, Test Loss: 30.73738804111233\n",
      "Epoch [90/100], Train Loss: 26.61951611940978, Test Loss: 29.263428180248706\n",
      "Epoch [100/100], Train Loss: 25.398370686515435, Test Loss: 28.942403719022675\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 45.75966815635806, Test Loss: 43.76869330468116\n",
      "Epoch [20/100], Train Loss: 41.48463429935643, Test Loss: 40.55974162708629\n",
      "Epoch [30/100], Train Loss: 38.26072735395588, Test Loss: 38.27005232773818\n",
      "Epoch [40/100], Train Loss: 34.200294976156265, Test Loss: 34.03201566423689\n",
      "Epoch [50/100], Train Loss: 31.985548394625305, Test Loss: 30.40275883364987\n",
      "Epoch [60/100], Train Loss: 29.637765928174627, Test Loss: 31.93833034069507\n",
      "Epoch [70/100], Train Loss: 28.13691137345111, Test Loss: 29.816276575063732\n",
      "Epoch [80/100], Train Loss: 26.54109905430528, Test Loss: 28.914032007192638\n",
      "Epoch [90/100], Train Loss: 26.403760991331005, Test Loss: 29.269835657887644\n",
      "Epoch [100/100], Train Loss: 25.661519022456936, Test Loss: 29.56576867537065\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.596327622210396, Test Loss: 44.06664266214742\n",
      "Epoch [20/100], Train Loss: 42.38021201462042, Test Loss: 40.67215981421533\n",
      "Epoch [30/100], Train Loss: 38.61378313908811, Test Loss: 36.882282306621605\n",
      "Epoch [40/100], Train Loss: 35.21435311739562, Test Loss: 35.2521782416802\n",
      "Epoch [50/100], Train Loss: 32.502450498987415, Test Loss: 32.98715571614055\n",
      "Epoch [60/100], Train Loss: 31.09326250045026, Test Loss: 30.743829479465237\n",
      "Epoch [70/100], Train Loss: 27.99235098166544, Test Loss: 30.0445715421206\n",
      "Epoch [80/100], Train Loss: 27.211080863827565, Test Loss: 28.90039198739188\n",
      "Epoch [90/100], Train Loss: 26.23511351288342, Test Loss: 28.69432649983988\n",
      "Epoch [100/100], Train Loss: 25.408306128079772, Test Loss: 27.31043711575595\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.00041792822666, Test Loss: 43.60853933359121\n",
      "Epoch [20/100], Train Loss: 41.62583498094903, Test Loss: 39.33283441716974\n",
      "Epoch [30/100], Train Loss: 37.63638585825436, Test Loss: 37.404849807937424\n",
      "Epoch [40/100], Train Loss: 34.874516296386716, Test Loss: 34.921987087695626\n",
      "Epoch [50/100], Train Loss: 31.62972108184314, Test Loss: 33.05837492509322\n",
      "Epoch [60/100], Train Loss: 29.06367106203173, Test Loss: 32.052465314988964\n",
      "Epoch [70/100], Train Loss: 27.54429669302018, Test Loss: 31.660650352378944\n",
      "Epoch [80/100], Train Loss: 27.26262203904449, Test Loss: 28.371235364443297\n",
      "Epoch [90/100], Train Loss: 25.80338132264184, Test Loss: 29.45196936966537\n",
      "Epoch [100/100], Train Loss: 26.04041690513736, Test Loss: 28.653171787014255\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 45.72511371550013, Test Loss: 43.00100688191203\n",
      "Epoch [20/100], Train Loss: 41.39726999001425, Test Loss: 40.65526595673004\n",
      "Epoch [30/100], Train Loss: 37.33951248419089, Test Loss: 38.38164242831144\n",
      "Epoch [40/100], Train Loss: 33.89066029533011, Test Loss: 36.40021180487298\n",
      "Epoch [50/100], Train Loss: 31.572708505098937, Test Loss: 32.690378907439\n",
      "Epoch [60/100], Train Loss: 29.620946877901673, Test Loss: 30.32582897334904\n",
      "Epoch [70/100], Train Loss: 27.860980712390337, Test Loss: 29.20602798461914\n",
      "Epoch [80/100], Train Loss: 26.872756607806096, Test Loss: 29.10502052307129\n",
      "Epoch [90/100], Train Loss: 25.497780021292265, Test Loss: 28.80691099786139\n",
      "Epoch [100/100], Train Loss: 24.819228106639425, Test Loss: 28.65687216721572\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.63202117544706, Test Loss: 43.012060685591265\n",
      "Epoch [20/100], Train Loss: 42.266097922403304, Test Loss: 41.82103441907214\n",
      "Epoch [30/100], Train Loss: 38.600652838535, Test Loss: 37.5669578453163\n",
      "Epoch [40/100], Train Loss: 35.40422459586722, Test Loss: 34.68462664121157\n",
      "Epoch [50/100], Train Loss: 32.74204574334817, Test Loss: 33.834055392773124\n",
      "Epoch [60/100], Train Loss: 29.88887755597224, Test Loss: 32.46737586677848\n",
      "Epoch [70/100], Train Loss: 29.399097480148566, Test Loss: 31.216116744202452\n",
      "Epoch [80/100], Train Loss: 27.29778965183946, Test Loss: 28.341794224528524\n",
      "Epoch [90/100], Train Loss: 26.677407561755572, Test Loss: 28.295069855528993\n",
      "Epoch [100/100], Train Loss: 26.264089134091236, Test Loss: 28.141980827628792\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.611204016013225, Test Loss: 30.33302673736176\n",
      "Epoch [20/100], Train Loss: 26.700941573596392, Test Loss: 29.123097333041105\n",
      "Epoch [30/100], Train Loss: 25.992041272022686, Test Loss: 28.656873653461407\n",
      "Epoch [40/100], Train Loss: 24.654322327160443, Test Loss: 28.24392895884328\n",
      "Epoch [50/100], Train Loss: 25.475338945232455, Test Loss: 27.85283913550439\n",
      "Epoch [60/100], Train Loss: 24.22717668502057, Test Loss: 27.75490733555385\n",
      "Epoch [70/100], Train Loss: 24.37359760472032, Test Loss: 28.668537783932376\n",
      "Epoch [80/100], Train Loss: 24.686873232732054, Test Loss: 27.277547390430005\n",
      "Epoch [90/100], Train Loss: 23.65208219309322, Test Loss: 27.976196363374786\n",
      "Epoch [100/100], Train Loss: 24.187725298521947, Test Loss: 28.19404027369115\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.47445909468854, Test Loss: 29.796776833472315\n",
      "Epoch [20/100], Train Loss: 27.67251693850658, Test Loss: 30.300437654767716\n",
      "Epoch [30/100], Train Loss: 25.524445149155913, Test Loss: 29.15483994917436\n",
      "Epoch [40/100], Train Loss: 25.93743257366243, Test Loss: 30.552175695245918\n",
      "Epoch [50/100], Train Loss: 23.978595095775166, Test Loss: 27.667148639629414\n",
      "Epoch [60/100], Train Loss: 25.013986893950914, Test Loss: 28.264465604509628\n",
      "Epoch [70/100], Train Loss: 22.971336458550123, Test Loss: 27.39769326247178\n",
      "Epoch [80/100], Train Loss: 24.42939049142306, Test Loss: 28.338734143740172\n",
      "Epoch [90/100], Train Loss: 23.428955553398758, Test Loss: 28.529303934667016\n",
      "Epoch [100/100], Train Loss: 23.141433881540767, Test Loss: 27.595815039300298\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.99030402761991, Test Loss: 29.57399843884753\n",
      "Epoch [20/100], Train Loss: 26.101922432321018, Test Loss: 28.752725650737812\n",
      "Epoch [30/100], Train Loss: 26.968334910908684, Test Loss: 28.51324732891925\n",
      "Epoch [40/100], Train Loss: 26.4993127854144, Test Loss: 30.482392397793856\n",
      "Epoch [50/100], Train Loss: 26.2828335965266, Test Loss: 28.085261258212004\n",
      "Epoch [60/100], Train Loss: 25.01370117312572, Test Loss: 27.060391463242567\n",
      "Epoch [70/100], Train Loss: 25.799372300945343, Test Loss: 27.20624012142033\n",
      "Epoch [80/100], Train Loss: 23.642957249625784, Test Loss: 27.769056543127284\n",
      "Epoch [90/100], Train Loss: 23.48540729460169, Test Loss: 29.449486967805143\n",
      "Epoch [100/100], Train Loss: 24.51328195040343, Test Loss: 27.351938049514573\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.15574674137303, Test Loss: 29.942526408604213\n",
      "Epoch [20/100], Train Loss: 26.41078115994813, Test Loss: 29.170111965823484\n",
      "Epoch [30/100], Train Loss: 25.78043204135582, Test Loss: 27.94661316314301\n",
      "Epoch [40/100], Train Loss: 25.947518157958985, Test Loss: 27.668057082535384\n",
      "Epoch [50/100], Train Loss: 24.928520452780802, Test Loss: 27.86141774561498\n",
      "Epoch [60/100], Train Loss: 24.663779787157402, Test Loss: 29.38946466322069\n",
      "Epoch [70/100], Train Loss: 27.809022246814166, Test Loss: 29.37233575597986\n",
      "Epoch [80/100], Train Loss: 25.385087572942016, Test Loss: 27.98524455281047\n",
      "Epoch [90/100], Train Loss: 24.186968725235737, Test Loss: 28.183860134768796\n",
      "Epoch [100/100], Train Loss: 24.702988177440204, Test Loss: 27.028049691930995\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.72503083025823, Test Loss: 29.25900360206505\n",
      "Epoch [20/100], Train Loss: 26.04635605733903, Test Loss: 30.03665089297604\n",
      "Epoch [30/100], Train Loss: 26.47865369828021, Test Loss: 29.786772988059305\n",
      "Epoch [40/100], Train Loss: 25.60402346751729, Test Loss: 28.379302953744862\n",
      "Epoch [50/100], Train Loss: 25.66833459197498, Test Loss: 30.546388824264724\n",
      "Epoch [60/100], Train Loss: 25.158311893900887, Test Loss: 27.499155589512416\n",
      "Epoch [70/100], Train Loss: 25.42294845581055, Test Loss: 27.465598019686613\n",
      "Epoch [80/100], Train Loss: 24.899963038084937, Test Loss: 27.956391693709733\n",
      "Epoch [90/100], Train Loss: 23.940851249069464, Test Loss: 28.263993597649907\n",
      "Epoch [100/100], Train Loss: 25.101320310498846, Test Loss: 28.110025480196075\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.442412448320233, Test Loss: 28.517320533851525\n",
      "Epoch [20/100], Train Loss: 26.679581126228708, Test Loss: 31.25042164790166\n",
      "Epoch [30/100], Train Loss: 26.42593180546995, Test Loss: 28.093932461428953\n",
      "Epoch [40/100], Train Loss: 25.53385349961578, Test Loss: 28.982844340336786\n",
      "Epoch [50/100], Train Loss: 26.592609493068007, Test Loss: 28.513266328093295\n",
      "Epoch [60/100], Train Loss: 26.262400104960456, Test Loss: 28.174157204566065\n",
      "Epoch [70/100], Train Loss: 24.82797302496238, Test Loss: 27.901174495746563\n",
      "Epoch [80/100], Train Loss: 25.111240971674686, Test Loss: 27.76242920020958\n",
      "Epoch [90/100], Train Loss: 23.746565921971055, Test Loss: 27.669982043179598\n",
      "Epoch [100/100], Train Loss: 25.11234733706615, Test Loss: 27.785809925624303\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.70375921530802, Test Loss: 31.18284138766202\n",
      "Epoch [20/100], Train Loss: 28.826697002473423, Test Loss: 29.691683781611456\n",
      "Epoch [30/100], Train Loss: 26.674126146660477, Test Loss: 28.045621029742353\n",
      "Epoch [40/100], Train Loss: 25.270153545942463, Test Loss: 28.09996141706194\n",
      "Epoch [50/100], Train Loss: 26.042390179243245, Test Loss: 30.223114533857867\n",
      "Epoch [60/100], Train Loss: 26.106181022769114, Test Loss: 28.9887833533349\n",
      "Epoch [70/100], Train Loss: 26.249582109294956, Test Loss: 27.44382293503006\n",
      "Epoch [80/100], Train Loss: 26.111667320376537, Test Loss: 27.303402987393465\n",
      "Epoch [90/100], Train Loss: 26.695040161883245, Test Loss: 28.095507237818335\n",
      "Epoch [100/100], Train Loss: 25.7864580248223, Test Loss: 28.760128145094043\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.750188396016107, Test Loss: 31.53773746242771\n",
      "Epoch [20/100], Train Loss: 26.375740726658556, Test Loss: 30.159656400804394\n",
      "Epoch [30/100], Train Loss: 26.30242989336858, Test Loss: 32.4376909825709\n",
      "Epoch [40/100], Train Loss: 26.814531057389058, Test Loss: 28.297825726595793\n",
      "Epoch [50/100], Train Loss: 25.847919151431224, Test Loss: 28.256451173262164\n",
      "Epoch [60/100], Train Loss: 25.406841490698643, Test Loss: 34.06122618836242\n",
      "Epoch [70/100], Train Loss: 25.318978788031906, Test Loss: 27.934449307330244\n",
      "Epoch [80/100], Train Loss: 25.6618251425321, Test Loss: 27.88296194200392\n",
      "Epoch [90/100], Train Loss: 25.19679970037742, Test Loss: 27.496381809185078\n",
      "Epoch [100/100], Train Loss: 25.632481940847928, Test Loss: 27.342318894027116\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.306473966504708, Test Loss: 39.79355155647575\n",
      "Epoch [20/100], Train Loss: 27.15541404348905, Test Loss: 32.78144816609172\n",
      "Epoch [30/100], Train Loss: 25.98961974597368, Test Loss: 28.307740521121335\n",
      "Epoch [40/100], Train Loss: 26.83854200019211, Test Loss: 30.15910173391367\n",
      "Epoch [50/100], Train Loss: 27.06234987602859, Test Loss: 28.29326154040052\n",
      "Epoch [60/100], Train Loss: 27.17353120397349, Test Loss: 27.535355431692942\n",
      "Epoch [70/100], Train Loss: 24.357988476362383, Test Loss: 28.078875925633813\n",
      "Epoch [80/100], Train Loss: 24.608164946759334, Test Loss: 27.23269945615298\n",
      "Epoch [90/100], Train Loss: 25.503842775938942, Test Loss: 27.670927270666347\n",
      "Epoch [100/100], Train Loss: 24.671628032746863, Test Loss: 27.570232118879044\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.354584678274687, Test Loss: 29.617451482004935\n",
      "Epoch [20/100], Train Loss: 27.283528199743053, Test Loss: 30.955274210347756\n",
      "Epoch [30/100], Train Loss: 26.283255079926036, Test Loss: 28.668067089923017\n",
      "Epoch [40/100], Train Loss: 26.777987283175108, Test Loss: 30.733036165113575\n",
      "Epoch [50/100], Train Loss: 25.957722704527807, Test Loss: 27.913052521742785\n",
      "Epoch [60/100], Train Loss: 25.83417866190926, Test Loss: 27.595828291657682\n",
      "Epoch [70/100], Train Loss: 25.29485339805728, Test Loss: 27.67672209306197\n",
      "Epoch [80/100], Train Loss: 24.69275900418641, Test Loss: 28.25675493711001\n",
      "Epoch [90/100], Train Loss: 25.169577289018473, Test Loss: 28.194762935886136\n",
      "Epoch [100/100], Train Loss: 25.21302197565798, Test Loss: 27.554763447154652\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.411296256643826, Test Loss: 30.67750205002822\n",
      "Epoch [20/100], Train Loss: 28.5299051190986, Test Loss: 31.094614747282748\n",
      "Epoch [30/100], Train Loss: 25.84824658378226, Test Loss: 29.860886586176886\n",
      "Epoch [40/100], Train Loss: 26.332626843061604, Test Loss: 29.214017025836103\n",
      "Epoch [50/100], Train Loss: 26.55237000262151, Test Loss: 27.485517675226387\n",
      "Epoch [60/100], Train Loss: 26.042889942106655, Test Loss: 29.488776764312348\n",
      "Epoch [70/100], Train Loss: 25.32164970147805, Test Loss: 28.04322232828512\n",
      "Epoch [80/100], Train Loss: 26.141477641121284, Test Loss: 28.610327237612243\n",
      "Epoch [90/100], Train Loss: 25.461925343998143, Test Loss: 27.420454396829978\n",
      "Epoch [100/100], Train Loss: 26.83218391293385, Test Loss: 27.010842558625455\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.53026008605957, Test Loss: 29.98090424475732\n",
      "Epoch [20/100], Train Loss: 25.284776418717183, Test Loss: 29.29471206665039\n",
      "Epoch [30/100], Train Loss: 26.350276796935034, Test Loss: 27.736417993322597\n",
      "Epoch [40/100], Train Loss: 26.514591060700965, Test Loss: 28.617292552799373\n",
      "Epoch [50/100], Train Loss: 25.810349592615346, Test Loss: 27.57730325475916\n",
      "Epoch [60/100], Train Loss: 26.121351185783013, Test Loss: 30.27299690246582\n",
      "Epoch [70/100], Train Loss: 25.153485101168272, Test Loss: 30.986643902667158\n",
      "Epoch [80/100], Train Loss: 25.921848697349674, Test Loss: 28.62966755458287\n",
      "Epoch [90/100], Train Loss: 25.19827540038062, Test Loss: 28.754371444900315\n",
      "Epoch [100/100], Train Loss: 25.76576140982206, Test Loss: 27.857084720165698\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.72974675287966, Test Loss: 31.081663082172344\n",
      "Epoch [20/100], Train Loss: 27.495787473584784, Test Loss: 29.993194035121373\n",
      "Epoch [30/100], Train Loss: 25.644882220909242, Test Loss: 30.766489648199702\n",
      "Epoch [40/100], Train Loss: 27.527424233858703, Test Loss: 30.256327740557783\n",
      "Epoch [50/100], Train Loss: 26.69846535354364, Test Loss: 27.852377160803066\n",
      "Epoch [60/100], Train Loss: 26.762258079403736, Test Loss: 28.098632366626294\n",
      "Epoch [70/100], Train Loss: 25.30240780564605, Test Loss: 27.935735330953225\n",
      "Epoch [80/100], Train Loss: 26.063051680267833, Test Loss: 27.417524709329978\n",
      "Epoch [90/100], Train Loss: 25.81385500548316, Test Loss: 29.489945993795022\n",
      "Epoch [100/100], Train Loss: 26.257834662765752, Test Loss: 27.844709569757637\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.10984904805168, Test Loss: 38.88353030712574\n",
      "Epoch [20/100], Train Loss: 27.403657481709466, Test Loss: 42.063865463455\n",
      "Epoch [30/100], Train Loss: 26.542662592403225, Test Loss: 32.15597165095342\n",
      "Epoch [40/100], Train Loss: 25.12919442849081, Test Loss: 30.235906278932248\n",
      "Epoch [50/100], Train Loss: 26.422475120669507, Test Loss: 28.655814381388875\n",
      "Epoch [60/100], Train Loss: 25.578128426973937, Test Loss: 28.563573366635804\n",
      "Epoch [70/100], Train Loss: 26.28576952824827, Test Loss: 27.802184290700144\n",
      "Epoch [80/100], Train Loss: 25.862600783051036, Test Loss: 28.93826620919364\n",
      "Epoch [90/100], Train Loss: 25.978885706917183, Test Loss: 29.369112832205637\n",
      "Epoch [100/100], Train Loss: 26.384155798740075, Test Loss: 29.76061048136129\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.487626685470833, Test Loss: 32.876300737455296\n",
      "Epoch [20/100], Train Loss: 27.21759015067679, Test Loss: 31.24606281131893\n",
      "Epoch [30/100], Train Loss: 27.01388524790279, Test Loss: 29.65763810393098\n",
      "Epoch [40/100], Train Loss: 26.078889634179287, Test Loss: 31.60777671615799\n",
      "Epoch [50/100], Train Loss: 26.743834498671234, Test Loss: 29.0071288022128\n",
      "Epoch [60/100], Train Loss: 27.1235212857606, Test Loss: 27.513297774574973\n",
      "Epoch [70/100], Train Loss: 24.456480019991517, Test Loss: 29.180739415156378\n",
      "Epoch [80/100], Train Loss: 25.236431828483205, Test Loss: 27.828589080216048\n",
      "Epoch [90/100], Train Loss: 25.8511375114566, Test Loss: 29.326617946872464\n",
      "Epoch [100/100], Train Loss: 25.731048909171683, Test Loss: 28.16960874780432\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.515447604069944, Test Loss: 31.427720230895204\n",
      "Epoch [20/100], Train Loss: 27.24382201022789, Test Loss: 32.89376102794301\n",
      "Epoch [30/100], Train Loss: 27.668969263795947, Test Loss: 28.716973292363154\n",
      "Epoch [40/100], Train Loss: 27.25691393867868, Test Loss: 28.549728740345348\n",
      "Epoch [50/100], Train Loss: 27.362877673790102, Test Loss: 27.893711808439974\n",
      "Epoch [60/100], Train Loss: 25.857863854580238, Test Loss: 28.38438945621639\n",
      "Epoch [70/100], Train Loss: 27.348281528910654, Test Loss: 29.15753577591537\n",
      "Epoch [80/100], Train Loss: 26.604914987282676, Test Loss: 28.57110442124404\n",
      "Epoch [90/100], Train Loss: 26.73971362504803, Test Loss: 29.50001161748713\n",
      "Epoch [100/100], Train Loss: 26.59721363255235, Test Loss: 28.206687729080002\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.32705201946321, Test Loss: 33.14400026395723\n",
      "Epoch [20/100], Train Loss: 28.526491721731716, Test Loss: 28.97208327751655\n",
      "Epoch [30/100], Train Loss: 26.743787659191696, Test Loss: 30.990908065399566\n",
      "Epoch [40/100], Train Loss: 26.672163728807792, Test Loss: 31.451570585176544\n",
      "Epoch [50/100], Train Loss: 26.92131157546747, Test Loss: 29.829441714596438\n",
      "Epoch [60/100], Train Loss: 25.32630515802102, Test Loss: 29.72841129055271\n",
      "Epoch [70/100], Train Loss: 26.8257737206631, Test Loss: 30.033465918008382\n",
      "Epoch [80/100], Train Loss: 25.912560441064052, Test Loss: 30.608647210257395\n",
      "Epoch [90/100], Train Loss: 25.624952572681863, Test Loss: 28.832412595872754\n",
      "Epoch [100/100], Train Loss: 25.761378391453476, Test Loss: 30.37899963577072\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.66083424052254, Test Loss: 35.990907743379665\n",
      "Epoch [20/100], Train Loss: 28.942978505619237, Test Loss: 30.778732052097073\n",
      "Epoch [30/100], Train Loss: 28.30296140576972, Test Loss: 30.256153775500014\n",
      "Epoch [40/100], Train Loss: 26.252816015775085, Test Loss: 28.502747795798562\n",
      "Epoch [50/100], Train Loss: 28.482802569279905, Test Loss: 30.189988024823077\n",
      "Epoch [60/100], Train Loss: 27.26290030557601, Test Loss: 30.164240552233412\n",
      "Epoch [70/100], Train Loss: 27.00174783175109, Test Loss: 29.624336614237205\n",
      "Epoch [80/100], Train Loss: 26.294334123955398, Test Loss: 28.131519169002384\n",
      "Epoch [90/100], Train Loss: 26.89877408137087, Test Loss: 30.562469779671012\n",
      "Epoch [100/100], Train Loss: 26.654113281750288, Test Loss: 30.61528302477552\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.9040919069384, Test Loss: 46.77472448968268\n",
      "Epoch [20/100], Train Loss: 45.43148708656186, Test Loss: 42.97817373895026\n",
      "Epoch [30/100], Train Loss: 42.96750913526191, Test Loss: 40.27770257925059\n",
      "Epoch [40/100], Train Loss: 40.569398998823324, Test Loss: 36.55288617022626\n",
      "Epoch [50/100], Train Loss: 38.40809212356317, Test Loss: 38.287015989229275\n",
      "Epoch [60/100], Train Loss: 36.31314039386687, Test Loss: 37.33544594900949\n",
      "Epoch [70/100], Train Loss: 34.32972704777952, Test Loss: 35.197350687794874\n",
      "Epoch [80/100], Train Loss: 32.57149183554728, Test Loss: 33.721869877406526\n",
      "Epoch [90/100], Train Loss: 31.202384292102252, Test Loss: 32.243359429495676\n",
      "Epoch [100/100], Train Loss: 30.530324541936157, Test Loss: 30.360577570927607\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.98091802128026, Test Loss: 47.686180956951986\n",
      "Epoch [20/100], Train Loss: 45.58461572615827, Test Loss: 42.92121411608411\n",
      "Epoch [30/100], Train Loss: 43.14564691762455, Test Loss: 37.226676098712076\n",
      "Epoch [40/100], Train Loss: 40.859961137615265, Test Loss: 39.79443795340402\n",
      "Epoch [50/100], Train Loss: 38.503995689016875, Test Loss: 37.183154564399224\n",
      "Epoch [60/100], Train Loss: 36.45676099433273, Test Loss: 36.359116913436296\n",
      "Epoch [70/100], Train Loss: 34.44184595326908, Test Loss: 35.9094671521868\n",
      "Epoch [80/100], Train Loss: 32.56557923614002, Test Loss: 34.21960595366242\n",
      "Epoch [90/100], Train Loss: 31.755068950965757, Test Loss: 32.02689195608164\n",
      "Epoch [100/100], Train Loss: 29.4637294456607, Test Loss: 30.294392622910536\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.3441584102443, Test Loss: 45.71192129556235\n",
      "Epoch [20/100], Train Loss: 46.10966709324571, Test Loss: 41.55190742789925\n",
      "Epoch [30/100], Train Loss: 43.84998175198915, Test Loss: 42.240609057537924\n",
      "Epoch [40/100], Train Loss: 41.51780401761415, Test Loss: 37.46659102997223\n",
      "Epoch [50/100], Train Loss: 39.27486772380892, Test Loss: 38.936853334501194\n",
      "Epoch [60/100], Train Loss: 37.471526937015724, Test Loss: 36.93313618449422\n",
      "Epoch [70/100], Train Loss: 35.63068118486248, Test Loss: 37.73774649880149\n",
      "Epoch [80/100], Train Loss: 34.027415491323005, Test Loss: 35.101189576186144\n",
      "Epoch [90/100], Train Loss: 32.343302448460314, Test Loss: 32.71166625580231\n",
      "Epoch [100/100], Train Loss: 30.36998299770668, Test Loss: 33.36494569654589\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.18158721923828, Test Loss: 46.61672825008244\n",
      "Epoch [20/100], Train Loss: 45.781373796306674, Test Loss: 42.24599377520673\n",
      "Epoch [30/100], Train Loss: 43.25531666239754, Test Loss: 41.06086572424158\n",
      "Epoch [40/100], Train Loss: 41.19847625982566, Test Loss: 38.56511960710798\n",
      "Epoch [50/100], Train Loss: 39.09931625616355, Test Loss: 37.53423358867695\n",
      "Epoch [60/100], Train Loss: 36.71415147625032, Test Loss: 35.983291006707525\n",
      "Epoch [70/100], Train Loss: 35.046366844802606, Test Loss: 36.26920430071942\n",
      "Epoch [80/100], Train Loss: 33.06068031436107, Test Loss: 35.12721861802138\n",
      "Epoch [90/100], Train Loss: 31.450394170792375, Test Loss: 34.327320297043045\n",
      "Epoch [100/100], Train Loss: 29.995419849333217, Test Loss: 32.4560084652591\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.953527844538456, Test Loss: 47.5479006086077\n",
      "Epoch [20/100], Train Loss: 45.479217841976975, Test Loss: 43.80949005523285\n",
      "Epoch [30/100], Train Loss: 42.96982983448466, Test Loss: 41.29250410005644\n",
      "Epoch [40/100], Train Loss: 40.788007404765146, Test Loss: 39.74375662865577\n",
      "Epoch [50/100], Train Loss: 38.52628394580278, Test Loss: 39.064974945861024\n",
      "Epoch [60/100], Train Loss: 36.70830276989546, Test Loss: 37.77895057975472\n",
      "Epoch [70/100], Train Loss: 34.71259230316662, Test Loss: 34.4388113393412\n",
      "Epoch [80/100], Train Loss: 32.74788505053911, Test Loss: 34.16201616262461\n",
      "Epoch [90/100], Train Loss: 31.72826922057105, Test Loss: 33.24582622577618\n",
      "Epoch [100/100], Train Loss: 30.140173446154986, Test Loss: 32.222007503757226\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.20561787339508, Test Loss: 46.31544687840846\n",
      "Epoch [20/100], Train Loss: 45.871419400074444, Test Loss: 43.74537346579812\n",
      "Epoch [30/100], Train Loss: 43.68614867163486, Test Loss: 39.99165393779804\n",
      "Epoch [40/100], Train Loss: 41.50080502619509, Test Loss: 40.969092678714105\n",
      "Epoch [50/100], Train Loss: 39.18465968898085, Test Loss: 38.89358069679954\n",
      "Epoch [60/100], Train Loss: 37.40172379290471, Test Loss: 38.45563843962434\n",
      "Epoch [70/100], Train Loss: 35.31702698254195, Test Loss: 35.598451564838356\n",
      "Epoch [80/100], Train Loss: 34.33148042022205, Test Loss: 34.3752539746173\n",
      "Epoch [90/100], Train Loss: 31.912284969892657, Test Loss: 32.36018433508935\n",
      "Epoch [100/100], Train Loss: 30.304284868084018, Test Loss: 31.69632607001763\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.01843039090516, Test Loss: 46.91223615175718\n",
      "Epoch [20/100], Train Loss: 45.6324673887159, Test Loss: 38.606326611011056\n",
      "Epoch [30/100], Train Loss: 43.19769929979668, Test Loss: 42.16255009638799\n",
      "Epoch [40/100], Train Loss: 40.792294136422576, Test Loss: 39.69563372723468\n",
      "Epoch [50/100], Train Loss: 38.7229540465308, Test Loss: 36.09804695922059\n",
      "Epoch [60/100], Train Loss: 36.52482780081327, Test Loss: 37.30561261362844\n",
      "Epoch [70/100], Train Loss: 34.7972731605905, Test Loss: 33.575349733426975\n",
      "Epoch [80/100], Train Loss: 33.14240328053959, Test Loss: 35.31973452382273\n",
      "Epoch [90/100], Train Loss: 31.853227490284404, Test Loss: 32.73966905668184\n",
      "Epoch [100/100], Train Loss: 30.225838257836514, Test Loss: 31.85970943005054\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.95249108486488, Test Loss: 47.43534286300857\n",
      "Epoch [20/100], Train Loss: 45.41649133650983, Test Loss: 42.99857553259119\n",
      "Epoch [30/100], Train Loss: 43.04479306330446, Test Loss: 40.70876906754135\n",
      "Epoch [40/100], Train Loss: 40.69133681000256, Test Loss: 40.146595149845275\n",
      "Epoch [50/100], Train Loss: 38.54058839141345, Test Loss: 39.005240551837076\n",
      "Epoch [60/100], Train Loss: 36.56356911581071, Test Loss: 36.40771333273355\n",
      "Epoch [70/100], Train Loss: 35.2638735286525, Test Loss: 35.93596128983931\n",
      "Epoch [80/100], Train Loss: 33.53368991789271, Test Loss: 34.813013250177555\n",
      "Epoch [90/100], Train Loss: 31.909907269086993, Test Loss: 32.843445864590734\n",
      "Epoch [100/100], Train Loss: 30.107455169177445, Test Loss: 31.28736079822887\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.48403095182825, Test Loss: 47.24661705710671\n",
      "Epoch [20/100], Train Loss: 46.13911262887423, Test Loss: 43.870271657968495\n",
      "Epoch [30/100], Train Loss: 43.82036994558866, Test Loss: 42.8161681534408\n",
      "Epoch [40/100], Train Loss: 41.77606825281362, Test Loss: 41.29247457330877\n",
      "Epoch [50/100], Train Loss: 39.544890606989625, Test Loss: 40.56682492541028\n",
      "Epoch [60/100], Train Loss: 37.6415629902824, Test Loss: 37.779717482529676\n",
      "Epoch [70/100], Train Loss: 35.72124492457655, Test Loss: 37.12337102518453\n",
      "Epoch [80/100], Train Loss: 33.69024822047499, Test Loss: 32.35045603962688\n",
      "Epoch [90/100], Train Loss: 32.404153730048506, Test Loss: 33.92905567218731\n",
      "Epoch [100/100], Train Loss: 30.664189910888673, Test Loss: 32.07749361806101\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.667860262511205, Test Loss: 31.456462042672293\n",
      "Epoch [20/100], Train Loss: 26.24853865826716, Test Loss: 28.84849498798321\n",
      "Epoch [30/100], Train Loss: 25.742516852206872, Test Loss: 28.584450238710875\n",
      "Epoch [40/100], Train Loss: 24.25381535389384, Test Loss: 28.392090933663503\n",
      "Epoch [50/100], Train Loss: 24.227279750636367, Test Loss: 28.53352130543102\n",
      "Epoch [60/100], Train Loss: 23.807571479922434, Test Loss: 28.226067010458415\n",
      "Epoch [70/100], Train Loss: 23.984785630273038, Test Loss: 27.379766241296544\n",
      "Epoch [80/100], Train Loss: 23.752018637735336, Test Loss: 29.524731029163707\n",
      "Epoch [90/100], Train Loss: 23.614107701035795, Test Loss: 29.656567759328073\n",
      "Epoch [100/100], Train Loss: 23.125057408067047, Test Loss: 28.684004473995852\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.734772372636638, Test Loss: 29.6805869263488\n",
      "Epoch [20/100], Train Loss: 26.93089307566158, Test Loss: 27.526561811372833\n",
      "Epoch [30/100], Train Loss: 25.940101598520748, Test Loss: 27.802296898581766\n",
      "Epoch [40/100], Train Loss: 24.419714142846278, Test Loss: 28.097719588837066\n",
      "Epoch [50/100], Train Loss: 23.669392832771678, Test Loss: 28.790054048810685\n",
      "Epoch [60/100], Train Loss: 23.918461965342036, Test Loss: 28.25841388454685\n",
      "Epoch [70/100], Train Loss: 23.716542428438782, Test Loss: 29.02233212954038\n",
      "Epoch [80/100], Train Loss: 22.366845784421827, Test Loss: 26.817177078940652\n",
      "Epoch [90/100], Train Loss: 23.305959063670674, Test Loss: 30.309939718865728\n",
      "Epoch [100/100], Train Loss: 24.11125211246678, Test Loss: 29.936760914790167\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.75388581322842, Test Loss: 33.665946687970845\n",
      "Epoch [20/100], Train Loss: 26.541130472402102, Test Loss: 27.782525644673928\n",
      "Epoch [30/100], Train Loss: 25.200948821521195, Test Loss: 28.020534664005428\n",
      "Epoch [40/100], Train Loss: 24.752894954994076, Test Loss: 27.979760578700475\n",
      "Epoch [50/100], Train Loss: 23.654468079864003, Test Loss: 28.61758214777166\n",
      "Epoch [60/100], Train Loss: 24.14415903560451, Test Loss: 27.52317203174938\n",
      "Epoch [70/100], Train Loss: 23.765668543831246, Test Loss: 29.27332727011148\n",
      "Epoch [80/100], Train Loss: 23.91111273843734, Test Loss: 32.97810212667886\n",
      "Epoch [90/100], Train Loss: 23.081425638667874, Test Loss: 30.399973782626066\n",
      "Epoch [100/100], Train Loss: 23.1224467793449, Test Loss: 30.022843274203215\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.459232180235816, Test Loss: 41.923068207579774\n",
      "Epoch [20/100], Train Loss: 25.3322021421839, Test Loss: 29.605858740868506\n",
      "Epoch [30/100], Train Loss: 25.800570847558195, Test Loss: 28.513140839415712\n",
      "Epoch [40/100], Train Loss: 25.12781168202885, Test Loss: 28.19191784053654\n",
      "Epoch [50/100], Train Loss: 24.387068463935226, Test Loss: 28.69842019019189\n",
      "Epoch [60/100], Train Loss: 24.602609352987322, Test Loss: 28.379102161952428\n",
      "Epoch [70/100], Train Loss: 24.44996247838755, Test Loss: 28.56167003086635\n",
      "Epoch [80/100], Train Loss: 25.01882089083312, Test Loss: 28.20425707334048\n",
      "Epoch [90/100], Train Loss: 24.61596052451212, Test Loss: 28.37582419754623\n",
      "Epoch [100/100], Train Loss: 23.51459798343846, Test Loss: 29.268022363836113\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.23861077730773, Test Loss: 32.6725721978522\n",
      "Epoch [20/100], Train Loss: 25.98802674715636, Test Loss: 29.047745469328646\n",
      "Epoch [30/100], Train Loss: 24.781611702090405, Test Loss: 27.2719723342301\n",
      "Epoch [40/100], Train Loss: 24.503809706891168, Test Loss: 28.53529186991902\n",
      "Epoch [50/100], Train Loss: 24.122154642323977, Test Loss: 29.538593762880797\n",
      "Epoch [60/100], Train Loss: 24.074653756813923, Test Loss: 27.86897134161615\n",
      "Epoch [70/100], Train Loss: 24.571636794043368, Test Loss: 27.501406112274566\n",
      "Epoch [80/100], Train Loss: 23.627416041639986, Test Loss: 27.10105447645311\n",
      "Epoch [90/100], Train Loss: 23.253330668465036, Test Loss: 29.42515737360174\n",
      "Epoch [100/100], Train Loss: 23.70128044378562, Test Loss: 29.24367300256506\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.686882156622215, Test Loss: 30.731491336574802\n",
      "Epoch [20/100], Train Loss: 25.342561402868053, Test Loss: 28.578090816349178\n",
      "Epoch [30/100], Train Loss: 25.235900584986954, Test Loss: 27.864804057331828\n",
      "Epoch [40/100], Train Loss: 25.047691320200435, Test Loss: 27.867844891238523\n",
      "Epoch [50/100], Train Loss: 24.538624485203478, Test Loss: 27.148853995583274\n",
      "Epoch [60/100], Train Loss: 24.653092262393137, Test Loss: 27.35330608912877\n",
      "Epoch [70/100], Train Loss: 24.636370937159803, Test Loss: 28.516100994952314\n",
      "Epoch [80/100], Train Loss: 23.23570261470607, Test Loss: 29.006368215982015\n",
      "Epoch [90/100], Train Loss: 23.89771266999792, Test Loss: 27.75555231664088\n",
      "Epoch [100/100], Train Loss: 23.459281214729685, Test Loss: 27.960292370288403\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.199134820406556, Test Loss: 34.75799659629921\n",
      "Epoch [20/100], Train Loss: 26.2279513874992, Test Loss: 29.481390569117163\n",
      "Epoch [30/100], Train Loss: 25.097415598884957, Test Loss: 29.391665025190875\n",
      "Epoch [40/100], Train Loss: 24.446071787349513, Test Loss: 30.30784594548213\n",
      "Epoch [50/100], Train Loss: 25.181362952560676, Test Loss: 28.916702022800198\n",
      "Epoch [60/100], Train Loss: 24.514756937496, Test Loss: 29.60579366807814\n",
      "Epoch [70/100], Train Loss: 24.10820386917865, Test Loss: 28.413078134710137\n",
      "Epoch [80/100], Train Loss: 24.849848600293768, Test Loss: 27.148816666045747\n",
      "Epoch [90/100], Train Loss: 24.10425154889216, Test Loss: 31.02085643619686\n",
      "Epoch [100/100], Train Loss: 24.5087577131928, Test Loss: 28.91261402972333\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.44454123074891, Test Loss: 34.5747733673492\n",
      "Epoch [20/100], Train Loss: 25.441044304019115, Test Loss: 29.97164094602907\n",
      "Epoch [30/100], Train Loss: 25.2665427911477, Test Loss: 28.307891796161602\n",
      "Epoch [40/100], Train Loss: 25.306201059310162, Test Loss: 29.527486578210606\n",
      "Epoch [50/100], Train Loss: 25.41149917352395, Test Loss: 29.94621348690677\n",
      "Epoch [60/100], Train Loss: 24.781985970794178, Test Loss: 28.94136654246937\n",
      "Epoch [70/100], Train Loss: 24.88972493156058, Test Loss: 28.12482583677614\n",
      "Epoch [80/100], Train Loss: 25.246510940301615, Test Loss: 28.96419621752454\n",
      "Epoch [90/100], Train Loss: 23.400826482303806, Test Loss: 29.01399716463956\n",
      "Epoch [100/100], Train Loss: 24.38827769170042, Test Loss: 28.238359030191\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.604631968013575, Test Loss: 34.23147937229702\n",
      "Epoch [20/100], Train Loss: 26.737613315269595, Test Loss: 28.768012604156098\n",
      "Epoch [30/100], Train Loss: 25.685444165839524, Test Loss: 30.846624176223557\n",
      "Epoch [40/100], Train Loss: 25.685606334248526, Test Loss: 30.91350203675109\n",
      "Epoch [50/100], Train Loss: 25.04378311907659, Test Loss: 29.093463451831372\n",
      "Epoch [60/100], Train Loss: 24.590065896706502, Test Loss: 27.89030833058543\n",
      "Epoch [70/100], Train Loss: 24.295119313724705, Test Loss: 34.758850816008334\n",
      "Epoch [80/100], Train Loss: 23.692009297355277, Test Loss: 29.420824620630835\n",
      "Epoch [90/100], Train Loss: 23.335627777850043, Test Loss: 28.595016231784573\n",
      "Epoch [100/100], Train Loss: 23.951558960461227, Test Loss: 28.44889648239334\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.19091128364938, Test Loss: 32.832533155168804\n",
      "Epoch [20/100], Train Loss: 26.406847056404487, Test Loss: 31.420747905582576\n",
      "Epoch [30/100], Train Loss: 25.835492418633134, Test Loss: 28.518745471904804\n",
      "Epoch [40/100], Train Loss: 25.214613073380267, Test Loss: 28.09731793093991\n",
      "Epoch [50/100], Train Loss: 24.21105273512543, Test Loss: 28.919217245919363\n",
      "Epoch [60/100], Train Loss: 24.37689791194728, Test Loss: 30.156368973967318\n",
      "Epoch [70/100], Train Loss: 23.991237302686347, Test Loss: 27.680639861466048\n",
      "Epoch [80/100], Train Loss: 24.48234739459929, Test Loss: 28.25188260264211\n",
      "Epoch [90/100], Train Loss: 23.809087565687836, Test Loss: 30.761276814844702\n",
      "Epoch [100/100], Train Loss: 23.65085181564581, Test Loss: 31.215797771107066\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.12206280817751, Test Loss: 31.375800145136846\n",
      "Epoch [20/100], Train Loss: 25.794588626799037, Test Loss: 28.981893291721097\n",
      "Epoch [30/100], Train Loss: 25.967200232333823, Test Loss: 28.37245443269804\n",
      "Epoch [40/100], Train Loss: 25.210921953545242, Test Loss: 49.63752910069057\n",
      "Epoch [50/100], Train Loss: 25.104405750212123, Test Loss: 27.760772828931934\n",
      "Epoch [60/100], Train Loss: 25.415250765691038, Test Loss: 28.537562927642426\n",
      "Epoch [70/100], Train Loss: 25.68037096867796, Test Loss: 31.143288327501967\n",
      "Epoch [80/100], Train Loss: 25.02703989998239, Test Loss: 37.06286789534928\n",
      "Epoch [90/100], Train Loss: 23.71348247215396, Test Loss: 27.29356934188248\n",
      "Epoch [100/100], Train Loss: 23.945687803674918, Test Loss: 28.474322603894517\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.22391589430512, Test Loss: 29.562474882447873\n",
      "Epoch [20/100], Train Loss: 26.729566649139905, Test Loss: 30.608334083061713\n",
      "Epoch [30/100], Train Loss: 25.696879683947955, Test Loss: 31.95890909665591\n",
      "Epoch [40/100], Train Loss: 26.598510723426696, Test Loss: 28.04666286319881\n",
      "Epoch [50/100], Train Loss: 24.542737948308226, Test Loss: 28.42351026658888\n",
      "Epoch [60/100], Train Loss: 25.210177962506403, Test Loss: 27.67897586079387\n",
      "Epoch [70/100], Train Loss: 24.602196890408877, Test Loss: 28.408471689595803\n",
      "Epoch [80/100], Train Loss: 24.057771132422275, Test Loss: 27.62863307804256\n",
      "Epoch [90/100], Train Loss: 24.04047658326196, Test Loss: 26.606728566157354\n",
      "Epoch [100/100], Train Loss: 24.374556344454405, Test Loss: 27.89640538104169\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.098583915585376, Test Loss: 29.651295946790025\n",
      "Epoch [20/100], Train Loss: 26.374827431850747, Test Loss: 29.196799241103136\n",
      "Epoch [30/100], Train Loss: 25.291210474733447, Test Loss: 29.194984039702973\n",
      "Epoch [40/100], Train Loss: 25.17265179118172, Test Loss: 30.093450521493885\n",
      "Epoch [50/100], Train Loss: 25.797574540435292, Test Loss: 29.446267313771433\n",
      "Epoch [60/100], Train Loss: 24.80096465564165, Test Loss: 28.725845559850917\n",
      "Epoch [70/100], Train Loss: 24.780290728709737, Test Loss: 29.745472697468546\n",
      "Epoch [80/100], Train Loss: 24.119656797315255, Test Loss: 31.53112503150841\n",
      "Epoch [90/100], Train Loss: 25.234235407094488, Test Loss: 27.543091464352297\n",
      "Epoch [100/100], Train Loss: 25.34506518879875, Test Loss: 28.69734860705091\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.095766298888158, Test Loss: 35.327441673774224\n",
      "Epoch [20/100], Train Loss: 25.555715654717115, Test Loss: 30.986970975801544\n",
      "Epoch [30/100], Train Loss: 25.813357281294024, Test Loss: 29.404159991772143\n",
      "Epoch [40/100], Train Loss: 25.5787645871522, Test Loss: 28.857291778960786\n",
      "Epoch [50/100], Train Loss: 25.326730728149414, Test Loss: 30.008888541878044\n",
      "Epoch [60/100], Train Loss: 25.728796161589074, Test Loss: 29.712398801531112\n",
      "Epoch [70/100], Train Loss: 25.284192770035542, Test Loss: 28.67014503479004\n",
      "Epoch [80/100], Train Loss: 25.179387151999553, Test Loss: 28.704861182671088\n",
      "Epoch [90/100], Train Loss: 24.80795409405818, Test Loss: 29.469291018201158\n",
      "Epoch [100/100], Train Loss: 24.085150146484374, Test Loss: 29.39966179488541\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.47983262734335, Test Loss: 30.31542597188578\n",
      "Epoch [20/100], Train Loss: 27.167577005605228, Test Loss: 29.6785807671485\n",
      "Epoch [30/100], Train Loss: 27.548051189985433, Test Loss: 28.296983991350448\n",
      "Epoch [40/100], Train Loss: 25.453807837064147, Test Loss: 28.527609614582804\n",
      "Epoch [50/100], Train Loss: 25.03151267630155, Test Loss: 28.0030750918698\n",
      "Epoch [60/100], Train Loss: 25.29230875734423, Test Loss: 28.014217896894976\n",
      "Epoch [70/100], Train Loss: 26.232139987632877, Test Loss: 28.667884777118633\n",
      "Epoch [80/100], Train Loss: 25.097754963108752, Test Loss: 28.589209445111162\n",
      "Epoch [90/100], Train Loss: 25.50378623086898, Test Loss: 27.676127768182134\n",
      "Epoch [100/100], Train Loss: 24.494360720525023, Test Loss: 26.895753910015156\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.264617144475217, Test Loss: 35.641806565321886\n",
      "Epoch [20/100], Train Loss: 27.52829068293337, Test Loss: 29.882237372460303\n",
      "Epoch [30/100], Train Loss: 26.267000129574637, Test Loss: 37.526827031915836\n",
      "Epoch [40/100], Train Loss: 29.116964590354044, Test Loss: 30.96965286948464\n",
      "Epoch [50/100], Train Loss: 27.44490105675869, Test Loss: 41.166921045873075\n",
      "Epoch [60/100], Train Loss: 25.914761371299868, Test Loss: 29.59316231368424\n",
      "Epoch [70/100], Train Loss: 26.071732574212746, Test Loss: 29.70810265974565\n",
      "Epoch [80/100], Train Loss: 25.875328845665102, Test Loss: 28.62475551258434\n",
      "Epoch [90/100], Train Loss: 25.900457157072474, Test Loss: 29.721819122116287\n",
      "Epoch [100/100], Train Loss: 26.51630388478764, Test Loss: 28.94567445036653\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.655252506693856, Test Loss: 33.181569532914594\n",
      "Epoch [20/100], Train Loss: 26.904937537771758, Test Loss: 32.1897128216632\n",
      "Epoch [30/100], Train Loss: 26.96492141973777, Test Loss: 29.1012372351312\n",
      "Epoch [40/100], Train Loss: 27.42396249614778, Test Loss: 33.99794271394804\n",
      "Epoch [50/100], Train Loss: 27.396025010405992, Test Loss: 29.50563031976873\n",
      "Epoch [60/100], Train Loss: 26.37918152730973, Test Loss: 29.1436246896719\n",
      "Epoch [70/100], Train Loss: 25.920534627945695, Test Loss: 29.049603449833857\n",
      "Epoch [80/100], Train Loss: 25.31786021248239, Test Loss: 28.160226648504082\n",
      "Epoch [90/100], Train Loss: 24.806362702416592, Test Loss: 30.322251654290532\n",
      "Epoch [100/100], Train Loss: 25.442179745533426, Test Loss: 32.8086346960687\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.406852684646356, Test Loss: 34.317362995890825\n",
      "Epoch [20/100], Train Loss: 28.495075944994316, Test Loss: 31.573746445891146\n",
      "Epoch [30/100], Train Loss: 27.74532712092165, Test Loss: 29.063702769093698\n",
      "Epoch [40/100], Train Loss: 26.640923084196498, Test Loss: 28.829967597862343\n",
      "Epoch [50/100], Train Loss: 26.128907525734824, Test Loss: 31.131040003392602\n",
      "Epoch [60/100], Train Loss: 26.117932751139655, Test Loss: 28.35676465715681\n",
      "Epoch [70/100], Train Loss: 26.15184527537862, Test Loss: 28.02488849689434\n",
      "Epoch [80/100], Train Loss: 25.563111177037975, Test Loss: 29.64449444064846\n",
      "Epoch [90/100], Train Loss: 25.273722673635014, Test Loss: 30.30124770820915\n",
      "Epoch [100/100], Train Loss: 26.14922486602283, Test Loss: 29.048469419603222\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.9322310776007, Test Loss: 48.36485290527344\n",
      "Epoch [20/100], Train Loss: 47.44259887445168, Test Loss: 46.58185958862305\n",
      "Epoch [30/100], Train Loss: 45.91585501999152, Test Loss: 44.839656829833984\n",
      "Epoch [40/100], Train Loss: 44.44778302927486, Test Loss: 44.20899200439453\n",
      "Epoch [50/100], Train Loss: 42.97159406318039, Test Loss: 41.463985443115234\n",
      "Epoch [60/100], Train Loss: 41.528922484350986, Test Loss: 39.52427291870117\n",
      "Epoch [70/100], Train Loss: 40.0062053868028, Test Loss: 40.16555404663086\n",
      "Epoch [80/100], Train Loss: 39.079517939833345, Test Loss: 38.47526168823242\n",
      "Epoch [90/100], Train Loss: 37.50163360345559, Test Loss: 37.44919967651367\n",
      "Epoch [100/100], Train Loss: 36.22190175290967, Test Loss: 38.3839111328125\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.72236873439101, Test Loss: 48.878841400146484\n",
      "Epoch [20/100], Train Loss: 47.207762120981684, Test Loss: 46.47079849243164\n",
      "Epoch [30/100], Train Loss: 45.663266866715226, Test Loss: 42.331695556640625\n",
      "Epoch [40/100], Train Loss: 44.120592523793704, Test Loss: 41.763702392578125\n",
      "Epoch [50/100], Train Loss: 42.78887669297515, Test Loss: 41.10127258300781\n",
      "Epoch [60/100], Train Loss: 41.20855529034724, Test Loss: 41.326805114746094\n",
      "Epoch [70/100], Train Loss: 39.92241358522509, Test Loss: 39.645015716552734\n",
      "Epoch [80/100], Train Loss: 38.50648636114402, Test Loss: 38.24077224731445\n",
      "Epoch [90/100], Train Loss: 37.218918184374196, Test Loss: 35.35823440551758\n",
      "Epoch [100/100], Train Loss: 35.94220305270836, Test Loss: 35.61588668823242\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.975182617687786, Test Loss: 46.97754669189453\n",
      "Epoch [20/100], Train Loss: 47.59064768806833, Test Loss: 46.34379577636719\n",
      "Epoch [30/100], Train Loss: 46.16975316532323, Test Loss: 43.4195442199707\n",
      "Epoch [40/100], Train Loss: 44.76199104434154, Test Loss: 40.938865661621094\n",
      "Epoch [50/100], Train Loss: 43.29515009395412, Test Loss: 41.17162322998047\n",
      "Epoch [60/100], Train Loss: 41.887156439609214, Test Loss: 40.1490478515625\n",
      "Epoch [70/100], Train Loss: 40.701497087322295, Test Loss: 38.542659759521484\n",
      "Epoch [80/100], Train Loss: 39.17659874587763, Test Loss: 36.340274810791016\n",
      "Epoch [90/100], Train Loss: 37.9472417987761, Test Loss: 40.537166595458984\n",
      "Epoch [100/100], Train Loss: 36.981869919573676, Test Loss: 35.26176452636719\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.91813056820729, Test Loss: 48.16558074951172\n",
      "Epoch [20/100], Train Loss: 47.457649174674614, Test Loss: 45.43656921386719\n",
      "Epoch [30/100], Train Loss: 45.953719667528496, Test Loss: 42.87746810913086\n",
      "Epoch [40/100], Train Loss: 44.453747721187405, Test Loss: 43.04513931274414\n",
      "Epoch [50/100], Train Loss: 42.965188598632814, Test Loss: 40.89757537841797\n",
      "Epoch [60/100], Train Loss: 41.57968344766586, Test Loss: 39.82612609863281\n",
      "Epoch [70/100], Train Loss: 40.24684281896372, Test Loss: 38.81549835205078\n",
      "Epoch [80/100], Train Loss: 38.98861413549204, Test Loss: 37.87255096435547\n",
      "Epoch [90/100], Train Loss: 37.807285834140465, Test Loss: 37.965511322021484\n",
      "Epoch [100/100], Train Loss: 36.593517666175714, Test Loss: 37.55131530761719\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.806212665995616, Test Loss: 48.07851028442383\n",
      "Epoch [20/100], Train Loss: 47.28332578315109, Test Loss: 45.8643798828125\n",
      "Epoch [30/100], Train Loss: 45.73566274173924, Test Loss: 44.128238677978516\n",
      "Epoch [40/100], Train Loss: 44.21663135466029, Test Loss: 42.72085952758789\n",
      "Epoch [50/100], Train Loss: 42.77414035484439, Test Loss: 39.565982818603516\n",
      "Epoch [60/100], Train Loss: 41.29271100153689, Test Loss: 40.525367736816406\n",
      "Epoch [70/100], Train Loss: 40.027433639276225, Test Loss: 40.5726318359375\n",
      "Epoch [80/100], Train Loss: 38.64702802564277, Test Loss: 41.166099548339844\n",
      "Epoch [90/100], Train Loss: 37.20859793991339, Test Loss: 35.79591369628906\n",
      "Epoch [100/100], Train Loss: 36.14071074939165, Test Loss: 37.04842758178711\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.92884077478627, Test Loss: 47.62318801879883\n",
      "Epoch [20/100], Train Loss: 47.560177149538134, Test Loss: 45.92804718017578\n",
      "Epoch [30/100], Train Loss: 46.21579075797659, Test Loss: 44.19408416748047\n",
      "Epoch [40/100], Train Loss: 44.70158800218926, Test Loss: 41.30210876464844\n",
      "Epoch [50/100], Train Loss: 43.35605777677942, Test Loss: 41.564552307128906\n",
      "Epoch [60/100], Train Loss: 42.00695276729396, Test Loss: 41.013999938964844\n",
      "Epoch [70/100], Train Loss: 40.671193044693744, Test Loss: 39.96904754638672\n",
      "Epoch [80/100], Train Loss: 39.47633092911517, Test Loss: 38.70243835449219\n",
      "Epoch [90/100], Train Loss: 38.25516408701412, Test Loss: 37.86460494995117\n",
      "Epoch [100/100], Train Loss: 36.76424177826428, Test Loss: 38.00365447998047\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.88140841624776, Test Loss: 48.30634689331055\n",
      "Epoch [20/100], Train Loss: 47.41192339287429, Test Loss: 45.7860107421875\n",
      "Epoch [30/100], Train Loss: 45.89380210501249, Test Loss: 43.061283111572266\n",
      "Epoch [40/100], Train Loss: 44.35150993222096, Test Loss: 43.190006256103516\n",
      "Epoch [50/100], Train Loss: 42.882406778804594, Test Loss: 41.160614013671875\n",
      "Epoch [60/100], Train Loss: 41.3724841508709, Test Loss: 39.081626892089844\n",
      "Epoch [70/100], Train Loss: 40.19912071853388, Test Loss: 38.411624908447266\n",
      "Epoch [80/100], Train Loss: 39.03706457419474, Test Loss: 38.93181610107422\n",
      "Epoch [90/100], Train Loss: 37.46600280511575, Test Loss: 37.75849533081055\n",
      "Epoch [100/100], Train Loss: 36.4680814836846, Test Loss: 34.36032485961914\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.78406678496814, Test Loss: 47.377540588378906\n",
      "Epoch [20/100], Train Loss: 47.24383309786437, Test Loss: 47.61383056640625\n",
      "Epoch [30/100], Train Loss: 45.696641115282404, Test Loss: 43.80259704589844\n",
      "Epoch [40/100], Train Loss: 44.2023660628522, Test Loss: 41.503700256347656\n",
      "Epoch [50/100], Train Loss: 42.72345883729028, Test Loss: 40.68604278564453\n",
      "Epoch [60/100], Train Loss: 41.28030235415599, Test Loss: 41.256492614746094\n",
      "Epoch [70/100], Train Loss: 39.86625219251289, Test Loss: 39.53516387939453\n",
      "Epoch [80/100], Train Loss: 38.624659653960684, Test Loss: 38.738929748535156\n",
      "Epoch [90/100], Train Loss: 37.52425180654057, Test Loss: 37.966983795166016\n",
      "Epoch [100/100], Train Loss: 36.140968016327406, Test Loss: 36.765777587890625\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.99831132732454, Test Loss: 47.170799255371094\n",
      "Epoch [20/100], Train Loss: 47.66706417896709, Test Loss: 46.106285095214844\n",
      "Epoch [30/100], Train Loss: 46.27070498857342, Test Loss: 42.111183166503906\n",
      "Epoch [40/100], Train Loss: 44.817853208448064, Test Loss: 41.80634689331055\n",
      "Epoch [50/100], Train Loss: 43.33686153224257, Test Loss: 40.964542388916016\n",
      "Epoch [60/100], Train Loss: 41.99895624879931, Test Loss: 39.26167297363281\n",
      "Epoch [70/100], Train Loss: 40.71166224245165, Test Loss: 39.551048278808594\n",
      "Epoch [80/100], Train Loss: 39.43388272895188, Test Loss: 37.80169677734375\n",
      "Epoch [90/100], Train Loss: 38.124253419969904, Test Loss: 38.039154052734375\n",
      "Epoch [100/100], Train Loss: 36.85966530471551, Test Loss: 36.99052429199219\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.53370265022653, Test Loss: 31.34486961364746\n",
      "Epoch [20/100], Train Loss: 25.282165877545467, Test Loss: 34.379478454589844\n",
      "Epoch [30/100], Train Loss: 24.879566680407915, Test Loss: 33.32006072998047\n",
      "Epoch [40/100], Train Loss: 24.33748008227739, Test Loss: 31.57542610168457\n",
      "Epoch [50/100], Train Loss: 23.423252668537078, Test Loss: 30.285520553588867\n",
      "Epoch [60/100], Train Loss: 23.964876956627016, Test Loss: 27.5302791595459\n",
      "Epoch [70/100], Train Loss: 23.800635753694127, Test Loss: 28.375558853149414\n",
      "Epoch [80/100], Train Loss: 22.906751970384942, Test Loss: 27.34150505065918\n",
      "Epoch [90/100], Train Loss: 22.408424896490377, Test Loss: 29.569583892822266\n",
      "Epoch [100/100], Train Loss: 22.441680942597937, Test Loss: 29.392057418823242\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.443627279312885, Test Loss: 33.885223388671875\n",
      "Epoch [20/100], Train Loss: 25.008917480218606, Test Loss: 31.880891799926758\n",
      "Epoch [30/100], Train Loss: 24.8940938980853, Test Loss: 29.232877731323242\n",
      "Epoch [40/100], Train Loss: 24.713997237408748, Test Loss: 28.14031982421875\n",
      "Epoch [50/100], Train Loss: 24.487799434974544, Test Loss: 32.30033493041992\n",
      "Epoch [60/100], Train Loss: 24.39879825154289, Test Loss: 28.76714324951172\n",
      "Epoch [70/100], Train Loss: 22.765072437974272, Test Loss: 29.7265567779541\n",
      "Epoch [80/100], Train Loss: 23.691933247300444, Test Loss: 28.42347526550293\n",
      "Epoch [90/100], Train Loss: 23.1640685222188, Test Loss: 28.353334426879883\n",
      "Epoch [100/100], Train Loss: 21.936268553186636, Test Loss: 29.198530197143555\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.421229152992122, Test Loss: 36.302181243896484\n",
      "Epoch [20/100], Train Loss: 25.400662318995742, Test Loss: 31.238510131835938\n",
      "Epoch [30/100], Train Loss: 24.51418068057201, Test Loss: 27.445236206054688\n",
      "Epoch [40/100], Train Loss: 24.4844612059046, Test Loss: 36.51702880859375\n",
      "Epoch [50/100], Train Loss: 24.35786718149654, Test Loss: 26.904205322265625\n",
      "Epoch [60/100], Train Loss: 23.649227592593334, Test Loss: 28.064205169677734\n",
      "Epoch [70/100], Train Loss: 22.954206898173346, Test Loss: 30.563920974731445\n",
      "Epoch [80/100], Train Loss: 22.7314085413198, Test Loss: 28.366985321044922\n",
      "Epoch [90/100], Train Loss: 22.58705526883485, Test Loss: 32.50117874145508\n",
      "Epoch [100/100], Train Loss: 22.40490107927166, Test Loss: 28.348608016967773\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.209857359088836, Test Loss: 37.68701934814453\n",
      "Epoch [20/100], Train Loss: 25.744254234188894, Test Loss: 31.081092834472656\n",
      "Epoch [30/100], Train Loss: 25.109157355886992, Test Loss: 30.486127853393555\n",
      "Epoch [40/100], Train Loss: 25.48634822407707, Test Loss: 28.74717140197754\n",
      "Epoch [50/100], Train Loss: 24.19617307694232, Test Loss: 28.436872482299805\n",
      "Epoch [60/100], Train Loss: 23.934644267598138, Test Loss: 28.997648239135742\n",
      "Epoch [70/100], Train Loss: 23.905461045562244, Test Loss: 28.475284576416016\n",
      "Epoch [80/100], Train Loss: 23.152920982485913, Test Loss: 29.160797119140625\n",
      "Epoch [90/100], Train Loss: 23.180558339103325, Test Loss: 28.128150939941406\n",
      "Epoch [100/100], Train Loss: 22.400112327200468, Test Loss: 29.122777938842773\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.108920819642115, Test Loss: 37.85752487182617\n",
      "Epoch [20/100], Train Loss: 25.50192738517386, Test Loss: 31.351442337036133\n",
      "Epoch [30/100], Train Loss: 24.81312430334873, Test Loss: 27.486467361450195\n",
      "Epoch [40/100], Train Loss: 24.11772744225674, Test Loss: 27.865070343017578\n",
      "Epoch [50/100], Train Loss: 23.37017393893883, Test Loss: 29.57921028137207\n",
      "Epoch [60/100], Train Loss: 23.70603877833632, Test Loss: 29.212932586669922\n",
      "Epoch [70/100], Train Loss: 23.317887653288295, Test Loss: 28.07255744934082\n",
      "Epoch [80/100], Train Loss: 23.569279642574124, Test Loss: 26.989730834960938\n",
      "Epoch [90/100], Train Loss: 23.632855699883134, Test Loss: 28.711315155029297\n",
      "Epoch [100/100], Train Loss: 23.23716866540127, Test Loss: 30.64859962463379\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.196701425020812, Test Loss: 31.811641693115234\n",
      "Epoch [20/100], Train Loss: 26.59549294768787, Test Loss: 31.763782501220703\n",
      "Epoch [30/100], Train Loss: 25.160066660896675, Test Loss: 29.819067001342773\n",
      "Epoch [40/100], Train Loss: 24.943713897955224, Test Loss: 27.56456756591797\n",
      "Epoch [50/100], Train Loss: 25.444116748747277, Test Loss: 27.822065353393555\n",
      "Epoch [60/100], Train Loss: 23.489226813394517, Test Loss: 33.515254974365234\n",
      "Epoch [70/100], Train Loss: 23.720072261622693, Test Loss: 28.47728157043457\n",
      "Epoch [80/100], Train Loss: 22.79672676461642, Test Loss: 30.15835189819336\n",
      "Epoch [90/100], Train Loss: 23.696385017770236, Test Loss: 29.530794143676758\n",
      "Epoch [100/100], Train Loss: 22.61066746320881, Test Loss: 29.562747955322266\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.064416553935068, Test Loss: 37.81025695800781\n",
      "Epoch [20/100], Train Loss: 26.136427813670675, Test Loss: 32.74028778076172\n",
      "Epoch [30/100], Train Loss: 25.429753225357807, Test Loss: 31.698270797729492\n",
      "Epoch [40/100], Train Loss: 26.128761097642244, Test Loss: 32.1602783203125\n",
      "Epoch [50/100], Train Loss: 24.917885408245148, Test Loss: 28.42898941040039\n",
      "Epoch [60/100], Train Loss: 24.743395720935258, Test Loss: 28.169147491455078\n",
      "Epoch [70/100], Train Loss: 24.248279846691695, Test Loss: 32.16007614135742\n",
      "Epoch [80/100], Train Loss: 23.738784483612562, Test Loss: 28.017013549804688\n",
      "Epoch [90/100], Train Loss: 24.092202646224226, Test Loss: 28.920080184936523\n",
      "Epoch [100/100], Train Loss: 24.37830993777416, Test Loss: 28.659982681274414\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.305505890142722, Test Loss: 35.159751892089844\n",
      "Epoch [20/100], Train Loss: 26.525518404851194, Test Loss: 29.703487396240234\n",
      "Epoch [30/100], Train Loss: 25.308087120681513, Test Loss: 31.78264808654785\n",
      "Epoch [40/100], Train Loss: 25.52110532541744, Test Loss: 28.16004180908203\n",
      "Epoch [50/100], Train Loss: 25.633414997038294, Test Loss: 30.849910736083984\n",
      "Epoch [60/100], Train Loss: 24.528468829295676, Test Loss: 27.713590621948242\n",
      "Epoch [70/100], Train Loss: 24.197323183153497, Test Loss: 32.208194732666016\n",
      "Epoch [80/100], Train Loss: 24.344395715682232, Test Loss: 30.749958038330078\n",
      "Epoch [90/100], Train Loss: 23.43923873276007, Test Loss: 29.15492057800293\n",
      "Epoch [100/100], Train Loss: 23.084850117417634, Test Loss: 28.224849700927734\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.323175811767577, Test Loss: 37.14424133300781\n",
      "Epoch [20/100], Train Loss: 25.95222954671891, Test Loss: 38.50755310058594\n",
      "Epoch [30/100], Train Loss: 25.687630193741594, Test Loss: 30.67755699157715\n",
      "Epoch [40/100], Train Loss: 25.53846169768787, Test Loss: 46.53757095336914\n",
      "Epoch [50/100], Train Loss: 24.674570859064822, Test Loss: 28.690387725830078\n",
      "Epoch [60/100], Train Loss: 24.696225294519643, Test Loss: 30.424808502197266\n",
      "Epoch [70/100], Train Loss: 24.8659110272517, Test Loss: 29.438859939575195\n",
      "Epoch [80/100], Train Loss: 24.936866009821657, Test Loss: 29.425067901611328\n",
      "Epoch [90/100], Train Loss: 24.227695352523053, Test Loss: 28.464982986450195\n",
      "Epoch [100/100], Train Loss: 24.5386390123211, Test Loss: 30.730464935302734\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.155221789000464, Test Loss: 31.454792022705078\n",
      "Epoch [20/100], Train Loss: 25.98096507963587, Test Loss: 29.302059173583984\n",
      "Epoch [30/100], Train Loss: 24.802811275544713, Test Loss: 35.773353576660156\n",
      "Epoch [40/100], Train Loss: 24.532854148989816, Test Loss: 27.971952438354492\n",
      "Epoch [50/100], Train Loss: 25.844688234172885, Test Loss: 29.387615203857422\n",
      "Epoch [60/100], Train Loss: 24.448950726868677, Test Loss: 29.6091365814209\n",
      "Epoch [70/100], Train Loss: 24.370434951782226, Test Loss: 28.201995849609375\n",
      "Epoch [80/100], Train Loss: 24.26179828956479, Test Loss: 27.60773277282715\n",
      "Epoch [90/100], Train Loss: 24.17564434614338, Test Loss: 27.144309997558594\n",
      "Epoch [100/100], Train Loss: 23.80632321091949, Test Loss: 27.54638671875\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.669712648235382, Test Loss: 41.10240936279297\n",
      "Epoch [20/100], Train Loss: 25.92022518720783, Test Loss: 30.19280433654785\n",
      "Epoch [30/100], Train Loss: 25.979323090099896, Test Loss: 29.483274459838867\n",
      "Epoch [40/100], Train Loss: 24.926659849823498, Test Loss: 33.60519027709961\n",
      "Epoch [50/100], Train Loss: 24.661013418729187, Test Loss: 29.198429107666016\n",
      "Epoch [60/100], Train Loss: 26.170288498675237, Test Loss: 27.640403747558594\n",
      "Epoch [70/100], Train Loss: 25.940733818929704, Test Loss: 29.210153579711914\n",
      "Epoch [80/100], Train Loss: 23.875841934954533, Test Loss: 28.84340476989746\n",
      "Epoch [90/100], Train Loss: 23.826569116310996, Test Loss: 29.02784538269043\n",
      "Epoch [100/100], Train Loss: 23.421508538918417, Test Loss: 27.84344482421875\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.14320602416992, Test Loss: 41.71411895751953\n",
      "Epoch [20/100], Train Loss: 25.8849305012187, Test Loss: 30.578556060791016\n",
      "Epoch [30/100], Train Loss: 25.632912457575564, Test Loss: 53.17157745361328\n",
      "Epoch [40/100], Train Loss: 25.572869929329293, Test Loss: 28.39340591430664\n",
      "Epoch [50/100], Train Loss: 25.297625963805153, Test Loss: 29.935569763183594\n",
      "Epoch [60/100], Train Loss: 25.461793055299854, Test Loss: 28.7563419342041\n",
      "Epoch [70/100], Train Loss: 24.609382466800877, Test Loss: 29.105852127075195\n",
      "Epoch [80/100], Train Loss: 23.999377766593557, Test Loss: 28.76871681213379\n",
      "Epoch [90/100], Train Loss: 24.20740270770964, Test Loss: 29.13629722595215\n",
      "Epoch [100/100], Train Loss: 23.33368278878634, Test Loss: 30.012935638427734\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.91055492963947, Test Loss: 32.01209259033203\n",
      "Epoch [20/100], Train Loss: 27.229564572944017, Test Loss: 33.39288330078125\n",
      "Epoch [30/100], Train Loss: 25.053833945852812, Test Loss: 29.71147918701172\n",
      "Epoch [40/100], Train Loss: 25.597621085995534, Test Loss: 41.532005310058594\n",
      "Epoch [50/100], Train Loss: 25.243721214669648, Test Loss: 31.417905807495117\n",
      "Epoch [60/100], Train Loss: 24.437642013049516, Test Loss: 30.335708618164062\n",
      "Epoch [70/100], Train Loss: 24.583689342561314, Test Loss: 26.4543514251709\n",
      "Epoch [80/100], Train Loss: 24.457778155217405, Test Loss: 26.61845588684082\n",
      "Epoch [90/100], Train Loss: 24.323977554821578, Test Loss: 28.93634605407715\n",
      "Epoch [100/100], Train Loss: 23.783099509067224, Test Loss: 31.63694953918457\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.511156519905466, Test Loss: 36.48490905761719\n",
      "Epoch [20/100], Train Loss: 26.500492596235432, Test Loss: 31.551565170288086\n",
      "Epoch [30/100], Train Loss: 25.229537713723104, Test Loss: 34.534263610839844\n",
      "Epoch [40/100], Train Loss: 25.116117020904042, Test Loss: 28.192855834960938\n",
      "Epoch [50/100], Train Loss: 25.134167236578268, Test Loss: 28.16343116760254\n",
      "Epoch [60/100], Train Loss: 25.490574201990345, Test Loss: 28.758947372436523\n",
      "Epoch [70/100], Train Loss: 24.94806703661309, Test Loss: 28.511693954467773\n",
      "Epoch [80/100], Train Loss: 24.883100272006676, Test Loss: 29.217782974243164\n",
      "Epoch [90/100], Train Loss: 25.340500690897958, Test Loss: 28.19085121154785\n",
      "Epoch [100/100], Train Loss: 24.342665400270555, Test Loss: 28.912742614746094\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.577963788392115, Test Loss: 33.665435791015625\n",
      "Epoch [20/100], Train Loss: 26.86883677498239, Test Loss: 29.188295364379883\n",
      "Epoch [30/100], Train Loss: 26.428670170268074, Test Loss: 30.86821746826172\n",
      "Epoch [40/100], Train Loss: 26.24663616868316, Test Loss: 30.92710304260254\n",
      "Epoch [50/100], Train Loss: 24.79438310842045, Test Loss: 27.71401596069336\n",
      "Epoch [60/100], Train Loss: 25.82513645359727, Test Loss: 30.947965621948242\n",
      "Epoch [70/100], Train Loss: 23.895544965149927, Test Loss: 28.553743362426758\n",
      "Epoch [80/100], Train Loss: 25.502241259715596, Test Loss: 28.145023345947266\n",
      "Epoch [90/100], Train Loss: 25.527494111608288, Test Loss: 29.120187759399414\n",
      "Epoch [100/100], Train Loss: 24.37730423974209, Test Loss: 28.325334548950195\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.286887672299244, Test Loss: 32.97416687011719\n",
      "Epoch [20/100], Train Loss: 26.744913476412414, Test Loss: 30.8698673248291\n",
      "Epoch [30/100], Train Loss: 27.12246442700996, Test Loss: 31.880725860595703\n",
      "Epoch [40/100], Train Loss: 27.09791009621542, Test Loss: 30.825895309448242\n",
      "Epoch [50/100], Train Loss: 25.90644174794682, Test Loss: 31.630319595336914\n",
      "Epoch [60/100], Train Loss: 25.52010922666456, Test Loss: 29.624114990234375\n",
      "Epoch [70/100], Train Loss: 25.4994755541692, Test Loss: 29.129352569580078\n",
      "Epoch [80/100], Train Loss: 25.549195617925925, Test Loss: 28.655628204345703\n",
      "Epoch [90/100], Train Loss: 25.270564163708297, Test Loss: 28.98307228088379\n",
      "Epoch [100/100], Train Loss: 24.752858477733174, Test Loss: 32.006752014160156\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.315511447093527, Test Loss: 36.30335998535156\n",
      "Epoch [20/100], Train Loss: 27.042355965786292, Test Loss: 31.997337341308594\n",
      "Epoch [30/100], Train Loss: 26.46026669486624, Test Loss: 28.534265518188477\n",
      "Epoch [40/100], Train Loss: 26.15040455802542, Test Loss: 42.841983795166016\n",
      "Epoch [50/100], Train Loss: 25.79846320230453, Test Loss: 30.33844566345215\n",
      "Epoch [60/100], Train Loss: 25.482793576600123, Test Loss: 29.824260711669922\n",
      "Epoch [70/100], Train Loss: 25.800721640665024, Test Loss: 29.17344093322754\n",
      "Epoch [80/100], Train Loss: 25.92731914832944, Test Loss: 28.0830020904541\n",
      "Epoch [90/100], Train Loss: 25.239371878201844, Test Loss: 29.67144775390625\n",
      "Epoch [100/100], Train Loss: 24.67340066003018, Test Loss: 28.38148307800293\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.154776888988057, Test Loss: 33.920143127441406\n",
      "Epoch [20/100], Train Loss: 29.54536997060307, Test Loss: 35.726417541503906\n",
      "Epoch [30/100], Train Loss: 26.418709207753665, Test Loss: 36.573631286621094\n",
      "Epoch [40/100], Train Loss: 26.767321846133374, Test Loss: 39.53755187988281\n",
      "Epoch [50/100], Train Loss: 25.746671895511813, Test Loss: 34.48228454589844\n",
      "Epoch [60/100], Train Loss: 25.831128067266746, Test Loss: 29.659212112426758\n",
      "Epoch [70/100], Train Loss: 26.136284143416606, Test Loss: 30.84305191040039\n",
      "Epoch [80/100], Train Loss: 26.026467964297435, Test Loss: 30.479536056518555\n",
      "Epoch [90/100], Train Loss: 26.297839611866436, Test Loss: 29.375158309936523\n",
      "Epoch [100/100], Train Loss: 25.939935371523998, Test Loss: 30.285812377929688\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 42.838866949863124, Test Loss: 39.09159524100168\n",
      "Epoch [20/100], Train Loss: 35.42143154456967, Test Loss: 35.071724210466655\n",
      "Epoch [30/100], Train Loss: 31.065870184976546, Test Loss: 31.656297237842114\n",
      "Epoch [40/100], Train Loss: 28.306680010185868, Test Loss: 29.845817342981114\n",
      "Epoch [50/100], Train Loss: 26.42540702194464, Test Loss: 28.298580566009917\n",
      "Epoch [60/100], Train Loss: 24.886437638079535, Test Loss: 27.348658970424108\n",
      "Epoch [70/100], Train Loss: 24.84924650973961, Test Loss: 28.149629568124745\n",
      "Epoch [80/100], Train Loss: 25.73864724831503, Test Loss: 27.53706169128418\n",
      "Epoch [90/100], Train Loss: 22.993011355790937, Test Loss: 27.144251489020014\n",
      "Epoch [100/100], Train Loss: 23.66873429720519, Test Loss: 26.6137049291041\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 42.74672274980389, Test Loss: 42.83665991448737\n",
      "Epoch [20/100], Train Loss: 35.97391174191334, Test Loss: 34.32295712557706\n",
      "Epoch [30/100], Train Loss: 30.864505217505283, Test Loss: 32.11223106879692\n",
      "Epoch [40/100], Train Loss: 29.241184759921715, Test Loss: 28.985756366283862\n",
      "Epoch [50/100], Train Loss: 26.544304638221615, Test Loss: 28.80118853086001\n",
      "Epoch [60/100], Train Loss: 26.33549787177414, Test Loss: 28.357529825978464\n",
      "Epoch [70/100], Train Loss: 24.740614568991738, Test Loss: 27.98607972380403\n",
      "Epoch [80/100], Train Loss: 24.96965217590332, Test Loss: 27.52920081398704\n",
      "Epoch [90/100], Train Loss: 23.549760030527583, Test Loss: 27.852598982971983\n",
      "Epoch [100/100], Train Loss: 24.979479855396708, Test Loss: 28.08940733872451\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.048928958079856, Test Loss: 37.45099758792233\n",
      "Epoch [20/100], Train Loss: 37.15064987432761, Test Loss: 38.28745215279715\n",
      "Epoch [30/100], Train Loss: 32.34836363245229, Test Loss: 35.74451746259417\n",
      "Epoch [40/100], Train Loss: 28.998743863965643, Test Loss: 31.567505303915446\n",
      "Epoch [50/100], Train Loss: 26.839185883568934, Test Loss: 28.506470989871335\n",
      "Epoch [60/100], Train Loss: 25.540154238216214, Test Loss: 28.745297543414228\n",
      "Epoch [70/100], Train Loss: 25.774012481189164, Test Loss: 28.651317026708032\n",
      "Epoch [80/100], Train Loss: 24.767700683093462, Test Loss: 28.160376115278765\n",
      "Epoch [90/100], Train Loss: 24.088688534595928, Test Loss: 27.792639373184798\n",
      "Epoch [100/100], Train Loss: 24.394352384473457, Test Loss: 28.703009568251574\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 43.4529736878442, Test Loss: 41.59195208859134\n",
      "Epoch [20/100], Train Loss: 36.357559172833554, Test Loss: 33.44480437737007\n",
      "Epoch [30/100], Train Loss: 31.881734823008053, Test Loss: 33.66349321836001\n",
      "Epoch [40/100], Train Loss: 28.864878044753777, Test Loss: 29.515478728653548\n",
      "Epoch [50/100], Train Loss: 27.697739003916254, Test Loss: 29.484357982486873\n",
      "Epoch [60/100], Train Loss: 25.944385916287782, Test Loss: 28.11277674390124\n",
      "Epoch [70/100], Train Loss: 24.885134131009462, Test Loss: 28.49862021904487\n",
      "Epoch [80/100], Train Loss: 25.179402710961515, Test Loss: 28.46526220247343\n",
      "Epoch [90/100], Train Loss: 24.11864037435563, Test Loss: 27.59731810433524\n",
      "Epoch [100/100], Train Loss: 23.469173765964197, Test Loss: 28.299984250749862\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 42.59468402549869, Test Loss: 43.391078998516136\n",
      "Epoch [20/100], Train Loss: 35.64510698162142, Test Loss: 35.202999114990234\n",
      "Epoch [30/100], Train Loss: 31.62847502661533, Test Loss: 33.76886504037039\n",
      "Epoch [40/100], Train Loss: 28.543181347456134, Test Loss: 30.03702768102869\n",
      "Epoch [50/100], Train Loss: 26.95442891980781, Test Loss: 28.538248706173587\n",
      "Epoch [60/100], Train Loss: 25.913982516429463, Test Loss: 28.27082346631335\n",
      "Epoch [70/100], Train Loss: 25.05177671088547, Test Loss: 28.36506798979524\n",
      "Epoch [80/100], Train Loss: 24.995964162857806, Test Loss: 27.66264685098227\n",
      "Epoch [90/100], Train Loss: 23.71180853921859, Test Loss: 28.139370682951693\n",
      "Epoch [100/100], Train Loss: 24.20369336487817, Test Loss: 27.965228167447176\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.2177610803823, Test Loss: 42.60043800651253\n",
      "Epoch [20/100], Train Loss: 37.334208785510455, Test Loss: 37.39175984766576\n",
      "Epoch [30/100], Train Loss: 32.44684293152856, Test Loss: 34.907738375973395\n",
      "Epoch [40/100], Train Loss: 28.63235966416656, Test Loss: 30.73926427766874\n",
      "Epoch [50/100], Train Loss: 26.72266779414943, Test Loss: 29.283745208343902\n",
      "Epoch [60/100], Train Loss: 26.12466947836954, Test Loss: 28.87996366426542\n",
      "Epoch [70/100], Train Loss: 24.925776647348872, Test Loss: 28.08513576953442\n",
      "Epoch [80/100], Train Loss: 25.496150751582913, Test Loss: 28.268750004954153\n",
      "Epoch [90/100], Train Loss: 23.85045743848457, Test Loss: 28.44789130966385\n",
      "Epoch [100/100], Train Loss: 23.527665641659596, Test Loss: 27.827234119563908\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 42.77610327923884, Test Loss: 42.25832089510831\n",
      "Epoch [20/100], Train Loss: 35.848544242733816, Test Loss: 36.63446188592292\n",
      "Epoch [30/100], Train Loss: 31.146392159383804, Test Loss: 32.120324072899756\n",
      "Epoch [40/100], Train Loss: 28.4279339024278, Test Loss: 29.55827589158888\n",
      "Epoch [50/100], Train Loss: 26.155646452356557, Test Loss: 29.008259612244444\n",
      "Epoch [60/100], Train Loss: 25.967196348846937, Test Loss: 29.813621644849903\n",
      "Epoch [70/100], Train Loss: 24.758780582615586, Test Loss: 27.989689641184622\n",
      "Epoch [80/100], Train Loss: 25.58231023569576, Test Loss: 28.27486773899623\n",
      "Epoch [90/100], Train Loss: 24.089899832303406, Test Loss: 27.608699439407943\n",
      "Epoch [100/100], Train Loss: 24.509571169243483, Test Loss: 27.64762695114334\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 42.55531585568287, Test Loss: 36.84008922824612\n",
      "Epoch [20/100], Train Loss: 36.28050186907659, Test Loss: 33.7352639483167\n",
      "Epoch [30/100], Train Loss: 30.74084331324843, Test Loss: 33.88972495438217\n",
      "Epoch [40/100], Train Loss: 27.998684367195505, Test Loss: 29.138264643681513\n",
      "Epoch [50/100], Train Loss: 26.49696011152424, Test Loss: 28.485752749752688\n",
      "Epoch [60/100], Train Loss: 25.043179790309217, Test Loss: 27.39814802888152\n",
      "Epoch [70/100], Train Loss: 25.042503675867298, Test Loss: 27.64286336031827\n",
      "Epoch [80/100], Train Loss: 24.429096221923828, Test Loss: 28.07264754060027\n",
      "Epoch [90/100], Train Loss: 25.381556188864785, Test Loss: 28.453203894875266\n",
      "Epoch [100/100], Train Loss: 23.840562376428824, Test Loss: 27.950941308752284\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.00235745789575, Test Loss: 41.18107173969219\n",
      "Epoch [20/100], Train Loss: 37.01043862514808, Test Loss: 36.225311403150684\n",
      "Epoch [30/100], Train Loss: 32.03510133086658, Test Loss: 32.59184059539398\n",
      "Epoch [40/100], Train Loss: 29.06187421454758, Test Loss: 29.80234433458997\n",
      "Epoch [50/100], Train Loss: 26.630875721915825, Test Loss: 28.800033717960506\n",
      "Epoch [60/100], Train Loss: 25.699247488428334, Test Loss: 28.053665854714133\n",
      "Epoch [70/100], Train Loss: 25.472778507920562, Test Loss: 28.32896896461388\n",
      "Epoch [80/100], Train Loss: 25.213308228039352, Test Loss: 27.881286695406036\n",
      "Epoch [90/100], Train Loss: 24.282733754642674, Test Loss: 28.283580309384828\n",
      "Epoch [100/100], Train Loss: 25.607659249227556, Test Loss: 27.729274898380428\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.606167464959817, Test Loss: 30.343266920609906\n",
      "Epoch [20/100], Train Loss: 26.085669914620823, Test Loss: 28.76807519987032\n",
      "Epoch [30/100], Train Loss: 25.59832860602707, Test Loss: 29.38969475882394\n",
      "Epoch [40/100], Train Loss: 25.346458947853964, Test Loss: 28.491202564982625\n",
      "Epoch [50/100], Train Loss: 25.21988950635566, Test Loss: 28.868222992141526\n",
      "Epoch [60/100], Train Loss: 25.98142702071393, Test Loss: 29.93242429758047\n",
      "Epoch [70/100], Train Loss: 25.345806997330463, Test Loss: 28.49607584073946\n",
      "Epoch [80/100], Train Loss: 26.265695059104043, Test Loss: 28.103383398675298\n",
      "Epoch [90/100], Train Loss: 24.773077305027694, Test Loss: 28.352263066675757\n",
      "Epoch [100/100], Train Loss: 25.115887363621447, Test Loss: 29.262689986786285\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.070237194123816, Test Loss: 29.596968737515535\n",
      "Epoch [20/100], Train Loss: 25.52502037423556, Test Loss: 29.74227662519975\n",
      "Epoch [30/100], Train Loss: 26.093204398233382, Test Loss: 28.564860504943056\n",
      "Epoch [40/100], Train Loss: 25.627877257300206, Test Loss: 28.364533015659877\n",
      "Epoch [50/100], Train Loss: 27.348294004846792, Test Loss: 29.268088179749327\n",
      "Epoch [60/100], Train Loss: 26.589158692907116, Test Loss: 28.93705719786805\n",
      "Epoch [70/100], Train Loss: 26.108438410524464, Test Loss: 26.75289369558359\n",
      "Epoch [80/100], Train Loss: 25.493538565713852, Test Loss: 27.90933945891145\n",
      "Epoch [90/100], Train Loss: 24.69152359258933, Test Loss: 27.531741773927365\n",
      "Epoch [100/100], Train Loss: 23.93345001095631, Test Loss: 28.806572554947493\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.167592870993694, Test Loss: 31.020688837224785\n",
      "Epoch [20/100], Train Loss: 25.7270987026027, Test Loss: 29.466117462554536\n",
      "Epoch [30/100], Train Loss: 25.573793198632412, Test Loss: 28.688163707782696\n",
      "Epoch [40/100], Train Loss: 25.98855697756908, Test Loss: 28.334719422575716\n",
      "Epoch [50/100], Train Loss: 25.20723225953149, Test Loss: 27.986333797504376\n",
      "Epoch [60/100], Train Loss: 24.558810262211033, Test Loss: 27.59473531896418\n",
      "Epoch [70/100], Train Loss: 24.79764242328581, Test Loss: 30.400819431651723\n",
      "Epoch [80/100], Train Loss: 24.891710262611266, Test Loss: 26.70958390173974\n",
      "Epoch [90/100], Train Loss: 24.469306323567373, Test Loss: 28.984645249007585\n",
      "Epoch [100/100], Train Loss: 23.640909113649464, Test Loss: 27.187091765465674\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.267841658045032, Test Loss: 28.567852763386515\n",
      "Epoch [20/100], Train Loss: 28.32507529336898, Test Loss: 28.264103901850714\n",
      "Epoch [30/100], Train Loss: 25.585764050092852, Test Loss: 29.647011224325603\n",
      "Epoch [40/100], Train Loss: 26.439227326189886, Test Loss: 28.310314971131163\n",
      "Epoch [50/100], Train Loss: 27.010155512074956, Test Loss: 28.437496011907403\n",
      "Epoch [60/100], Train Loss: 25.87583722599217, Test Loss: 28.62481956977349\n",
      "Epoch [70/100], Train Loss: 25.501130957681625, Test Loss: 27.812227372999317\n",
      "Epoch [80/100], Train Loss: 26.601110151947523, Test Loss: 27.873545857218954\n",
      "Epoch [90/100], Train Loss: 25.11854226784628, Test Loss: 29.120936257498606\n",
      "Epoch [100/100], Train Loss: 23.864240924647596, Test Loss: 28.523636904629793\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.360911109799243, Test Loss: 29.26443119792195\n",
      "Epoch [20/100], Train Loss: 26.849775195512617, Test Loss: 31.85708660274357\n",
      "Epoch [30/100], Train Loss: 26.656739356869558, Test Loss: 27.97115382900486\n",
      "Epoch [40/100], Train Loss: 25.737715023853738, Test Loss: 28.789622071501498\n",
      "Epoch [50/100], Train Loss: 24.821410751342775, Test Loss: 27.644391815383713\n",
      "Epoch [60/100], Train Loss: 24.33517921635362, Test Loss: 27.93560268352558\n",
      "Epoch [70/100], Train Loss: 25.435481531111922, Test Loss: 27.586623724404866\n",
      "Epoch [80/100], Train Loss: 24.6443148378466, Test Loss: 28.463394363205154\n",
      "Epoch [90/100], Train Loss: 25.74020219396372, Test Loss: 28.274785549609692\n",
      "Epoch [100/100], Train Loss: 25.080816343964123, Test Loss: 28.17196967385032\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.178935973370663, Test Loss: 30.144138757284587\n",
      "Epoch [20/100], Train Loss: 26.835486602783202, Test Loss: 28.30852887537572\n",
      "Epoch [30/100], Train Loss: 26.315820262471185, Test Loss: 29.329794673176554\n",
      "Epoch [40/100], Train Loss: 25.351991759753616, Test Loss: 27.55515804538479\n",
      "Epoch [50/100], Train Loss: 24.754393299290392, Test Loss: 29.107656652277168\n",
      "Epoch [60/100], Train Loss: 25.164289505755313, Test Loss: 27.561356532109247\n",
      "Epoch [70/100], Train Loss: 24.912886159928117, Test Loss: 27.736648881590213\n",
      "Epoch [80/100], Train Loss: 24.752479759591523, Test Loss: 33.12138002569025\n",
      "Epoch [90/100], Train Loss: 24.682059053514823, Test Loss: 27.73591001931723\n",
      "Epoch [100/100], Train Loss: 24.3938925696201, Test Loss: 28.941107886178152\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.309709592725408, Test Loss: 31.211724120301085\n",
      "Epoch [20/100], Train Loss: 26.479374138253633, Test Loss: 29.65864696750393\n",
      "Epoch [30/100], Train Loss: 26.92705940496726, Test Loss: 29.38628119926948\n",
      "Epoch [40/100], Train Loss: 26.25923985340556, Test Loss: 27.9028969554158\n",
      "Epoch [50/100], Train Loss: 25.42412265715052, Test Loss: 29.20649340245631\n",
      "Epoch [60/100], Train Loss: 26.068569471015305, Test Loss: 30.931268394767464\n",
      "Epoch [70/100], Train Loss: 25.140547730492763, Test Loss: 28.745525112399807\n",
      "Epoch [80/100], Train Loss: 26.832700185306738, Test Loss: 28.801964970378133\n",
      "Epoch [90/100], Train Loss: 24.74820924352427, Test Loss: 28.5946921063708\n",
      "Epoch [100/100], Train Loss: 25.574770636636703, Test Loss: 29.665859519661247\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.053429888115556, Test Loss: 33.24598047330782\n",
      "Epoch [20/100], Train Loss: 27.695682075375416, Test Loss: 30.134068451918566\n",
      "Epoch [30/100], Train Loss: 27.322882205150165, Test Loss: 28.71468095655565\n",
      "Epoch [40/100], Train Loss: 26.301085300132875, Test Loss: 29.573764900108436\n",
      "Epoch [50/100], Train Loss: 26.086710157550748, Test Loss: 29.462414630047686\n",
      "Epoch [60/100], Train Loss: 26.2500129574635, Test Loss: 30.06287956237793\n",
      "Epoch [70/100], Train Loss: 26.06983443088219, Test Loss: 28.588912295056627\n",
      "Epoch [80/100], Train Loss: 25.146037142394018, Test Loss: 28.713616705560064\n",
      "Epoch [90/100], Train Loss: 25.73817493876473, Test Loss: 33.6137076291171\n",
      "Epoch [100/100], Train Loss: 25.217529284367796, Test Loss: 27.791830434427634\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.60335336904057, Test Loss: 32.10946709769113\n",
      "Epoch [20/100], Train Loss: 26.853574346323484, Test Loss: 30.600840135054156\n",
      "Epoch [30/100], Train Loss: 27.04365699017634, Test Loss: 29.35577219182795\n",
      "Epoch [40/100], Train Loss: 25.899635002261302, Test Loss: 28.759677639255276\n",
      "Epoch [50/100], Train Loss: 25.25159861455198, Test Loss: 29.50074983572031\n",
      "Epoch [60/100], Train Loss: 26.976539724381244, Test Loss: 28.901650639323446\n",
      "Epoch [70/100], Train Loss: 25.96415916192727, Test Loss: 28.65819886442903\n",
      "Epoch [80/100], Train Loss: 25.089820080116148, Test Loss: 29.813529547158772\n",
      "Epoch [90/100], Train Loss: 24.867177475475874, Test Loss: 29.137081493030895\n",
      "Epoch [100/100], Train Loss: 25.73622540333232, Test Loss: 28.932293681355265\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.99471380515177, Test Loss: 30.905261497992974\n",
      "Epoch [20/100], Train Loss: 26.812752658031027, Test Loss: 28.544321184034473\n",
      "Epoch [30/100], Train Loss: 26.732255354083954, Test Loss: 28.91149971701882\n",
      "Epoch [40/100], Train Loss: 26.13975408272665, Test Loss: 27.679432113449295\n",
      "Epoch [50/100], Train Loss: 26.163050967357197, Test Loss: 28.695148542329864\n",
      "Epoch [60/100], Train Loss: 25.13539698866547, Test Loss: 27.975655270861342\n",
      "Epoch [70/100], Train Loss: 25.955886403068167, Test Loss: 27.160620231133002\n",
      "Epoch [80/100], Train Loss: 25.071283252903672, Test Loss: 28.70427047432243\n",
      "Epoch [90/100], Train Loss: 25.129290646412333, Test Loss: 30.048494338989258\n",
      "Epoch [100/100], Train Loss: 25.4032778130203, Test Loss: 29.86244568267426\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.669516047493357, Test Loss: 32.662839319798856\n",
      "Epoch [20/100], Train Loss: 26.96414289005467, Test Loss: 31.816697801862443\n",
      "Epoch [30/100], Train Loss: 26.4892529909728, Test Loss: 30.277707582944398\n",
      "Epoch [40/100], Train Loss: 25.240214982579968, Test Loss: 30.192559898673714\n",
      "Epoch [50/100], Train Loss: 26.028125012507203, Test Loss: 29.768410026253044\n",
      "Epoch [60/100], Train Loss: 25.665138719902664, Test Loss: 30.592367271324257\n",
      "Epoch [70/100], Train Loss: 25.643734759971743, Test Loss: 29.027004687817065\n",
      "Epoch [80/100], Train Loss: 26.041210975021613, Test Loss: 28.1815811008602\n",
      "Epoch [90/100], Train Loss: 26.085838355392706, Test Loss: 28.844753884649897\n",
      "Epoch [100/100], Train Loss: 25.80501516373431, Test Loss: 28.335517313573266\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.595988101646547, Test Loss: 37.15299740085354\n",
      "Epoch [20/100], Train Loss: 29.321597390096695, Test Loss: 28.727581544355914\n",
      "Epoch [30/100], Train Loss: 26.26519214442519, Test Loss: 30.676831133953936\n",
      "Epoch [40/100], Train Loss: 25.852101723092503, Test Loss: 28.17762902495149\n",
      "Epoch [50/100], Train Loss: 26.602316121586032, Test Loss: 27.69327364339457\n",
      "Epoch [60/100], Train Loss: 26.75149908222136, Test Loss: 29.13222563111937\n",
      "Epoch [70/100], Train Loss: 26.37395679911629, Test Loss: 28.695127462411854\n",
      "Epoch [80/100], Train Loss: 25.972695597664256, Test Loss: 27.2671747579203\n",
      "Epoch [90/100], Train Loss: 25.93464696290063, Test Loss: 27.140864979137074\n",
      "Epoch [100/100], Train Loss: 25.253477578084976, Test Loss: 28.266834110408634\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.229254275462665, Test Loss: 43.4950346513228\n",
      "Epoch [20/100], Train Loss: 26.66803300576132, Test Loss: 31.27263049336223\n",
      "Epoch [30/100], Train Loss: 27.186378385199877, Test Loss: 31.829968464838995\n",
      "Epoch [40/100], Train Loss: 27.48036099418265, Test Loss: 28.073177535812576\n",
      "Epoch [50/100], Train Loss: 26.09492591482694, Test Loss: 28.81356415191254\n",
      "Epoch [60/100], Train Loss: 25.34474982589972, Test Loss: 27.605198624846224\n",
      "Epoch [70/100], Train Loss: 26.14105128303903, Test Loss: 28.231956333309025\n",
      "Epoch [80/100], Train Loss: 25.531762326349977, Test Loss: 27.653591304630428\n",
      "Epoch [90/100], Train Loss: 25.395909819055777, Test Loss: 27.100840308449484\n",
      "Epoch [100/100], Train Loss: 26.165959392610144, Test Loss: 27.977361802930957\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.820308791613968, Test Loss: 58.45353708638773\n",
      "Epoch [20/100], Train Loss: 28.42062352915279, Test Loss: 29.728118624005997\n",
      "Epoch [30/100], Train Loss: 27.291186779835186, Test Loss: 30.870091054346656\n",
      "Epoch [40/100], Train Loss: 27.75305399034844, Test Loss: 34.36067103101062\n",
      "Epoch [50/100], Train Loss: 26.438451835757395, Test Loss: 30.944012555209074\n",
      "Epoch [60/100], Train Loss: 26.036206842641363, Test Loss: 29.74616543658368\n",
      "Epoch [70/100], Train Loss: 25.797793616623174, Test Loss: 28.642613398564325\n",
      "Epoch [80/100], Train Loss: 25.84834429631468, Test Loss: 28.950601181426606\n",
      "Epoch [90/100], Train Loss: 26.500317132668417, Test Loss: 30.251913120220234\n",
      "Epoch [100/100], Train Loss: 25.432577033121078, Test Loss: 29.730091342678318\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.693860413598234, Test Loss: 31.213393198979364\n",
      "Epoch [20/100], Train Loss: 26.777261515132718, Test Loss: 28.810671224222556\n",
      "Epoch [30/100], Train Loss: 26.29819616098873, Test Loss: 29.25014116856959\n",
      "Epoch [40/100], Train Loss: 28.055343978131404, Test Loss: 29.809509004865372\n",
      "Epoch [50/100], Train Loss: 26.481202066140096, Test Loss: 27.878950787829115\n",
      "Epoch [60/100], Train Loss: 26.800219538954437, Test Loss: 30.113932275152827\n",
      "Epoch [70/100], Train Loss: 26.111586636402567, Test Loss: 28.360417427954737\n",
      "Epoch [80/100], Train Loss: 25.52996503486008, Test Loss: 29.42917335807503\n",
      "Epoch [90/100], Train Loss: 24.47251530631644, Test Loss: 30.348287532855938\n",
      "Epoch [100/100], Train Loss: 25.15786292904713, Test Loss: 29.502192608721845\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.53499999749856, Test Loss: 34.10256826722777\n",
      "Epoch [20/100], Train Loss: 29.080903637995487, Test Loss: 30.57920312262201\n",
      "Epoch [30/100], Train Loss: 27.802695346269452, Test Loss: 32.63727113798067\n",
      "Epoch [40/100], Train Loss: 26.69772258821081, Test Loss: 29.394553593226842\n",
      "Epoch [50/100], Train Loss: 27.68204108066246, Test Loss: 28.82921028137207\n",
      "Epoch [60/100], Train Loss: 26.41365571569224, Test Loss: 30.765353561995866\n",
      "Epoch [70/100], Train Loss: 28.850705274988393, Test Loss: 28.8600634537734\n",
      "Epoch [80/100], Train Loss: 26.024229731325242, Test Loss: 28.55344891238522\n",
      "Epoch [90/100], Train Loss: 24.888902464069304, Test Loss: 30.876811758264317\n",
      "Epoch [100/100], Train Loss: 25.483464825739627, Test Loss: 29.38709192152147\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.943511262487192, Test Loss: 34.0576179553936\n",
      "Epoch [20/100], Train Loss: 28.524259823658426, Test Loss: 42.92205270544275\n",
      "Epoch [30/100], Train Loss: 28.186627291069655, Test Loss: 32.67897088187082\n",
      "Epoch [40/100], Train Loss: 26.154543016777662, Test Loss: 28.63553775440563\n",
      "Epoch [50/100], Train Loss: 27.22349200014208, Test Loss: 28.920940498252968\n",
      "Epoch [60/100], Train Loss: 26.452378907750866, Test Loss: 29.15773780624588\n",
      "Epoch [70/100], Train Loss: 26.302535429157196, Test Loss: 34.6444865140048\n",
      "Epoch [80/100], Train Loss: 26.037148516295385, Test Loss: 29.151128397359475\n",
      "Epoch [90/100], Train Loss: 27.437888711397765, Test Loss: 29.314071952522575\n",
      "Epoch [100/100], Train Loss: 26.766195366030832, Test Loss: 29.067673670781122\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.193492945686714, Test Loss: 35.41578074863979\n",
      "Epoch [20/100], Train Loss: 29.820987244903066, Test Loss: 30.090774238883675\n",
      "Epoch [30/100], Train Loss: 26.47296121315878, Test Loss: 33.12423185868697\n",
      "Epoch [40/100], Train Loss: 27.78124716399146, Test Loss: 30.539537355497284\n",
      "Epoch [50/100], Train Loss: 27.07162519986512, Test Loss: 30.252373138031402\n",
      "Epoch [60/100], Train Loss: 27.972073195410555, Test Loss: 30.863445182899376\n",
      "Epoch [70/100], Train Loss: 26.204233713619043, Test Loss: 31.921856446699664\n",
      "Epoch [80/100], Train Loss: 26.165993837450372, Test Loss: 29.32909110923866\n",
      "Epoch [90/100], Train Loss: 27.11693384139264, Test Loss: 30.923160404353947\n",
      "Epoch [100/100], Train Loss: 27.33272413034908, Test Loss: 29.66666506482409\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.959827210473236, Test Loss: 46.26860868775999\n",
      "Epoch [20/100], Train Loss: 42.2398126946121, Test Loss: 41.98430633544922\n",
      "Epoch [30/100], Train Loss: 38.30691508308786, Test Loss: 36.858767323679736\n",
      "Epoch [40/100], Train Loss: 34.7718469838627, Test Loss: 39.66491510960963\n",
      "Epoch [50/100], Train Loss: 31.991626051605724, Test Loss: 32.60540072948902\n",
      "Epoch [60/100], Train Loss: 30.426354154993277, Test Loss: 32.15764437093363\n",
      "Epoch [70/100], Train Loss: 27.471262215786293, Test Loss: 29.37511815653219\n",
      "Epoch [80/100], Train Loss: 25.97441830869581, Test Loss: 29.25196969663942\n",
      "Epoch [90/100], Train Loss: 25.29715901359183, Test Loss: 29.353398707005883\n",
      "Epoch [100/100], Train Loss: 23.604469286809202, Test Loss: 28.72512943713696\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.25039711623895, Test Loss: 44.76215803468382\n",
      "Epoch [20/100], Train Loss: 41.98428901297147, Test Loss: 40.05057173889953\n",
      "Epoch [30/100], Train Loss: 37.471530870531424, Test Loss: 36.637199822958415\n",
      "Epoch [40/100], Train Loss: 34.001861559758424, Test Loss: 37.18734334970449\n",
      "Epoch [50/100], Train Loss: 30.977135542572523, Test Loss: 32.97325337397588\n",
      "Epoch [60/100], Train Loss: 28.693364240302415, Test Loss: 30.395381630241097\n",
      "Epoch [70/100], Train Loss: 26.662942204709914, Test Loss: 29.182503167684978\n",
      "Epoch [80/100], Train Loss: 26.003245600716014, Test Loss: 28.3715167850643\n",
      "Epoch [90/100], Train Loss: 25.25855167576524, Test Loss: 28.394199420879414\n",
      "Epoch [100/100], Train Loss: 24.032592098048475, Test Loss: 28.417420350111925\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.821523359955336, Test Loss: 44.91707388147131\n",
      "Epoch [20/100], Train Loss: 42.924870475393824, Test Loss: 40.3848470712637\n",
      "Epoch [30/100], Train Loss: 38.84804731275214, Test Loss: 36.067070799988585\n",
      "Epoch [40/100], Train Loss: 35.30752974963579, Test Loss: 34.15727209115957\n",
      "Epoch [50/100], Train Loss: 32.34839336833016, Test Loss: 32.007595285192714\n",
      "Epoch [60/100], Train Loss: 30.25069887755347, Test Loss: 31.808101183408265\n",
      "Epoch [70/100], Train Loss: 28.027365793947315, Test Loss: 31.4005144292658\n",
      "Epoch [80/100], Train Loss: 26.944596837778562, Test Loss: 29.349059315470907\n",
      "Epoch [90/100], Train Loss: 25.054995195982887, Test Loss: 28.48155891121208\n",
      "Epoch [100/100], Train Loss: 25.375085555529985, Test Loss: 27.955609755082563\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.698723239586, Test Loss: 46.048239720332155\n",
      "Epoch [20/100], Train Loss: 42.24565434690382, Test Loss: 40.750571907340706\n",
      "Epoch [30/100], Train Loss: 38.157209965440096, Test Loss: 39.03670818774731\n",
      "Epoch [40/100], Train Loss: 34.82211266189325, Test Loss: 33.68470516452542\n",
      "Epoch [50/100], Train Loss: 32.08101759113249, Test Loss: 33.21536955895362\n",
      "Epoch [60/100], Train Loss: 29.405539265616994, Test Loss: 31.913505653282265\n",
      "Epoch [70/100], Train Loss: 27.656847794329533, Test Loss: 32.10879744492568\n",
      "Epoch [80/100], Train Loss: 26.820694369957096, Test Loss: 29.599156218689757\n",
      "Epoch [90/100], Train Loss: 25.082212892125863, Test Loss: 29.40158244541713\n",
      "Epoch [100/100], Train Loss: 24.988226080722495, Test Loss: 28.798109029794666\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.40944361452196, Test Loss: 46.23317470798245\n",
      "Epoch [20/100], Train Loss: 41.8236665631904, Test Loss: 40.08530282354974\n",
      "Epoch [30/100], Train Loss: 37.853084376600926, Test Loss: 38.28355888267616\n",
      "Epoch [40/100], Train Loss: 34.038726662807775, Test Loss: 36.160526771049994\n",
      "Epoch [50/100], Train Loss: 31.175199208494092, Test Loss: 32.822198644861\n",
      "Epoch [60/100], Train Loss: 28.706923275306576, Test Loss: 32.524810146975824\n",
      "Epoch [70/100], Train Loss: 27.710981550373013, Test Loss: 30.307365813812652\n",
      "Epoch [80/100], Train Loss: 26.43086045061956, Test Loss: 28.694957237739068\n",
      "Epoch [90/100], Train Loss: 24.531421204864003, Test Loss: 28.42849548141678\n",
      "Epoch [100/100], Train Loss: 24.90897467566318, Test Loss: 28.48976989845177\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.02096657674821, Test Loss: 44.453211648123606\n",
      "Epoch [20/100], Train Loss: 42.74291368078013, Test Loss: 40.633519308907644\n",
      "Epoch [30/100], Train Loss: 38.78391453477203, Test Loss: 38.172683468112695\n",
      "Epoch [40/100], Train Loss: 35.133192143674755, Test Loss: 35.73680067681647\n",
      "Epoch [50/100], Train Loss: 32.300193792874694, Test Loss: 35.26659078721876\n",
      "Epoch [60/100], Train Loss: 30.31881045982486, Test Loss: 32.28069439182034\n",
      "Epoch [70/100], Train Loss: 28.92261251230709, Test Loss: 30.533924920218332\n",
      "Epoch [80/100], Train Loss: 27.007749701327963, Test Loss: 29.026495128482967\n",
      "Epoch [90/100], Train Loss: 25.21709506550773, Test Loss: 28.675159380033417\n",
      "Epoch [100/100], Train Loss: 24.351960397939212, Test Loss: 27.674969512146788\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.564435352262905, Test Loss: 45.07804568402179\n",
      "Epoch [20/100], Train Loss: 42.11179594446401, Test Loss: 39.532828690169694\n",
      "Epoch [30/100], Train Loss: 38.194807021344296, Test Loss: 38.423375711812604\n",
      "Epoch [40/100], Train Loss: 35.05111599281186, Test Loss: 37.833033920882585\n",
      "Epoch [50/100], Train Loss: 31.962011349787478, Test Loss: 36.708082422033534\n",
      "Epoch [60/100], Train Loss: 29.40195662701716, Test Loss: 30.618339167012795\n",
      "Epoch [70/100], Train Loss: 27.92231604779353, Test Loss: 29.953048359264027\n",
      "Epoch [80/100], Train Loss: 26.914585976522478, Test Loss: 28.67670185534985\n",
      "Epoch [90/100], Train Loss: 26.098917845428968, Test Loss: 29.568897618875877\n",
      "Epoch [100/100], Train Loss: 24.51740888376705, Test Loss: 28.045538914668096\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.54935705466349, Test Loss: 45.52205534105177\n",
      "Epoch [20/100], Train Loss: 41.93099939315046, Test Loss: 41.48752162982891\n",
      "Epoch [30/100], Train Loss: 37.93518045144003, Test Loss: 34.89720465920188\n",
      "Epoch [40/100], Train Loss: 34.76717097172972, Test Loss: 35.15723713961515\n",
      "Epoch [50/100], Train Loss: 31.88999116616171, Test Loss: 33.50164983179662\n",
      "Epoch [60/100], Train Loss: 29.60009560506852, Test Loss: 31.57050742112197\n",
      "Epoch [70/100], Train Loss: 29.067105777928088, Test Loss: 29.32246983515752\n",
      "Epoch [80/100], Train Loss: 26.816696410882667, Test Loss: 28.257560135482194\n",
      "Epoch [90/100], Train Loss: 24.539827440605787, Test Loss: 28.259240856418362\n",
      "Epoch [100/100], Train Loss: 24.60464003516025, Test Loss: 27.455537399688325\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.980117009897704, Test Loss: 39.737937679538476\n",
      "Epoch [20/100], Train Loss: 42.86153654504995, Test Loss: 41.40029525756836\n",
      "Epoch [30/100], Train Loss: 39.08636345159812, Test Loss: 38.73722765043184\n",
      "Epoch [40/100], Train Loss: 35.659406505647254, Test Loss: 35.00065697013558\n",
      "Epoch [50/100], Train Loss: 32.92555523231381, Test Loss: 32.44169207981655\n",
      "Epoch [60/100], Train Loss: 30.5629138321173, Test Loss: 30.96311465176669\n",
      "Epoch [70/100], Train Loss: 28.220519194055775, Test Loss: 30.75060834513082\n",
      "Epoch [80/100], Train Loss: 27.0032744986112, Test Loss: 29.00463109202199\n",
      "Epoch [90/100], Train Loss: 25.56416362699915, Test Loss: 28.441681378847594\n",
      "Epoch [100/100], Train Loss: 25.016436504926837, Test Loss: 29.66038842634721\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.62518162336506, Test Loss: 30.411275640710606\n",
      "Epoch [20/100], Train Loss: 25.117123162941855, Test Loss: 28.18454445182503\n",
      "Epoch [30/100], Train Loss: 24.94129168400999, Test Loss: 28.40630035895806\n",
      "Epoch [40/100], Train Loss: 25.057751383546922, Test Loss: 27.95541490827288\n",
      "Epoch [50/100], Train Loss: 23.858339253409966, Test Loss: 28.089361785294173\n",
      "Epoch [60/100], Train Loss: 24.613512677051983, Test Loss: 28.081579877184584\n",
      "Epoch [70/100], Train Loss: 23.06454466522717, Test Loss: 27.512138267616173\n",
      "Epoch [80/100], Train Loss: 23.7137008041632, Test Loss: 28.44704583403352\n",
      "Epoch [90/100], Train Loss: 22.452282370895635, Test Loss: 28.872911651413162\n",
      "Epoch [100/100], Train Loss: 23.70441945810787, Test Loss: 28.670078574837028\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.718602633867107, Test Loss: 32.96363810749797\n",
      "Epoch [20/100], Train Loss: 25.414138593830046, Test Loss: 29.358001560359806\n",
      "Epoch [30/100], Train Loss: 24.570862404244846, Test Loss: 28.701425552368164\n",
      "Epoch [40/100], Train Loss: 24.62575536634101, Test Loss: 29.521391162624607\n",
      "Epoch [50/100], Train Loss: 24.412513251382798, Test Loss: 28.031009996092166\n",
      "Epoch [60/100], Train Loss: 23.282405740706647, Test Loss: 27.939635512116666\n",
      "Epoch [70/100], Train Loss: 23.772828467947537, Test Loss: 29.479936426336113\n",
      "Epoch [80/100], Train Loss: 23.481821935684955, Test Loss: 27.629174294409815\n",
      "Epoch [90/100], Train Loss: 22.90025221402528, Test Loss: 28.02125123259309\n",
      "Epoch [100/100], Train Loss: 24.689870502909677, Test Loss: 28.188372673926416\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.550818327606702, Test Loss: 29.173139770309646\n",
      "Epoch [20/100], Train Loss: 26.92925193036189, Test Loss: 28.032595547762785\n",
      "Epoch [30/100], Train Loss: 25.351990333932346, Test Loss: 32.205991596370545\n",
      "Epoch [40/100], Train Loss: 23.66972735670746, Test Loss: 27.560229264296495\n",
      "Epoch [50/100], Train Loss: 25.171915473312627, Test Loss: 27.71185302734375\n",
      "Epoch [60/100], Train Loss: 24.522886044861842, Test Loss: 28.83429277098024\n",
      "Epoch [70/100], Train Loss: 22.90279673591989, Test Loss: 28.268420826305043\n",
      "Epoch [80/100], Train Loss: 23.620090590930374, Test Loss: 28.012960805521384\n",
      "Epoch [90/100], Train Loss: 24.279164429961657, Test Loss: 29.599071032041078\n",
      "Epoch [100/100], Train Loss: 23.09330699482902, Test Loss: 28.67965755214939\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.683457540293208, Test Loss: 31.212098059716162\n",
      "Epoch [20/100], Train Loss: 25.7890962506904, Test Loss: 28.77538237633643\n",
      "Epoch [30/100], Train Loss: 25.045191549082272, Test Loss: 28.913828713553293\n",
      "Epoch [40/100], Train Loss: 25.19329453765369, Test Loss: 27.851399632243368\n",
      "Epoch [50/100], Train Loss: 25.18903293296939, Test Loss: 28.497081756591797\n",
      "Epoch [60/100], Train Loss: 25.81426230258629, Test Loss: 27.857178006853378\n",
      "Epoch [70/100], Train Loss: 23.948428569856237, Test Loss: 27.447556186031985\n",
      "Epoch [80/100], Train Loss: 23.659326459540697, Test Loss: 27.663567778352018\n",
      "Epoch [90/100], Train Loss: 24.103195390544954, Test Loss: 29.21516532402534\n",
      "Epoch [100/100], Train Loss: 23.498500867749822, Test Loss: 29.627536278266412\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.26959499296595, Test Loss: 31.01873861040388\n",
      "Epoch [20/100], Train Loss: 25.608325476724595, Test Loss: 29.03281035980621\n",
      "Epoch [30/100], Train Loss: 25.474474609875287, Test Loss: 28.18932548126617\n",
      "Epoch [40/100], Train Loss: 24.52431721921827, Test Loss: 28.078013655427213\n",
      "Epoch [50/100], Train Loss: 24.661150047427316, Test Loss: 27.944631452684277\n",
      "Epoch [60/100], Train Loss: 24.37606505409616, Test Loss: 30.421836085133737\n",
      "Epoch [70/100], Train Loss: 24.643132156622215, Test Loss: 27.781752920770025\n",
      "Epoch [80/100], Train Loss: 24.23523455760518, Test Loss: 27.901417818936434\n",
      "Epoch [90/100], Train Loss: 23.50891031984423, Test Loss: 31.59441933074555\n",
      "Epoch [100/100], Train Loss: 25.623729930940222, Test Loss: 29.32412798992999\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.803555147765113, Test Loss: 31.737602679760425\n",
      "Epoch [20/100], Train Loss: 25.147143660998736, Test Loss: 28.274326522628982\n",
      "Epoch [30/100], Train Loss: 25.78596787999888, Test Loss: 28.447992027579964\n",
      "Epoch [40/100], Train Loss: 24.662211934074026, Test Loss: 27.209110953591086\n",
      "Epoch [50/100], Train Loss: 24.682622446779344, Test Loss: 28.741198799826883\n",
      "Epoch [60/100], Train Loss: 24.388914458478084, Test Loss: 27.000876339999113\n",
      "Epoch [70/100], Train Loss: 24.23222670633285, Test Loss: 27.799755542309253\n",
      "Epoch [80/100], Train Loss: 24.7514383222236, Test Loss: 26.891079915034307\n",
      "Epoch [90/100], Train Loss: 24.049982564957414, Test Loss: 28.14203405999518\n",
      "Epoch [100/100], Train Loss: 22.920679048632014, Test Loss: 28.354289364505124\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.03561716548732, Test Loss: 34.60575906332437\n",
      "Epoch [20/100], Train Loss: 27.287427608302384, Test Loss: 30.20963629190024\n",
      "Epoch [30/100], Train Loss: 26.464253447485753, Test Loss: 31.03349110987279\n",
      "Epoch [40/100], Train Loss: 26.261198725465867, Test Loss: 30.3671827440138\n",
      "Epoch [50/100], Train Loss: 25.528148132074076, Test Loss: 28.13399012677081\n",
      "Epoch [60/100], Train Loss: 25.826887168258917, Test Loss: 42.15385863068816\n",
      "Epoch [70/100], Train Loss: 24.73381667840676, Test Loss: 29.04266392100941\n",
      "Epoch [80/100], Train Loss: 24.761208925090852, Test Loss: 28.66162736075265\n",
      "Epoch [90/100], Train Loss: 23.785985640228773, Test Loss: 28.634773303936054\n",
      "Epoch [100/100], Train Loss: 24.67730209475658, Test Loss: 30.201868750832297\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.34408112822986, Test Loss: 34.357511594698025\n",
      "Epoch [20/100], Train Loss: 26.962457932018843, Test Loss: 30.943300420587715\n",
      "Epoch [30/100], Train Loss: 26.788920680812147, Test Loss: 28.63820809203309\n",
      "Epoch [40/100], Train Loss: 25.725915158381227, Test Loss: 28.342691793070212\n",
      "Epoch [50/100], Train Loss: 25.88761260235896, Test Loss: 28.568767275129044\n",
      "Epoch [60/100], Train Loss: 24.207792926225505, Test Loss: 30.494220213456586\n",
      "Epoch [70/100], Train Loss: 24.895950630062917, Test Loss: 28.627351414073598\n",
      "Epoch [80/100], Train Loss: 24.390610066398246, Test Loss: 28.394411309972988\n",
      "Epoch [90/100], Train Loss: 25.64578544116411, Test Loss: 28.876788399436258\n",
      "Epoch [100/100], Train Loss: 23.820387443167263, Test Loss: 28.619667722033217\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.75957306408491, Test Loss: 33.61354391915457\n",
      "Epoch [20/100], Train Loss: 27.015932846069337, Test Loss: 30.016355291589512\n",
      "Epoch [30/100], Train Loss: 25.80488293131844, Test Loss: 33.587576606056906\n",
      "Epoch [40/100], Train Loss: 25.412991426811843, Test Loss: 31.826298750840223\n",
      "Epoch [50/100], Train Loss: 25.199647847159966, Test Loss: 29.075501602965517\n",
      "Epoch [60/100], Train Loss: 25.03676717789447, Test Loss: 29.28829408620859\n",
      "Epoch [70/100], Train Loss: 25.094198846035315, Test Loss: 31.539450063333884\n",
      "Epoch [80/100], Train Loss: 25.1659212894127, Test Loss: 28.16866399096204\n",
      "Epoch [90/100], Train Loss: 24.157745942913117, Test Loss: 28.487945952972808\n",
      "Epoch [100/100], Train Loss: 24.697411033755444, Test Loss: 29.57162837239055\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.72561090187948, Test Loss: 30.35839591088233\n",
      "Epoch [20/100], Train Loss: 25.81227271283259, Test Loss: 30.02136619369705\n",
      "Epoch [30/100], Train Loss: 26.713571679787556, Test Loss: 28.24131341414018\n",
      "Epoch [40/100], Train Loss: 24.018994872296442, Test Loss: 29.023511168244596\n",
      "Epoch [50/100], Train Loss: 25.016355301903896, Test Loss: 27.975244794573104\n",
      "Epoch [60/100], Train Loss: 25.375431542318374, Test Loss: 28.763225060004693\n",
      "Epoch [70/100], Train Loss: 23.736066030283443, Test Loss: 27.400989730636795\n",
      "Epoch [80/100], Train Loss: 23.67416832720647, Test Loss: 28.040273047112798\n",
      "Epoch [90/100], Train Loss: 25.11034921739922, Test Loss: 27.725181728214412\n",
      "Epoch [100/100], Train Loss: 24.11686396989666, Test Loss: 28.905649061326855\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.54818164012471, Test Loss: 30.894779725508258\n",
      "Epoch [20/100], Train Loss: 27.25131862827989, Test Loss: 30.876974477396384\n",
      "Epoch [30/100], Train Loss: 25.780964191624374, Test Loss: 30.600966441166864\n",
      "Epoch [40/100], Train Loss: 25.09702805065718, Test Loss: 27.199787115121815\n",
      "Epoch [50/100], Train Loss: 25.021834708041833, Test Loss: 27.485457482276026\n",
      "Epoch [60/100], Train Loss: 25.6778443883677, Test Loss: 29.381590905127588\n",
      "Epoch [70/100], Train Loss: 24.812177526755413, Test Loss: 29.72245890134341\n",
      "Epoch [80/100], Train Loss: 24.293794869594887, Test Loss: 28.334299459085837\n",
      "Epoch [90/100], Train Loss: 24.59478168174869, Test Loss: 28.75335069136186\n",
      "Epoch [100/100], Train Loss: 24.597139689961416, Test Loss: 29.81683810345538\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.077822494506837, Test Loss: 29.786999169882243\n",
      "Epoch [20/100], Train Loss: 26.310377258550925, Test Loss: 29.015879494803293\n",
      "Epoch [30/100], Train Loss: 25.176136592177095, Test Loss: 29.705131084888013\n",
      "Epoch [40/100], Train Loss: 23.78176139456327, Test Loss: 27.883861838997184\n",
      "Epoch [50/100], Train Loss: 25.68118655095335, Test Loss: 30.09984251740691\n",
      "Epoch [60/100], Train Loss: 24.969671424490507, Test Loss: 28.486819725532037\n",
      "Epoch [70/100], Train Loss: 26.051880289296633, Test Loss: 28.728829569630808\n",
      "Epoch [80/100], Train Loss: 24.636280585117028, Test Loss: 30.033427696723443\n",
      "Epoch [90/100], Train Loss: 24.574376078121, Test Loss: 28.65463960325563\n",
      "Epoch [100/100], Train Loss: 24.344169485373577, Test Loss: 28.47162643036285\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.520458127631517, Test Loss: 31.071956163877015\n",
      "Epoch [20/100], Train Loss: 27.406892226172275, Test Loss: 28.880379391955092\n",
      "Epoch [30/100], Train Loss: 25.317018102427, Test Loss: 29.49486918263621\n",
      "Epoch [40/100], Train Loss: 25.85581069696145, Test Loss: 28.835795910327466\n",
      "Epoch [50/100], Train Loss: 26.589309817454854, Test Loss: 29.16799597306685\n",
      "Epoch [60/100], Train Loss: 25.290402834532692, Test Loss: 28.293279796451717\n",
      "Epoch [70/100], Train Loss: 24.87484817504883, Test Loss: 27.798953118262354\n",
      "Epoch [80/100], Train Loss: 24.670310886570665, Test Loss: 27.871142176838664\n",
      "Epoch [90/100], Train Loss: 25.090498158189117, Test Loss: 30.677124097749786\n",
      "Epoch [100/100], Train Loss: 24.79947647970231, Test Loss: 27.96934060926561\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.67741143898886, Test Loss: 32.148711836183224\n",
      "Epoch [20/100], Train Loss: 26.825707357437885, Test Loss: 32.296254096093115\n",
      "Epoch [30/100], Train Loss: 26.524943104728322, Test Loss: 32.55120056945008\n",
      "Epoch [40/100], Train Loss: 27.27980706261807, Test Loss: 34.537025625055485\n",
      "Epoch [50/100], Train Loss: 25.772828943221295, Test Loss: 29.090365224070364\n",
      "Epoch [60/100], Train Loss: 27.054468661448993, Test Loss: 27.628734910642947\n",
      "Epoch [70/100], Train Loss: 24.866029676843862, Test Loss: 28.18053773161653\n",
      "Epoch [80/100], Train Loss: 25.701702480628843, Test Loss: 27.51173817027699\n",
      "Epoch [90/100], Train Loss: 26.308532152019563, Test Loss: 29.439259789206766\n",
      "Epoch [100/100], Train Loss: 25.946549975285766, Test Loss: 29.208858836780895\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.5716447173572, Test Loss: 35.94863022147835\n",
      "Epoch [20/100], Train Loss: 26.405020573100106, Test Loss: 30.646902084350586\n",
      "Epoch [30/100], Train Loss: 26.916584965440094, Test Loss: 28.689224788120814\n",
      "Epoch [40/100], Train Loss: 26.8158603918357, Test Loss: 28.429781108707576\n",
      "Epoch [50/100], Train Loss: 25.795704738429336, Test Loss: 29.491051587191496\n",
      "Epoch [60/100], Train Loss: 24.798659283997583, Test Loss: 31.490435464041575\n",
      "Epoch [70/100], Train Loss: 25.780935137389136, Test Loss: 28.351985039649072\n",
      "Epoch [80/100], Train Loss: 24.666796562319895, Test Loss: 29.389427061204785\n",
      "Epoch [90/100], Train Loss: 25.471142834522684, Test Loss: 30.141457743458933\n",
      "Epoch [100/100], Train Loss: 25.020071139101123, Test Loss: 28.88265547814307\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.640937705118148, Test Loss: 33.97743945926815\n",
      "Epoch [20/100], Train Loss: 27.295639425809266, Test Loss: 32.97759147743126\n",
      "Epoch [30/100], Train Loss: 26.86508960411197, Test Loss: 29.18018135467133\n",
      "Epoch [40/100], Train Loss: 25.701910738085136, Test Loss: 31.507970165896726\n",
      "Epoch [50/100], Train Loss: 26.901878976040198, Test Loss: 29.94289120760831\n",
      "Epoch [60/100], Train Loss: 25.73216821639264, Test Loss: 30.306942506269976\n",
      "Epoch [70/100], Train Loss: 26.967135826486057, Test Loss: 28.220137137871284\n",
      "Epoch [80/100], Train Loss: 26.839240402471823, Test Loss: 32.19156096817611\n",
      "Epoch [90/100], Train Loss: 26.540899026589315, Test Loss: 28.985638085897868\n",
      "Epoch [100/100], Train Loss: 24.94210874838907, Test Loss: 31.258758544921875\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.378849267177895, Test Loss: 38.864878270533175\n",
      "Epoch [20/100], Train Loss: 27.889694864241804, Test Loss: 32.00821170559177\n",
      "Epoch [30/100], Train Loss: 28.426709397112738, Test Loss: 38.3451915096927\n",
      "Epoch [40/100], Train Loss: 27.133262058945952, Test Loss: 34.15959340875799\n",
      "Epoch [50/100], Train Loss: 27.055687350914127, Test Loss: 30.00604790526551\n",
      "Epoch [60/100], Train Loss: 26.002077265254787, Test Loss: 29.68383315940956\n",
      "Epoch [70/100], Train Loss: 27.61713529492988, Test Loss: 29.067303422209505\n",
      "Epoch [80/100], Train Loss: 25.58355987423756, Test Loss: 31.234637669154576\n",
      "Epoch [90/100], Train Loss: 25.579820301493662, Test Loss: 30.779024916809874\n",
      "Epoch [100/100], Train Loss: 25.57578131253602, Test Loss: 29.902366316163693\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.696601161018748, Test Loss: 47.9176658531288\n",
      "Epoch [20/100], Train Loss: 27.018464660644533, Test Loss: 38.0883914402553\n",
      "Epoch [30/100], Train Loss: 26.43683195895836, Test Loss: 29.479248913851652\n",
      "Epoch [40/100], Train Loss: 27.00889139644435, Test Loss: 31.109020679028003\n",
      "Epoch [50/100], Train Loss: 25.908180061715548, Test Loss: 30.471845651601818\n",
      "Epoch [60/100], Train Loss: 25.946671382716445, Test Loss: 29.427256844260476\n",
      "Epoch [70/100], Train Loss: 25.91012318095223, Test Loss: 34.02442676990063\n",
      "Epoch [80/100], Train Loss: 26.111179339299436, Test Loss: 28.354271306620017\n",
      "Epoch [90/100], Train Loss: 24.941876095631084, Test Loss: 28.068005400818663\n",
      "Epoch [100/100], Train Loss: 26.179352619608895, Test Loss: 27.688695808509728\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.05789737388736, Test Loss: 49.921268463134766\n",
      "Epoch [20/100], Train Loss: 45.20791125688397, Test Loss: 44.42770004272461\n",
      "Epoch [30/100], Train Loss: 42.30027318235304, Test Loss: 41.1817741394043\n",
      "Epoch [40/100], Train Loss: 39.77677272108735, Test Loss: 38.48554992675781\n",
      "Epoch [50/100], Train Loss: 37.419396835077, Test Loss: 40.036624908447266\n",
      "Epoch [60/100], Train Loss: 35.201423282310614, Test Loss: 36.71879196166992\n",
      "Epoch [70/100], Train Loss: 33.0504713965244, Test Loss: 32.68191146850586\n",
      "Epoch [80/100], Train Loss: 31.276306289922996, Test Loss: 32.48834228515625\n",
      "Epoch [90/100], Train Loss: 29.703094175995375, Test Loss: 30.754505157470703\n",
      "Epoch [100/100], Train Loss: 28.257493059752417, Test Loss: 29.366331100463867\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.71895007774478, Test Loss: 50.3355712890625\n",
      "Epoch [20/100], Train Loss: 44.70108377425397, Test Loss: 43.70497131347656\n",
      "Epoch [30/100], Train Loss: 41.94680571008901, Test Loss: 37.895389556884766\n",
      "Epoch [40/100], Train Loss: 39.27742071933434, Test Loss: 37.00484848022461\n",
      "Epoch [50/100], Train Loss: 36.82768693517466, Test Loss: 36.85219192504883\n",
      "Epoch [60/100], Train Loss: 35.07347867371606, Test Loss: 37.60626220703125\n",
      "Epoch [70/100], Train Loss: 32.685137701816245, Test Loss: 33.75471496582031\n",
      "Epoch [80/100], Train Loss: 30.724217943285332, Test Loss: 31.57391357421875\n",
      "Epoch [90/100], Train Loss: 29.234039932000833, Test Loss: 29.780996322631836\n",
      "Epoch [100/100], Train Loss: 27.73384148019259, Test Loss: 29.469955444335938\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.31379662185419, Test Loss: 46.50440216064453\n",
      "Epoch [20/100], Train Loss: 45.74368621325884, Test Loss: 43.442073822021484\n",
      "Epoch [30/100], Train Loss: 43.14029807419073, Test Loss: 39.39051055908203\n",
      "Epoch [40/100], Train Loss: 40.5555088480965, Test Loss: 38.249427795410156\n",
      "Epoch [50/100], Train Loss: 38.260775844386366, Test Loss: 40.674659729003906\n",
      "Epoch [60/100], Train Loss: 35.93325030217405, Test Loss: 37.134849548339844\n",
      "Epoch [70/100], Train Loss: 34.110105533287175, Test Loss: 35.73025131225586\n",
      "Epoch [80/100], Train Loss: 32.25687583548124, Test Loss: 33.713340759277344\n",
      "Epoch [90/100], Train Loss: 30.42571922677462, Test Loss: 30.764366149902344\n",
      "Epoch [100/100], Train Loss: 29.038686683529715, Test Loss: 31.407747268676758\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.928855558301585, Test Loss: 48.06252670288086\n",
      "Epoch [20/100], Train Loss: 45.135077179455365, Test Loss: 44.28030776977539\n",
      "Epoch [30/100], Train Loss: 42.35177705483358, Test Loss: 40.36904525756836\n",
      "Epoch [40/100], Train Loss: 39.804206160248306, Test Loss: 40.8337287902832\n",
      "Epoch [50/100], Train Loss: 37.46688415027056, Test Loss: 34.475502014160156\n",
      "Epoch [60/100], Train Loss: 35.08395346969855, Test Loss: 37.80834197998047\n",
      "Epoch [70/100], Train Loss: 33.31326600371814, Test Loss: 33.84504318237305\n",
      "Epoch [80/100], Train Loss: 31.985573390272798, Test Loss: 33.77254104614258\n",
      "Epoch [90/100], Train Loss: 29.914447653098186, Test Loss: 32.071617126464844\n",
      "Epoch [100/100], Train Loss: 27.741612343709978, Test Loss: 31.62461280822754\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.0019012701316, Test Loss: 49.52545166015625\n",
      "Epoch [20/100], Train Loss: 45.237009354888414, Test Loss: 44.864463806152344\n",
      "Epoch [30/100], Train Loss: 42.435060319744174, Test Loss: 40.35725021362305\n",
      "Epoch [40/100], Train Loss: 39.85462427608302, Test Loss: 38.77348709106445\n",
      "Epoch [50/100], Train Loss: 37.12513649111889, Test Loss: 35.8807373046875\n",
      "Epoch [60/100], Train Loss: 34.989149468844055, Test Loss: 35.382781982421875\n",
      "Epoch [70/100], Train Loss: 32.97363448846536, Test Loss: 32.96297836303711\n",
      "Epoch [80/100], Train Loss: 31.67574070164415, Test Loss: 32.89576721191406\n",
      "Epoch [90/100], Train Loss: 29.45649405307457, Test Loss: 29.808042526245117\n",
      "Epoch [100/100], Train Loss: 28.43738480864978, Test Loss: 31.797697067260742\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.31733812425957, Test Loss: 45.8881950378418\n",
      "Epoch [20/100], Train Loss: 45.82562762401143, Test Loss: 45.81311798095703\n",
      "Epoch [30/100], Train Loss: 43.17337008616963, Test Loss: 40.942562103271484\n",
      "Epoch [40/100], Train Loss: 40.71440836796995, Test Loss: 40.00651550292969\n",
      "Epoch [50/100], Train Loss: 38.342236828413164, Test Loss: 40.58208084106445\n",
      "Epoch [60/100], Train Loss: 36.136380355084526, Test Loss: 35.304115295410156\n",
      "Epoch [70/100], Train Loss: 33.899860444616095, Test Loss: 36.85403060913086\n",
      "Epoch [80/100], Train Loss: 32.06932004709713, Test Loss: 33.83612823486328\n",
      "Epoch [90/100], Train Loss: 29.886568994991116, Test Loss: 31.041189193725586\n",
      "Epoch [100/100], Train Loss: 28.491975590440095, Test Loss: 30.52361488342285\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.10748160940702, Test Loss: 47.82170867919922\n",
      "Epoch [20/100], Train Loss: 45.16037561385358, Test Loss: 44.50457000732422\n",
      "Epoch [30/100], Train Loss: 42.362958163902405, Test Loss: 39.37052917480469\n",
      "Epoch [40/100], Train Loss: 39.81119767486072, Test Loss: 37.7943115234375\n",
      "Epoch [50/100], Train Loss: 37.36070350271756, Test Loss: 36.86195755004883\n",
      "Epoch [60/100], Train Loss: 35.49495348695849, Test Loss: 35.27295684814453\n",
      "Epoch [70/100], Train Loss: 33.922755944924276, Test Loss: 36.28354263305664\n",
      "Epoch [80/100], Train Loss: 31.966288913664272, Test Loss: 34.80489730834961\n",
      "Epoch [90/100], Train Loss: 29.796314664746895, Test Loss: 33.467498779296875\n",
      "Epoch [100/100], Train Loss: 28.42926513171587, Test Loss: 30.734407424926758\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.04378757164127, Test Loss: 49.56842803955078\n",
      "Epoch [20/100], Train Loss: 45.256976193287336, Test Loss: 46.577213287353516\n",
      "Epoch [30/100], Train Loss: 42.49217972051902, Test Loss: 40.87302017211914\n",
      "Epoch [40/100], Train Loss: 40.02778541690014, Test Loss: 40.26863098144531\n",
      "Epoch [50/100], Train Loss: 38.060268333309985, Test Loss: 35.9632568359375\n",
      "Epoch [60/100], Train Loss: 36.39407105993052, Test Loss: 36.91800308227539\n",
      "Epoch [70/100], Train Loss: 33.867527833532115, Test Loss: 37.38290786743164\n",
      "Epoch [80/100], Train Loss: 31.851586094840627, Test Loss: 33.45178985595703\n",
      "Epoch [90/100], Train Loss: 30.445334037405544, Test Loss: 34.790035247802734\n",
      "Epoch [100/100], Train Loss: 28.517872832251378, Test Loss: 34.3475227355957\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 48.26103674466493, Test Loss: 46.58325958251953\n",
      "Epoch [20/100], Train Loss: 45.60788469158235, Test Loss: 45.37239456176758\n",
      "Epoch [30/100], Train Loss: 42.99332993304143, Test Loss: 42.6895866394043\n",
      "Epoch [40/100], Train Loss: 40.490977090304014, Test Loss: 38.76854705810547\n",
      "Epoch [50/100], Train Loss: 37.87925935964115, Test Loss: 38.3194465637207\n",
      "Epoch [60/100], Train Loss: 35.52723188556609, Test Loss: 35.511653900146484\n",
      "Epoch [70/100], Train Loss: 34.00766256363666, Test Loss: 32.89287185668945\n",
      "Epoch [80/100], Train Loss: 32.07925543863265, Test Loss: 33.67434310913086\n",
      "Epoch [90/100], Train Loss: 29.94477006255603, Test Loss: 35.088687896728516\n",
      "Epoch [100/100], Train Loss: 28.808759032702838, Test Loss: 32.327754974365234\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.118508110671748, Test Loss: 36.0135383605957\n",
      "Epoch [20/100], Train Loss: 25.420694582579564, Test Loss: 29.977386474609375\n",
      "Epoch [30/100], Train Loss: 25.36327950524502, Test Loss: 29.420480728149414\n",
      "Epoch [40/100], Train Loss: 25.100322066760455, Test Loss: 28.07068634033203\n",
      "Epoch [50/100], Train Loss: 23.36494233803671, Test Loss: 30.48033332824707\n",
      "Epoch [60/100], Train Loss: 23.90277895067559, Test Loss: 29.012996673583984\n",
      "Epoch [70/100], Train Loss: 23.24371704977067, Test Loss: 28.565330505371094\n",
      "Epoch [80/100], Train Loss: 23.14765770083568, Test Loss: 30.84852409362793\n",
      "Epoch [90/100], Train Loss: 23.291108197071512, Test Loss: 31.143766403198242\n",
      "Epoch [100/100], Train Loss: 22.85140407124504, Test Loss: 29.396968841552734\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.007031500144084, Test Loss: 34.46561813354492\n",
      "Epoch [20/100], Train Loss: 25.715845608320393, Test Loss: 31.537080764770508\n",
      "Epoch [30/100], Train Loss: 24.49000985817831, Test Loss: 29.435861587524414\n",
      "Epoch [40/100], Train Loss: 24.99687835193071, Test Loss: 28.583351135253906\n",
      "Epoch [50/100], Train Loss: 23.96448359880291, Test Loss: 32.1009407043457\n",
      "Epoch [60/100], Train Loss: 23.171281483134287, Test Loss: 30.67000389099121\n",
      "Epoch [70/100], Train Loss: 23.234490210111023, Test Loss: 31.428874969482422\n",
      "Epoch [80/100], Train Loss: 23.973942259491466, Test Loss: 30.04819679260254\n",
      "Epoch [90/100], Train Loss: 23.636461214159358, Test Loss: 27.910175323486328\n",
      "Epoch [100/100], Train Loss: 22.306435363019098, Test Loss: 31.097171783447266\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.448058525460667, Test Loss: 32.982242584228516\n",
      "Epoch [20/100], Train Loss: 25.771079054035123, Test Loss: 32.90047836303711\n",
      "Epoch [30/100], Train Loss: 24.882799955274237, Test Loss: 35.37141799926758\n",
      "Epoch [40/100], Train Loss: 24.040377651277137, Test Loss: 29.322629928588867\n",
      "Epoch [50/100], Train Loss: 24.439459178486807, Test Loss: 28.279800415039062\n",
      "Epoch [60/100], Train Loss: 24.412711009041207, Test Loss: 28.67836570739746\n",
      "Epoch [70/100], Train Loss: 23.567651129550622, Test Loss: 30.69633674621582\n",
      "Epoch [80/100], Train Loss: 23.32819797328261, Test Loss: 28.84739875793457\n",
      "Epoch [90/100], Train Loss: 22.91752697053503, Test Loss: 28.728303909301758\n",
      "Epoch [100/100], Train Loss: 23.572599404757142, Test Loss: 30.13226318359375\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.141404161296908, Test Loss: 35.38097381591797\n",
      "Epoch [20/100], Train Loss: 25.606395352473026, Test Loss: 29.737722396850586\n",
      "Epoch [30/100], Train Loss: 24.473854277563877, Test Loss: 30.421142578125\n",
      "Epoch [40/100], Train Loss: 24.805461552104013, Test Loss: 27.574119567871094\n",
      "Epoch [50/100], Train Loss: 23.994585637577245, Test Loss: 28.48538589477539\n",
      "Epoch [60/100], Train Loss: 23.992622575603548, Test Loss: 28.380252838134766\n",
      "Epoch [70/100], Train Loss: 23.33444781694256, Test Loss: 29.87636375427246\n",
      "Epoch [80/100], Train Loss: 23.679891898983815, Test Loss: 29.856870651245117\n",
      "Epoch [90/100], Train Loss: 23.973777645924052, Test Loss: 28.83074951171875\n",
      "Epoch [100/100], Train Loss: 23.63782938347488, Test Loss: 29.163501739501953\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.475425357505923, Test Loss: 38.09653854370117\n",
      "Epoch [20/100], Train Loss: 27.566713889700466, Test Loss: 30.60621452331543\n",
      "Epoch [30/100], Train Loss: 25.03428102086802, Test Loss: 42.21273422241211\n",
      "Epoch [40/100], Train Loss: 23.007905040803504, Test Loss: 33.43142318725586\n",
      "Epoch [50/100], Train Loss: 23.769789298636013, Test Loss: 30.106359481811523\n",
      "Epoch [60/100], Train Loss: 23.943067288007892, Test Loss: 33.003448486328125\n",
      "Epoch [70/100], Train Loss: 23.74520363729508, Test Loss: 28.620779037475586\n",
      "Epoch [80/100], Train Loss: 24.618221351748605, Test Loss: 28.216968536376953\n",
      "Epoch [90/100], Train Loss: 23.188706201021787, Test Loss: 28.064424514770508\n",
      "Epoch [100/100], Train Loss: 23.108246343643938, Test Loss: 30.70488166809082\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.3446683852399, Test Loss: 35.65631866455078\n",
      "Epoch [20/100], Train Loss: 26.397443352370967, Test Loss: 32.566890716552734\n",
      "Epoch [30/100], Train Loss: 25.30632366743244, Test Loss: 30.590843200683594\n",
      "Epoch [40/100], Train Loss: 24.392206235791814, Test Loss: 33.25239181518555\n",
      "Epoch [50/100], Train Loss: 24.422436004388526, Test Loss: 28.381595611572266\n",
      "Epoch [60/100], Train Loss: 24.10730447612825, Test Loss: 28.99803352355957\n",
      "Epoch [70/100], Train Loss: 24.632172962876616, Test Loss: 29.165847778320312\n",
      "Epoch [80/100], Train Loss: 24.7643545682313, Test Loss: 28.107465744018555\n",
      "Epoch [90/100], Train Loss: 23.44440956741083, Test Loss: 28.950769424438477\n",
      "Epoch [100/100], Train Loss: 23.424025657528738, Test Loss: 28.158077239990234\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.029675843285734, Test Loss: 36.07166290283203\n",
      "Epoch [20/100], Train Loss: 28.78092293661149, Test Loss: 31.44405746459961\n",
      "Epoch [30/100], Train Loss: 25.829129547369284, Test Loss: 31.45684051513672\n",
      "Epoch [40/100], Train Loss: 25.382567984158875, Test Loss: 30.463302612304688\n",
      "Epoch [50/100], Train Loss: 24.78194327432601, Test Loss: 31.951257705688477\n",
      "Epoch [60/100], Train Loss: 25.330850588689085, Test Loss: 31.003341674804688\n",
      "Epoch [70/100], Train Loss: 24.971376062612066, Test Loss: 30.150007247924805\n",
      "Epoch [80/100], Train Loss: 25.030298295568247, Test Loss: 29.8022518157959\n",
      "Epoch [90/100], Train Loss: 24.37256808671795, Test Loss: 28.627073287963867\n",
      "Epoch [100/100], Train Loss: 24.69818429165199, Test Loss: 28.55082893371582\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.411718649942365, Test Loss: 37.23234939575195\n",
      "Epoch [20/100], Train Loss: 27.215098277858047, Test Loss: 32.66704177856445\n",
      "Epoch [30/100], Train Loss: 25.225583986376154, Test Loss: 29.196441650390625\n",
      "Epoch [40/100], Train Loss: 25.932083229940446, Test Loss: 29.161218643188477\n",
      "Epoch [50/100], Train Loss: 25.318805919709753, Test Loss: 29.625709533691406\n",
      "Epoch [60/100], Train Loss: 25.006324918152856, Test Loss: 28.453201293945312\n",
      "Epoch [70/100], Train Loss: 26.235446004398533, Test Loss: 28.15786361694336\n",
      "Epoch [80/100], Train Loss: 24.41483869083592, Test Loss: 28.66816520690918\n",
      "Epoch [90/100], Train Loss: 25.050669510638127, Test Loss: 29.03152847290039\n",
      "Epoch [100/100], Train Loss: 24.281569265146725, Test Loss: 27.897144317626953\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 31.912166720531026, Test Loss: 38.144588470458984\n",
      "Epoch [20/100], Train Loss: 26.857174038496172, Test Loss: 35.1678581237793\n",
      "Epoch [30/100], Train Loss: 26.533534440837922, Test Loss: 32.758544921875\n",
      "Epoch [40/100], Train Loss: 26.052069304419344, Test Loss: 29.393478393554688\n",
      "Epoch [50/100], Train Loss: 25.0372974458288, Test Loss: 29.862428665161133\n",
      "Epoch [60/100], Train Loss: 24.72431239769107, Test Loss: 28.81363868713379\n",
      "Epoch [70/100], Train Loss: 24.581948833778256, Test Loss: 32.61233139038086\n",
      "Epoch [80/100], Train Loss: 24.745395066308195, Test Loss: 29.183277130126953\n",
      "Epoch [90/100], Train Loss: 24.4444453505219, Test Loss: 28.57217025756836\n",
      "Epoch [100/100], Train Loss: 25.39706479682297, Test Loss: 28.669050216674805\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.812275833380028, Test Loss: 33.217681884765625\n",
      "Epoch [20/100], Train Loss: 26.564111002937693, Test Loss: 27.93177604675293\n",
      "Epoch [30/100], Train Loss: 27.745856776002977, Test Loss: 30.779809951782227\n",
      "Epoch [40/100], Train Loss: 25.017227116569146, Test Loss: 30.291988372802734\n",
      "Epoch [50/100], Train Loss: 26.148662448320234, Test Loss: 28.955101013183594\n",
      "Epoch [60/100], Train Loss: 25.25022389146148, Test Loss: 28.952728271484375\n",
      "Epoch [70/100], Train Loss: 24.577354281065894, Test Loss: 29.248754501342773\n",
      "Epoch [80/100], Train Loss: 24.11932584418625, Test Loss: 28.534255981445312\n",
      "Epoch [90/100], Train Loss: 23.87101623034868, Test Loss: 30.235553741455078\n",
      "Epoch [100/100], Train Loss: 24.859378320662703, Test Loss: 27.460668563842773\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.10735424229356, Test Loss: 39.283416748046875\n",
      "Epoch [20/100], Train Loss: 26.410730268134447, Test Loss: 30.696260452270508\n",
      "Epoch [30/100], Train Loss: 26.632952274259974, Test Loss: 30.45598793029785\n",
      "Epoch [40/100], Train Loss: 25.27270600991171, Test Loss: 28.696897506713867\n",
      "Epoch [50/100], Train Loss: 25.25585928119597, Test Loss: 29.878604888916016\n",
      "Epoch [60/100], Train Loss: 25.039331035926693, Test Loss: 32.97715759277344\n",
      "Epoch [70/100], Train Loss: 24.96226513972048, Test Loss: 28.586977005004883\n",
      "Epoch [80/100], Train Loss: 23.38743960271116, Test Loss: 31.63748550415039\n",
      "Epoch [90/100], Train Loss: 26.28473805286845, Test Loss: 28.766620635986328\n",
      "Epoch [100/100], Train Loss: 24.47995511664719, Test Loss: 29.214210510253906\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.15472801708784, Test Loss: 34.366188049316406\n",
      "Epoch [20/100], Train Loss: 26.85949918403, Test Loss: 37.372440338134766\n",
      "Epoch [30/100], Train Loss: 25.79773436999712, Test Loss: 30.742910385131836\n",
      "Epoch [40/100], Train Loss: 26.028733681850746, Test Loss: 34.85388946533203\n",
      "Epoch [50/100], Train Loss: 24.76792004069344, Test Loss: 27.21420669555664\n",
      "Epoch [60/100], Train Loss: 25.61051953425173, Test Loss: 27.810871124267578\n",
      "Epoch [70/100], Train Loss: 24.796500665633406, Test Loss: 29.000890731811523\n",
      "Epoch [80/100], Train Loss: 26.12435148895764, Test Loss: 32.97407913208008\n",
      "Epoch [90/100], Train Loss: 24.667626865574572, Test Loss: 27.598709106445312\n",
      "Epoch [100/100], Train Loss: 23.882567346291463, Test Loss: 28.16407585144043\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.75482117699795, Test Loss: 32.48634719848633\n",
      "Epoch [20/100], Train Loss: 25.83205936619493, Test Loss: 34.14888000488281\n",
      "Epoch [30/100], Train Loss: 26.078728203695327, Test Loss: 31.326635360717773\n",
      "Epoch [40/100], Train Loss: 25.324947094526447, Test Loss: 28.524070739746094\n",
      "Epoch [50/100], Train Loss: 25.769849183129484, Test Loss: 34.2698974609375\n",
      "Epoch [60/100], Train Loss: 25.372109172383293, Test Loss: 28.19740867614746\n",
      "Epoch [70/100], Train Loss: 24.745568309846472, Test Loss: 28.585405349731445\n",
      "Epoch [80/100], Train Loss: 26.082931368468238, Test Loss: 28.708099365234375\n",
      "Epoch [90/100], Train Loss: 25.60717976054207, Test Loss: 29.244680404663086\n",
      "Epoch [100/100], Train Loss: 24.706183849397252, Test Loss: 31.646780014038086\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.659481442560914, Test Loss: 32.36321258544922\n",
      "Epoch [20/100], Train Loss: 26.59010363094142, Test Loss: 38.91529083251953\n",
      "Epoch [30/100], Train Loss: 26.83648620980685, Test Loss: 33.1573486328125\n",
      "Epoch [40/100], Train Loss: 26.60379740605589, Test Loss: 31.786081314086914\n",
      "Epoch [50/100], Train Loss: 26.433229577736775, Test Loss: 28.38284683227539\n",
      "Epoch [60/100], Train Loss: 24.735607622490555, Test Loss: 28.338619232177734\n",
      "Epoch [70/100], Train Loss: 24.825771281758293, Test Loss: 29.3286075592041\n",
      "Epoch [80/100], Train Loss: 24.693241206935195, Test Loss: 28.6716251373291\n",
      "Epoch [90/100], Train Loss: 24.327276911501023, Test Loss: 30.47213363647461\n",
      "Epoch [100/100], Train Loss: 24.124537439815334, Test Loss: 28.344539642333984\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.824328331869157, Test Loss: 33.317623138427734\n",
      "Epoch [20/100], Train Loss: 26.203295179273262, Test Loss: 37.53559494018555\n",
      "Epoch [30/100], Train Loss: 28.070156747786726, Test Loss: 31.306440353393555\n",
      "Epoch [40/100], Train Loss: 26.346400332841718, Test Loss: 30.602149963378906\n",
      "Epoch [50/100], Train Loss: 26.76216945335513, Test Loss: 27.54697036743164\n",
      "Epoch [60/100], Train Loss: 25.70810793266922, Test Loss: 30.347671508789062\n",
      "Epoch [70/100], Train Loss: 24.884458960861455, Test Loss: 29.68055534362793\n",
      "Epoch [80/100], Train Loss: 24.347333795516217, Test Loss: 28.006423950195312\n",
      "Epoch [90/100], Train Loss: 24.197846090598183, Test Loss: 27.878690719604492\n",
      "Epoch [100/100], Train Loss: 25.04094506560779, Test Loss: 28.107872009277344\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.991254606403288, Test Loss: 37.07235336303711\n",
      "Epoch [20/100], Train Loss: 28.823051089927798, Test Loss: 35.964515686035156\n",
      "Epoch [30/100], Train Loss: 27.35389066602363, Test Loss: 33.54314041137695\n",
      "Epoch [40/100], Train Loss: 25.868301041399846, Test Loss: 34.98185729980469\n",
      "Epoch [50/100], Train Loss: 26.366985452370564, Test Loss: 30.462116241455078\n",
      "Epoch [60/100], Train Loss: 25.027898406982423, Test Loss: 31.461111068725586\n",
      "Epoch [70/100], Train Loss: 25.951143489900183, Test Loss: 32.18712615966797\n",
      "Epoch [80/100], Train Loss: 26.06759462200227, Test Loss: 30.144268035888672\n",
      "Epoch [90/100], Train Loss: 25.780425662681704, Test Loss: 32.39714050292969\n",
      "Epoch [100/100], Train Loss: 25.150039228845817, Test Loss: 33.57767105102539\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.635692358798668, Test Loss: 49.60879135131836\n",
      "Epoch [20/100], Train Loss: 29.10124234058818, Test Loss: 31.403560638427734\n",
      "Epoch [30/100], Train Loss: 29.209024360531668, Test Loss: 32.13625717163086\n",
      "Epoch [40/100], Train Loss: 26.958249958225938, Test Loss: 31.363813400268555\n",
      "Epoch [50/100], Train Loss: 26.270977182857326, Test Loss: 33.99213790893555\n",
      "Epoch [60/100], Train Loss: 25.34102536185843, Test Loss: 31.001413345336914\n",
      "Epoch [70/100], Train Loss: 26.62606713341885, Test Loss: 35.662452697753906\n",
      "Epoch [80/100], Train Loss: 25.34478699731045, Test Loss: 28.85805320739746\n",
      "Epoch [90/100], Train Loss: 25.071729234789238, Test Loss: 30.534610748291016\n",
      "Epoch [100/100], Train Loss: 25.020528199242765, Test Loss: 32.197723388671875\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.41077513772933, Test Loss: 35.778011322021484\n",
      "Epoch [20/100], Train Loss: 30.429829331695057, Test Loss: 34.042537689208984\n",
      "Epoch [30/100], Train Loss: 27.04064589328453, Test Loss: 34.69786834716797\n",
      "Epoch [40/100], Train Loss: 26.885378828205045, Test Loss: 35.695491790771484\n",
      "Epoch [50/100], Train Loss: 27.087888561311317, Test Loss: 32.33599090576172\n",
      "Epoch [60/100], Train Loss: 26.035998691496303, Test Loss: 33.24269104003906\n",
      "Epoch [70/100], Train Loss: 27.164701761964892, Test Loss: 35.201011657714844\n",
      "Epoch [80/100], Train Loss: 27.398553216652793, Test Loss: 30.008804321289062\n",
      "Epoch [90/100], Train Loss: 26.31055618661349, Test Loss: 31.217430114746094\n",
      "Epoch [100/100], Train Loss: 26.10228410314341, Test Loss: 32.85818099975586\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 39.235034454845994, Test Loss: 32.69713604914678\n",
      "Epoch [20/100], Train Loss: 30.641254262455174, Test Loss: 31.500245775495255\n",
      "Epoch [30/100], Train Loss: 26.667188513083538, Test Loss: 28.30166333681577\n",
      "Epoch [40/100], Train Loss: 26.277049868224097, Test Loss: 28.715251303338384\n",
      "Epoch [50/100], Train Loss: 24.839120477144835, Test Loss: 27.46043978108988\n",
      "Epoch [60/100], Train Loss: 25.301867087942654, Test Loss: 28.03006870715649\n",
      "Epoch [70/100], Train Loss: 24.829776526279137, Test Loss: 27.860604471974558\n",
      "Epoch [80/100], Train Loss: 25.001703030945826, Test Loss: 28.202975657079126\n",
      "Epoch [90/100], Train Loss: 22.60722272904193, Test Loss: 28.12177276611328\n",
      "Epoch [100/100], Train Loss: 22.614974575355404, Test Loss: 28.841615453943028\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 38.878940056972816, Test Loss: 38.44735370982777\n",
      "Epoch [20/100], Train Loss: 30.716596090598184, Test Loss: 31.965875254048928\n",
      "Epoch [30/100], Train Loss: 26.33085614188773, Test Loss: 28.871057262668362\n",
      "Epoch [40/100], Train Loss: 26.276903721543608, Test Loss: 27.732670300966735\n",
      "Epoch [50/100], Train Loss: 24.6601381645828, Test Loss: 27.69396957793793\n",
      "Epoch [60/100], Train Loss: 24.859137200527503, Test Loss: 27.412362383557603\n",
      "Epoch [70/100], Train Loss: 25.937346273953796, Test Loss: 27.794998528121354\n",
      "Epoch [80/100], Train Loss: 23.545179836085584, Test Loss: 29.548150867610783\n",
      "Epoch [90/100], Train Loss: 22.87203050206919, Test Loss: 28.004948628413214\n",
      "Epoch [100/100], Train Loss: 23.40283271914623, Test Loss: 27.469757773659445\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 39.65836696937436, Test Loss: 36.15900849676751\n",
      "Epoch [20/100], Train Loss: 31.224093177670337, Test Loss: 31.76134429039893\n",
      "Epoch [30/100], Train Loss: 27.417777439805327, Test Loss: 29.51011523952732\n",
      "Epoch [40/100], Train Loss: 26.286333277968108, Test Loss: 28.759993466463957\n",
      "Epoch [50/100], Train Loss: 25.711230994052574, Test Loss: 28.174635354574626\n",
      "Epoch [60/100], Train Loss: 24.500109250428245, Test Loss: 27.86591051770495\n",
      "Epoch [70/100], Train Loss: 25.249233364668047, Test Loss: 27.935778580702745\n",
      "Epoch [80/100], Train Loss: 23.961176737800972, Test Loss: 27.95545954518504\n",
      "Epoch [90/100], Train Loss: 23.9828034572914, Test Loss: 28.589916675121753\n",
      "Epoch [100/100], Train Loss: 25.364261233220336, Test Loss: 27.708840952291116\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 39.382022732593974, Test Loss: 39.05999845034116\n",
      "Epoch [20/100], Train Loss: 30.788141413204006, Test Loss: 30.638409800343698\n",
      "Epoch [30/100], Train Loss: 27.673339712424358, Test Loss: 29.923201920150163\n",
      "Epoch [40/100], Train Loss: 26.532115561063172, Test Loss: 27.50971643026773\n",
      "Epoch [50/100], Train Loss: 26.25718234327973, Test Loss: 28.678489338267934\n",
      "Epoch [60/100], Train Loss: 24.572310788514184, Test Loss: 27.940211308466925\n",
      "Epoch [70/100], Train Loss: 25.698926800587138, Test Loss: 27.85055024283273\n",
      "Epoch [80/100], Train Loss: 25.178651053006533, Test Loss: 30.7683673710018\n",
      "Epoch [90/100], Train Loss: 24.400758874611775, Test Loss: 27.472143495237674\n",
      "Epoch [100/100], Train Loss: 23.648646095150806, Test Loss: 27.767461851045685\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 38.24755054536413, Test Loss: 35.6083805034687\n",
      "Epoch [20/100], Train Loss: 30.185398201864274, Test Loss: 32.147097352263216\n",
      "Epoch [30/100], Train Loss: 29.12058850898117, Test Loss: 29.319933581661868\n",
      "Epoch [40/100], Train Loss: 24.99110017370005, Test Loss: 28.476704213526343\n",
      "Epoch [50/100], Train Loss: 24.932816677406187, Test Loss: 27.954164182984982\n",
      "Epoch [60/100], Train Loss: 24.870817797301246, Test Loss: 28.191439789611024\n",
      "Epoch [70/100], Train Loss: 25.515467534299756, Test Loss: 28.60332335434951\n",
      "Epoch [80/100], Train Loss: 24.393407039955015, Test Loss: 28.385021556507457\n",
      "Epoch [90/100], Train Loss: 24.35122268864366, Test Loss: 28.380318257715796\n",
      "Epoch [100/100], Train Loss: 23.698016513761928, Test Loss: 28.63942168594955\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 40.37377315583776, Test Loss: 38.66498025671228\n",
      "Epoch [20/100], Train Loss: 31.275639368276128, Test Loss: 32.196544449050705\n",
      "Epoch [30/100], Train Loss: 27.163928622886782, Test Loss: 29.84968024414855\n",
      "Epoch [40/100], Train Loss: 25.724490112554832, Test Loss: 28.12802639255276\n",
      "Epoch [50/100], Train Loss: 25.161015213512982, Test Loss: 28.086775123298942\n",
      "Epoch [60/100], Train Loss: 25.059383667492476, Test Loss: 30.835777480880935\n",
      "Epoch [70/100], Train Loss: 24.53339358470479, Test Loss: 27.767901854081586\n",
      "Epoch [80/100], Train Loss: 23.57684485013368, Test Loss: 28.388030114111963\n",
      "Epoch [90/100], Train Loss: 24.548024074366836, Test Loss: 27.87839565029392\n",
      "Epoch [100/100], Train Loss: 22.933527124123493, Test Loss: 28.024735017256305\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 40.0453896569424, Test Loss: 33.102630986795795\n",
      "Epoch [20/100], Train Loss: 31.656942961645907, Test Loss: 32.1101232999331\n",
      "Epoch [30/100], Train Loss: 27.177871497732696, Test Loss: 28.780360878287972\n",
      "Epoch [40/100], Train Loss: 27.840420938710697, Test Loss: 28.91652654672598\n",
      "Epoch [50/100], Train Loss: 26.14331732577965, Test Loss: 28.987742139147475\n",
      "Epoch [60/100], Train Loss: 25.91353150664783, Test Loss: 30.82664474883637\n",
      "Epoch [70/100], Train Loss: 25.04564040137119, Test Loss: 29.380932845078505\n",
      "Epoch [80/100], Train Loss: 24.593573360755794, Test Loss: 27.901245488748923\n",
      "Epoch [90/100], Train Loss: 24.36379973614802, Test Loss: 29.30673292085722\n",
      "Epoch [100/100], Train Loss: 23.86041949537934, Test Loss: 28.105675462004427\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 39.40656943399398, Test Loss: 38.64133854655476\n",
      "Epoch [20/100], Train Loss: 30.776715600685996, Test Loss: 32.96775483465814\n",
      "Epoch [30/100], Train Loss: 27.142417851432427, Test Loss: 30.13954328561758\n",
      "Epoch [40/100], Train Loss: 25.835221193657546, Test Loss: 28.500470966487736\n",
      "Epoch [50/100], Train Loss: 25.663115854732325, Test Loss: 27.735569223180995\n",
      "Epoch [60/100], Train Loss: 23.79947898426994, Test Loss: 27.51028635594752\n",
      "Epoch [70/100], Train Loss: 25.44517165637407, Test Loss: 28.28849038210782\n",
      "Epoch [80/100], Train Loss: 24.55893635358967, Test Loss: 28.634354653296533\n",
      "Epoch [90/100], Train Loss: 24.23660719824619, Test Loss: 27.90115822135628\n",
      "Epoch [100/100], Train Loss: 25.102274591414655, Test Loss: 28.166545917461445\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 40.36812043737193, Test Loss: 40.622781530603184\n",
      "Epoch [20/100], Train Loss: 31.578531071397126, Test Loss: 33.66808603955554\n",
      "Epoch [30/100], Train Loss: 28.621406467625352, Test Loss: 30.808628874939757\n",
      "Epoch [40/100], Train Loss: 26.551986700589538, Test Loss: 28.976255367328594\n",
      "Epoch [50/100], Train Loss: 25.420928085827438, Test Loss: 29.2306003322849\n",
      "Epoch [60/100], Train Loss: 25.290223018458633, Test Loss: 28.64427494693112\n",
      "Epoch [70/100], Train Loss: 24.381873352801215, Test Loss: 28.709361906175488\n",
      "Epoch [80/100], Train Loss: 25.281625785202277, Test Loss: 27.782324332695502\n",
      "Epoch [90/100], Train Loss: 26.64562331027672, Test Loss: 29.47566086905343\n",
      "Epoch [100/100], Train Loss: 24.203640578223055, Test Loss: 28.080433560656264\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.288461391261368, Test Loss: 30.222224000212435\n",
      "Epoch [20/100], Train Loss: 25.939587471133372, Test Loss: 29.929255770398424\n",
      "Epoch [30/100], Train Loss: 25.540510046286663, Test Loss: 27.661590774337967\n",
      "Epoch [40/100], Train Loss: 25.44412599782475, Test Loss: 29.15603087784408\n",
      "Epoch [50/100], Train Loss: 24.740188373503138, Test Loss: 29.68959501192167\n",
      "Epoch [60/100], Train Loss: 24.796425616154906, Test Loss: 28.364716814709947\n",
      "Epoch [70/100], Train Loss: 25.84941801477651, Test Loss: 28.88798329737279\n",
      "Epoch [80/100], Train Loss: 25.84391508884117, Test Loss: 28.579818973293552\n",
      "Epoch [90/100], Train Loss: 25.941494425789255, Test Loss: 28.04427181590687\n",
      "Epoch [100/100], Train Loss: 24.452466433165505, Test Loss: 27.562728287337663\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.77263367449651, Test Loss: 35.24752834865025\n",
      "Epoch [20/100], Train Loss: 26.586923718061602, Test Loss: 29.50667938628754\n",
      "Epoch [30/100], Train Loss: 25.3159634824659, Test Loss: 29.405422012527268\n",
      "Epoch [40/100], Train Loss: 26.06871319755179, Test Loss: 29.09075135070008\n",
      "Epoch [50/100], Train Loss: 25.49056517804255, Test Loss: 26.483847927737546\n",
      "Epoch [60/100], Train Loss: 25.118610907382653, Test Loss: 27.943005425589426\n",
      "Epoch [70/100], Train Loss: 25.738000438252435, Test Loss: 28.068875671981218\n",
      "Epoch [80/100], Train Loss: 24.550692949138703, Test Loss: 31.311411721365793\n",
      "Epoch [90/100], Train Loss: 24.264506318139247, Test Loss: 31.525684579626308\n",
      "Epoch [100/100], Train Loss: 24.13259057216957, Test Loss: 26.959172063059622\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.695254216428662, Test Loss: 28.63561335476962\n",
      "Epoch [20/100], Train Loss: 28.069950310128633, Test Loss: 29.21706454784839\n",
      "Epoch [30/100], Train Loss: 26.073562909736008, Test Loss: 28.57260780829888\n",
      "Epoch [40/100], Train Loss: 24.913614454425748, Test Loss: 30.692344021487546\n",
      "Epoch [50/100], Train Loss: 25.425192617197506, Test Loss: 28.083808279656743\n",
      "Epoch [60/100], Train Loss: 26.605735641229348, Test Loss: 27.041859540072355\n",
      "Epoch [70/100], Train Loss: 25.118074310803024, Test Loss: 29.792864366011187\n",
      "Epoch [80/100], Train Loss: 25.283725988669474, Test Loss: 28.150765332308683\n",
      "Epoch [90/100], Train Loss: 24.847914423708055, Test Loss: 27.837552949979706\n",
      "Epoch [100/100], Train Loss: 24.59349335217085, Test Loss: 26.676183551936955\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.85028344451404, Test Loss: 30.235856539243226\n",
      "Epoch [20/100], Train Loss: 27.94437854954454, Test Loss: 29.86024388399991\n",
      "Epoch [30/100], Train Loss: 26.28665792121262, Test Loss: 29.43870804526589\n",
      "Epoch [40/100], Train Loss: 24.793789403946672, Test Loss: 28.367622449800567\n",
      "Epoch [50/100], Train Loss: 25.806790773985817, Test Loss: 30.456255132501777\n",
      "Epoch [60/100], Train Loss: 25.57901481253202, Test Loss: 28.000029848767564\n",
      "Epoch [70/100], Train Loss: 25.97309768551686, Test Loss: 29.286699468439277\n",
      "Epoch [80/100], Train Loss: 25.238954012511208, Test Loss: 27.541215698440354\n",
      "Epoch [90/100], Train Loss: 25.780646258494894, Test Loss: 29.668166767467152\n",
      "Epoch [100/100], Train Loss: 25.724173073690444, Test Loss: 28.98357980901545\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.34142128991299, Test Loss: 31.200561077563794\n",
      "Epoch [20/100], Train Loss: 26.776985306036277, Test Loss: 29.517820581213222\n",
      "Epoch [30/100], Train Loss: 25.904310933097463, Test Loss: 29.7035486791041\n",
      "Epoch [40/100], Train Loss: 27.13475561923668, Test Loss: 28.68095762079412\n",
      "Epoch [50/100], Train Loss: 26.902900408135086, Test Loss: 29.07292366027832\n",
      "Epoch [60/100], Train Loss: 25.812131662837796, Test Loss: 30.13938891423213\n",
      "Epoch [70/100], Train Loss: 26.97504503218854, Test Loss: 29.042958172884855\n",
      "Epoch [80/100], Train Loss: 25.263421830974643, Test Loss: 29.561642337155032\n",
      "Epoch [90/100], Train Loss: 26.037716943709576, Test Loss: 29.041831202321237\n",
      "Epoch [100/100], Train Loss: 24.743786208356013, Test Loss: 28.093279033512264\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.500384796642866, Test Loss: 31.778203270652078\n",
      "Epoch [20/100], Train Loss: 27.740218497104333, Test Loss: 28.91820107497178\n",
      "Epoch [30/100], Train Loss: 26.108154296875, Test Loss: 30.408681374091607\n",
      "Epoch [40/100], Train Loss: 25.830696987714923, Test Loss: 28.441998543677393\n",
      "Epoch [50/100], Train Loss: 26.07574866873319, Test Loss: 29.30130131213696\n",
      "Epoch [60/100], Train Loss: 26.050757098588786, Test Loss: 29.985183641508026\n",
      "Epoch [70/100], Train Loss: 25.957975762789367, Test Loss: 28.61069325038365\n",
      "Epoch [80/100], Train Loss: 24.684715583676198, Test Loss: 28.65897525440563\n",
      "Epoch [90/100], Train Loss: 25.348384075477476, Test Loss: 27.598945592905018\n",
      "Epoch [100/100], Train Loss: 25.577575602296925, Test Loss: 29.388501600785688\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.24286741663198, Test Loss: 75.97739628383091\n",
      "Epoch [20/100], Train Loss: 26.285115870491403, Test Loss: 30.89337718022334\n",
      "Epoch [30/100], Train Loss: 26.394589883773055, Test Loss: 30.773555235429242\n",
      "Epoch [40/100], Train Loss: 26.31682764272221, Test Loss: 31.379457052651937\n",
      "Epoch [50/100], Train Loss: 25.550151237112576, Test Loss: 29.715001440667486\n",
      "Epoch [60/100], Train Loss: 26.335469780593623, Test Loss: 30.357898291055257\n",
      "Epoch [70/100], Train Loss: 26.83627939067903, Test Loss: 27.912341055931982\n",
      "Epoch [80/100], Train Loss: 24.81443461433786, Test Loss: 29.014572812365248\n",
      "Epoch [90/100], Train Loss: 25.560609861280096, Test Loss: 28.72127745987533\n",
      "Epoch [100/100], Train Loss: 25.115372141853708, Test Loss: 28.64538690641329\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.968188363997662, Test Loss: 32.33482412858443\n",
      "Epoch [20/100], Train Loss: 26.942750236636304, Test Loss: 29.99625639481978\n",
      "Epoch [30/100], Train Loss: 27.590741592157084, Test Loss: 30.49000105919776\n",
      "Epoch [40/100], Train Loss: 27.02370855612833, Test Loss: 31.029174086335416\n",
      "Epoch [50/100], Train Loss: 27.26096151383197, Test Loss: 30.609148223678787\n",
      "Epoch [60/100], Train Loss: 28.02711124107486, Test Loss: 30.626276189630683\n",
      "Epoch [70/100], Train Loss: 25.40855321415135, Test Loss: 29.610196249825613\n",
      "Epoch [80/100], Train Loss: 25.56996037217437, Test Loss: 29.343588048761543\n",
      "Epoch [90/100], Train Loss: 25.507836663918418, Test Loss: 28.328457101598964\n",
      "Epoch [100/100], Train Loss: 26.172784423828126, Test Loss: 28.9908075704203\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.084130696781347, Test Loss: 32.77453796584885\n",
      "Epoch [20/100], Train Loss: 27.604266776413215, Test Loss: 30.039533788507637\n",
      "Epoch [30/100], Train Loss: 26.892131117523693, Test Loss: 29.46942326929662\n",
      "Epoch [40/100], Train Loss: 26.206197200837682, Test Loss: 30.64898914485783\n",
      "Epoch [50/100], Train Loss: 27.04922459711794, Test Loss: 28.320503804590796\n",
      "Epoch [60/100], Train Loss: 27.044207582317416, Test Loss: 30.103400366646902\n",
      "Epoch [70/100], Train Loss: 26.708807191692415, Test Loss: 32.598696275190875\n",
      "Epoch [80/100], Train Loss: 26.280635208380026, Test Loss: 28.284903538691534\n",
      "Epoch [90/100], Train Loss: 26.5703601149262, Test Loss: 28.888293897950803\n",
      "Epoch [100/100], Train Loss: 25.111677319886255, Test Loss: 28.388093353865983\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.698523924780673, Test Loss: 52.587041086964796\n",
      "Epoch [20/100], Train Loss: 26.698622844258292, Test Loss: 30.763018868186258\n",
      "Epoch [30/100], Train Loss: 26.684484150370615, Test Loss: 33.65872031372863\n",
      "Epoch [40/100], Train Loss: 25.769988256986025, Test Loss: 28.923358892465565\n",
      "Epoch [50/100], Train Loss: 25.64068152005555, Test Loss: 27.848645396046823\n",
      "Epoch [60/100], Train Loss: 24.37917969500432, Test Loss: 27.993318632051544\n",
      "Epoch [70/100], Train Loss: 25.77571201637143, Test Loss: 27.175201292161816\n",
      "Epoch [80/100], Train Loss: 26.124028371592036, Test Loss: 30.491019781533772\n",
      "Epoch [90/100], Train Loss: 24.566188374503714, Test Loss: 28.036242868993188\n",
      "Epoch [100/100], Train Loss: 25.041826742203508, Test Loss: 27.727021626063756\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 31.003604826379995, Test Loss: 34.37130217118697\n",
      "Epoch [20/100], Train Loss: 27.046203569506037, Test Loss: 33.38661966695414\n",
      "Epoch [30/100], Train Loss: 26.27845911432485, Test Loss: 28.98163111798175\n",
      "Epoch [40/100], Train Loss: 27.41884519233078, Test Loss: 30.959878971050312\n",
      "Epoch [50/100], Train Loss: 25.920076401507266, Test Loss: 28.048708184972988\n",
      "Epoch [60/100], Train Loss: 26.93692600062636, Test Loss: 27.479584904460165\n",
      "Epoch [70/100], Train Loss: 27.148990355945024, Test Loss: 27.81169878972041\n",
      "Epoch [80/100], Train Loss: 25.85462789066502, Test Loss: 27.87191663469587\n",
      "Epoch [90/100], Train Loss: 24.812351151763416, Test Loss: 28.25827915637524\n",
      "Epoch [100/100], Train Loss: 25.133355950527505, Test Loss: 28.188484736851283\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.9104021916624, Test Loss: 36.02966598411659\n",
      "Epoch [20/100], Train Loss: 26.712671855238618, Test Loss: 33.308516935868695\n",
      "Epoch [30/100], Train Loss: 26.441401634841668, Test Loss: 28.38584785956841\n",
      "Epoch [40/100], Train Loss: 26.72807722248015, Test Loss: 27.55541147504534\n",
      "Epoch [50/100], Train Loss: 26.406935057092884, Test Loss: 28.420167947744396\n",
      "Epoch [60/100], Train Loss: 25.312859507076077, Test Loss: 27.29143491967932\n",
      "Epoch [70/100], Train Loss: 26.930143506409692, Test Loss: 28.77227424027084\n",
      "Epoch [80/100], Train Loss: 26.336442847330062, Test Loss: 28.03927775791713\n",
      "Epoch [90/100], Train Loss: 24.578724908047036, Test Loss: 32.02755999874759\n",
      "Epoch [100/100], Train Loss: 26.350026290143123, Test Loss: 27.9959872357257\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.59525122720687, Test Loss: 31.572206967836852\n",
      "Epoch [20/100], Train Loss: 27.106155045306096, Test Loss: 32.571581605192904\n",
      "Epoch [30/100], Train Loss: 26.531081227787205, Test Loss: 28.73148876041561\n",
      "Epoch [40/100], Train Loss: 26.541311695536628, Test Loss: 29.1374011597076\n",
      "Epoch [50/100], Train Loss: 27.144645928554848, Test Loss: 29.398264426689643\n",
      "Epoch [60/100], Train Loss: 26.562370988189198, Test Loss: 28.86413995321695\n",
      "Epoch [70/100], Train Loss: 26.866840137419153, Test Loss: 29.95985288743849\n",
      "Epoch [80/100], Train Loss: 27.060214708672195, Test Loss: 28.141595766141815\n",
      "Epoch [90/100], Train Loss: 25.169415852280913, Test Loss: 27.27390851602926\n",
      "Epoch [100/100], Train Loss: 26.174982089683656, Test Loss: 28.475038255964005\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.627968397296844, Test Loss: 31.68839201989112\n",
      "Epoch [20/100], Train Loss: 27.485700601046204, Test Loss: 31.13173160305271\n",
      "Epoch [30/100], Train Loss: 27.92602976189285, Test Loss: 31.11755452837263\n",
      "Epoch [40/100], Train Loss: 27.763503159069625, Test Loss: 29.286388694465934\n",
      "Epoch [50/100], Train Loss: 28.291277588390912, Test Loss: 28.906015222722832\n",
      "Epoch [60/100], Train Loss: 26.829168544831823, Test Loss: 29.0785507350773\n",
      "Epoch [70/100], Train Loss: 26.391490786192847, Test Loss: 31.26303464716131\n",
      "Epoch [80/100], Train Loss: 26.156677955877587, Test Loss: 32.45079897595691\n",
      "Epoch [90/100], Train Loss: 25.028396618952517, Test Loss: 30.176977380529628\n",
      "Epoch [100/100], Train Loss: 25.582696714557585, Test Loss: 27.65388159318404\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.05528854620261, Test Loss: 32.081324242926264\n",
      "Epoch [20/100], Train Loss: 27.924804587442367, Test Loss: 31.10269395407144\n",
      "Epoch [30/100], Train Loss: 27.62324435749992, Test Loss: 30.120911511507902\n",
      "Epoch [40/100], Train Loss: 27.217404793911292, Test Loss: 29.92691149030413\n",
      "Epoch [50/100], Train Loss: 27.047594420636287, Test Loss: 30.28592671976461\n",
      "Epoch [60/100], Train Loss: 27.035847085421203, Test Loss: 29.865564024293576\n",
      "Epoch [70/100], Train Loss: 25.782178259677575, Test Loss: 28.87238036812126\n",
      "Epoch [80/100], Train Loss: 26.153411321170996, Test Loss: 28.12534679066051\n",
      "Epoch [90/100], Train Loss: 25.689701036547053, Test Loss: 29.47511219668698\n",
      "Epoch [100/100], Train Loss: 26.71583284471856, Test Loss: 30.013619038965796\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.76301338946233, Test Loss: 34.75321286684507\n",
      "Epoch [20/100], Train Loss: 28.27042046531302, Test Loss: 43.41084725516183\n",
      "Epoch [30/100], Train Loss: 28.06621115012247, Test Loss: 31.16427718818962\n",
      "Epoch [40/100], Train Loss: 26.366473476222303, Test Loss: 32.31783121282404\n",
      "Epoch [50/100], Train Loss: 26.35149688720703, Test Loss: 30.6266467602222\n",
      "Epoch [60/100], Train Loss: 26.59273379591645, Test Loss: 33.051552958302686\n",
      "Epoch [70/100], Train Loss: 27.344525659279746, Test Loss: 29.340275157581676\n",
      "Epoch [80/100], Train Loss: 26.638100420842406, Test Loss: 30.37862530002346\n",
      "Epoch [90/100], Train Loss: 26.62464400744829, Test Loss: 29.492946550443573\n",
      "Epoch [100/100], Train Loss: 27.339013796947043, Test Loss: 33.75001875146643\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.57373306399486, Test Loss: 38.736470953210606\n",
      "Epoch [20/100], Train Loss: 27.997597578705335, Test Loss: 35.011266782686306\n",
      "Epoch [30/100], Train Loss: 27.257682894096998, Test Loss: 30.92586376140644\n",
      "Epoch [40/100], Train Loss: 28.248546650370614, Test Loss: 29.015795893483347\n",
      "Epoch [50/100], Train Loss: 27.49293655645652, Test Loss: 29.249725341796875\n",
      "Epoch [60/100], Train Loss: 26.86758925015809, Test Loss: 27.962882401107194\n",
      "Epoch [70/100], Train Loss: 26.351981954105565, Test Loss: 28.754259059955547\n",
      "Epoch [80/100], Train Loss: 27.596859941326205, Test Loss: 29.18699162966245\n",
      "Epoch [90/100], Train Loss: 26.25575047477347, Test Loss: 29.259145365132913\n",
      "Epoch [100/100], Train Loss: 26.141120410356365, Test Loss: 29.09751134104543\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 31.343626134903705, Test Loss: 32.71834544392375\n",
      "Epoch [20/100], Train Loss: 29.311287958113873, Test Loss: 32.63128751284116\n",
      "Epoch [30/100], Train Loss: 28.390441194127817, Test Loss: 33.24045560267064\n",
      "Epoch [40/100], Train Loss: 27.733337846349496, Test Loss: 32.13198996209479\n",
      "Epoch [50/100], Train Loss: 26.7574336317719, Test Loss: 31.143771753682717\n",
      "Epoch [60/100], Train Loss: 26.49302198066086, Test Loss: 29.730143361277396\n",
      "Epoch [70/100], Train Loss: 26.772652479077948, Test Loss: 32.024307399601135\n",
      "Epoch [80/100], Train Loss: 26.99708701587114, Test Loss: 29.684578660246615\n",
      "Epoch [90/100], Train Loss: 27.050716212538422, Test Loss: 29.276361688390956\n",
      "Epoch [100/100], Train Loss: 27.188185820032338, Test Loss: 29.18282412244128\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 43.8658646380315, Test Loss: 44.136632250500966\n",
      "Epoch [20/100], Train Loss: 36.69889433813877, Test Loss: 36.50502635906269\n",
      "Epoch [30/100], Train Loss: 31.275350101658557, Test Loss: 33.7972777230399\n",
      "Epoch [40/100], Train Loss: 27.88812262425657, Test Loss: 29.017163239516222\n",
      "Epoch [50/100], Train Loss: 26.061814667748624, Test Loss: 27.765105581902837\n",
      "Epoch [60/100], Train Loss: 24.837691985583696, Test Loss: 27.746489586768213\n",
      "Epoch [70/100], Train Loss: 24.57009835165055, Test Loss: 28.013079507010325\n",
      "Epoch [80/100], Train Loss: 23.67698268577701, Test Loss: 27.781423692579395\n",
      "Epoch [90/100], Train Loss: 23.492467530047307, Test Loss: 27.840396881103516\n",
      "Epoch [100/100], Train Loss: 22.570764416553935, Test Loss: 27.55481296390682\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 43.87724733196321, Test Loss: 44.883350520939025\n",
      "Epoch [20/100], Train Loss: 36.4876395553839, Test Loss: 37.637540643865414\n",
      "Epoch [30/100], Train Loss: 31.614813476312356, Test Loss: 32.765672956194194\n",
      "Epoch [40/100], Train Loss: 27.69022138001489, Test Loss: 31.663934236997132\n",
      "Epoch [50/100], Train Loss: 25.960748691246156, Test Loss: 29.417022729848888\n",
      "Epoch [60/100], Train Loss: 24.937534132160124, Test Loss: 28.47445163479099\n",
      "Epoch [70/100], Train Loss: 23.577100266003217, Test Loss: 28.51431423038631\n",
      "Epoch [80/100], Train Loss: 24.844710178062563, Test Loss: 28.097567422049387\n",
      "Epoch [90/100], Train Loss: 23.678310857053663, Test Loss: 29.671510622098847\n",
      "Epoch [100/100], Train Loss: 22.014500821223024, Test Loss: 28.28862135750907\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.92008805822154, Test Loss: 38.84331026944247\n",
      "Epoch [20/100], Train Loss: 37.953664491997394, Test Loss: 38.31410905912325\n",
      "Epoch [30/100], Train Loss: 32.540740654116775, Test Loss: 36.52396922916561\n",
      "Epoch [40/100], Train Loss: 29.314843606167152, Test Loss: 30.33253692032455\n",
      "Epoch [50/100], Train Loss: 26.978535355114545, Test Loss: 29.032414795516374\n",
      "Epoch [60/100], Train Loss: 24.846568116985384, Test Loss: 28.44855140091537\n",
      "Epoch [70/100], Train Loss: 25.73368471489578, Test Loss: 27.602769083790964\n",
      "Epoch [80/100], Train Loss: 23.49904691977579, Test Loss: 27.872362533173003\n",
      "Epoch [90/100], Train Loss: 23.718329157594773, Test Loss: 27.513118892521053\n",
      "Epoch [100/100], Train Loss: 23.554180589269418, Test Loss: 28.205317980283265\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.576677316134095, Test Loss: 43.66905975341797\n",
      "Epoch [20/100], Train Loss: 37.882582229864404, Test Loss: 37.34558016294009\n",
      "Epoch [30/100], Train Loss: 32.85342275900919, Test Loss: 34.39507075718471\n",
      "Epoch [40/100], Train Loss: 28.42581965337034, Test Loss: 30.205265887371905\n",
      "Epoch [50/100], Train Loss: 26.09470640088691, Test Loss: 28.826681533417144\n",
      "Epoch [60/100], Train Loss: 24.62183328847416, Test Loss: 28.61519481609394\n",
      "Epoch [70/100], Train Loss: 24.521477971311477, Test Loss: 28.40272142980006\n",
      "Epoch [80/100], Train Loss: 22.986061840370052, Test Loss: 28.799827427059025\n",
      "Epoch [90/100], Train Loss: 22.629671840980404, Test Loss: 28.88411417874423\n",
      "Epoch [100/100], Train Loss: 22.876976106987623, Test Loss: 28.812600197730127\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.52586826261927, Test Loss: 45.559879748852225\n",
      "Epoch [20/100], Train Loss: 37.348764125636364, Test Loss: 32.88207227533514\n",
      "Epoch [30/100], Train Loss: 32.52181716668801, Test Loss: 35.33481501294421\n",
      "Epoch [40/100], Train Loss: 29.701772058205528, Test Loss: 28.565403628658938\n",
      "Epoch [50/100], Train Loss: 26.849186606485336, Test Loss: 28.174630004090147\n",
      "Epoch [60/100], Train Loss: 25.8484218284732, Test Loss: 28.130061285836355\n",
      "Epoch [70/100], Train Loss: 23.3083809524286, Test Loss: 28.458895794757\n",
      "Epoch [80/100], Train Loss: 24.696128663860385, Test Loss: 28.96411994834999\n",
      "Epoch [90/100], Train Loss: 23.373439626224705, Test Loss: 27.45514963818835\n",
      "Epoch [100/100], Train Loss: 23.987808446415134, Test Loss: 28.36368701984356\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.8840666598961, Test Loss: 42.93032108653676\n",
      "Epoch [20/100], Train Loss: 37.55108431206375, Test Loss: 37.09474092954165\n",
      "Epoch [30/100], Train Loss: 32.43846990866739, Test Loss: 32.85483625337675\n",
      "Epoch [40/100], Train Loss: 28.920291112680903, Test Loss: 29.519392186945137\n",
      "Epoch [50/100], Train Loss: 26.259769327132428, Test Loss: 29.03813939280324\n",
      "Epoch [60/100], Train Loss: 25.30168053048556, Test Loss: 27.625938948098714\n",
      "Epoch [70/100], Train Loss: 24.540153228259477, Test Loss: 28.165656795749417\n",
      "Epoch [80/100], Train Loss: 23.936871512991484, Test Loss: 27.98963737487793\n",
      "Epoch [90/100], Train Loss: 23.25506501745005, Test Loss: 27.84442544912363\n",
      "Epoch [100/100], Train Loss: 24.03020167116259, Test Loss: 27.625546467768682\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.0376681843742, Test Loss: 46.144482154350776\n",
      "Epoch [20/100], Train Loss: 36.821145617375606, Test Loss: 37.01293479622184\n",
      "Epoch [30/100], Train Loss: 31.846706853147413, Test Loss: 31.91456346387987\n",
      "Epoch [40/100], Train Loss: 28.067773181102314, Test Loss: 30.841287835851894\n",
      "Epoch [50/100], Train Loss: 26.224830783781456, Test Loss: 29.097730661367443\n",
      "Epoch [60/100], Train Loss: 25.342072552540262, Test Loss: 28.88123576672046\n",
      "Epoch [70/100], Train Loss: 25.42558825133277, Test Loss: 29.707007519610517\n",
      "Epoch [80/100], Train Loss: 24.631389261464605, Test Loss: 28.654957015793045\n",
      "Epoch [90/100], Train Loss: 23.948839631627816, Test Loss: 28.90310322154652\n",
      "Epoch [100/100], Train Loss: 23.751580110143443, Test Loss: 27.81557626848097\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 44.04267551859871, Test Loss: 44.628647346001166\n",
      "Epoch [20/100], Train Loss: 36.932130782330624, Test Loss: 40.02212856342266\n",
      "Epoch [30/100], Train Loss: 31.8101016998291, Test Loss: 32.98413170158089\n",
      "Epoch [40/100], Train Loss: 27.765569555564007, Test Loss: 29.139721015831093\n",
      "Epoch [50/100], Train Loss: 26.36857678147613, Test Loss: 28.355089113309784\n",
      "Epoch [60/100], Train Loss: 24.399668859262935, Test Loss: 28.0805478281789\n",
      "Epoch [70/100], Train Loss: 24.403666980931018, Test Loss: 27.769339078432555\n",
      "Epoch [80/100], Train Loss: 23.91394832798692, Test Loss: 28.412347867891388\n",
      "Epoch [90/100], Train Loss: 23.37683238045114, Test Loss: 26.909321599192435\n",
      "Epoch [100/100], Train Loss: 24.289162939102923, Test Loss: 28.49706810790223\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 45.02109787737737, Test Loss: 44.8245951169497\n",
      "Epoch [20/100], Train Loss: 38.52842891255363, Test Loss: 34.182256624296116\n",
      "Epoch [30/100], Train Loss: 32.812444317927124, Test Loss: 33.84367744643967\n",
      "Epoch [40/100], Train Loss: 28.535993919997917, Test Loss: 32.41032588017451\n",
      "Epoch [50/100], Train Loss: 27.03822313527592, Test Loss: 29.106326412844968\n",
      "Epoch [60/100], Train Loss: 26.485727253898247, Test Loss: 28.90350574642033\n",
      "Epoch [70/100], Train Loss: 25.708644435444818, Test Loss: 29.389600109744382\n",
      "Epoch [80/100], Train Loss: 24.493947788926423, Test Loss: 27.785209086034204\n",
      "Epoch [90/100], Train Loss: 23.936279640823113, Test Loss: 28.082668676004783\n",
      "Epoch [100/100], Train Loss: 23.763233316140095, Test Loss: 29.643064375047558\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.328525524452086, Test Loss: 31.521964234191103\n",
      "Epoch [20/100], Train Loss: 26.689887450171298, Test Loss: 32.49698520016361\n",
      "Epoch [30/100], Train Loss: 25.47502480178583, Test Loss: 28.2099453814618\n",
      "Epoch [40/100], Train Loss: 25.298139009319367, Test Loss: 29.258938430191634\n",
      "Epoch [50/100], Train Loss: 24.099752563726707, Test Loss: 28.352182091056527\n",
      "Epoch [60/100], Train Loss: 23.482486656063895, Test Loss: 28.692875577257826\n",
      "Epoch [70/100], Train Loss: 22.962308952456613, Test Loss: 28.411410145945364\n",
      "Epoch [80/100], Train Loss: 23.14844500432249, Test Loss: 28.691461885130252\n",
      "Epoch [90/100], Train Loss: 23.27014814283027, Test Loss: 28.738473545421254\n",
      "Epoch [100/100], Train Loss: 22.827683983474483, Test Loss: 29.501281019929166\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.49163368725386, Test Loss: 31.56622210415927\n",
      "Epoch [20/100], Train Loss: 25.439848621556017, Test Loss: 33.4758824682855\n",
      "Epoch [30/100], Train Loss: 25.181692698744477, Test Loss: 28.36596206565956\n",
      "Epoch [40/100], Train Loss: 25.10945813538598, Test Loss: 28.191727625859247\n",
      "Epoch [50/100], Train Loss: 25.26245512415151, Test Loss: 29.217093207619406\n",
      "Epoch [60/100], Train Loss: 24.82933542845679, Test Loss: 29.457923542369496\n",
      "Epoch [70/100], Train Loss: 24.1187605435731, Test Loss: 27.930905106779818\n",
      "Epoch [80/100], Train Loss: 22.841880066668402, Test Loss: 28.94835241738852\n",
      "Epoch [90/100], Train Loss: 22.875953030195394, Test Loss: 28.49139287874296\n",
      "Epoch [100/100], Train Loss: 21.98701682168929, Test Loss: 29.33112218782499\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.403766044241483, Test Loss: 30.956313442874265\n",
      "Epoch [20/100], Train Loss: 25.358961280447538, Test Loss: 31.603150974620473\n",
      "Epoch [30/100], Train Loss: 26.105450652075596, Test Loss: 29.612354947375014\n",
      "Epoch [40/100], Train Loss: 24.241379765995212, Test Loss: 31.96855099170239\n",
      "Epoch [50/100], Train Loss: 26.191736133763047, Test Loss: 28.64750644138881\n",
      "Epoch [60/100], Train Loss: 23.64876232459897, Test Loss: 28.777736564735314\n",
      "Epoch [70/100], Train Loss: 23.35840082637599, Test Loss: 29.100747888738457\n",
      "Epoch [80/100], Train Loss: 24.378746282858927, Test Loss: 28.462511236017402\n",
      "Epoch [90/100], Train Loss: 22.880368967525296, Test Loss: 30.11507576781434\n",
      "Epoch [100/100], Train Loss: 22.46959647506964, Test Loss: 28.862449992786754\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.46922882580366, Test Loss: 35.034262099823394\n",
      "Epoch [20/100], Train Loss: 28.119905997104333, Test Loss: 28.45635998713506\n",
      "Epoch [30/100], Train Loss: 24.694761932873334, Test Loss: 30.188417905336852\n",
      "Epoch [40/100], Train Loss: 25.345329447261623, Test Loss: 31.7573867649227\n",
      "Epoch [50/100], Train Loss: 26.379665355995055, Test Loss: 28.11954183702345\n",
      "Epoch [60/100], Train Loss: 25.20413628875232, Test Loss: 28.17840623236322\n",
      "Epoch [70/100], Train Loss: 24.90812979526207, Test Loss: 27.914989942080016\n",
      "Epoch [80/100], Train Loss: 23.589281970164816, Test Loss: 32.52769898749017\n",
      "Epoch [90/100], Train Loss: 25.13565317998167, Test Loss: 28.580368339241325\n",
      "Epoch [100/100], Train Loss: 23.52663070803783, Test Loss: 28.357504435947963\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.36291692765033, Test Loss: 34.13610884431121\n",
      "Epoch [20/100], Train Loss: 26.115946685290726, Test Loss: 29.290571955891398\n",
      "Epoch [30/100], Train Loss: 25.590968366529122, Test Loss: 30.73293941051929\n",
      "Epoch [40/100], Train Loss: 25.002240528044155, Test Loss: 28.394365038190568\n",
      "Epoch [50/100], Train Loss: 25.260212182216957, Test Loss: 30.174207117650415\n",
      "Epoch [60/100], Train Loss: 24.47491925349001, Test Loss: 29.590202826958198\n",
      "Epoch [70/100], Train Loss: 24.320791894881452, Test Loss: 28.492227133218343\n",
      "Epoch [80/100], Train Loss: 23.917722883380826, Test Loss: 28.07250570322012\n",
      "Epoch [90/100], Train Loss: 23.88140941682409, Test Loss: 27.771407560868695\n",
      "Epoch [100/100], Train Loss: 23.58596267700195, Test Loss: 27.706030449309907\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.66296268525671, Test Loss: 32.47135494281719\n",
      "Epoch [20/100], Train Loss: 26.872837229244045, Test Loss: 32.77097362666935\n",
      "Epoch [30/100], Train Loss: 25.700533113323274, Test Loss: 28.180335874681347\n",
      "Epoch [40/100], Train Loss: 25.62195403302302, Test Loss: 27.54420045134309\n",
      "Epoch [50/100], Train Loss: 25.016223826173878, Test Loss: 29.500647606787744\n",
      "Epoch [60/100], Train Loss: 24.397785515081686, Test Loss: 27.775103630957666\n",
      "Epoch [70/100], Train Loss: 24.661470181824733, Test Loss: 30.27906155276608\n",
      "Epoch [80/100], Train Loss: 23.45853763017498, Test Loss: 28.84436139193448\n",
      "Epoch [90/100], Train Loss: 23.95470302144035, Test Loss: 28.181020811006622\n",
      "Epoch [100/100], Train Loss: 24.875467444247885, Test Loss: 27.546886592716366\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.64997404129779, Test Loss: 35.662177494594026\n",
      "Epoch [20/100], Train Loss: 28.625572060756998, Test Loss: 30.36574968115076\n",
      "Epoch [30/100], Train Loss: 26.370785216034434, Test Loss: 38.807614759965375\n",
      "Epoch [40/100], Train Loss: 27.087341258564933, Test Loss: 28.900811182988154\n",
      "Epoch [50/100], Train Loss: 25.535325828927462, Test Loss: 29.93018816663073\n",
      "Epoch [60/100], Train Loss: 25.047864382384255, Test Loss: 30.787169171618178\n",
      "Epoch [70/100], Train Loss: 25.278213175789254, Test Loss: 30.19424475632705\n",
      "Epoch [80/100], Train Loss: 24.672604957955784, Test Loss: 29.667974546358185\n",
      "Epoch [90/100], Train Loss: 24.609427280113344, Test Loss: 29.295017836929915\n",
      "Epoch [100/100], Train Loss: 25.317392105352685, Test Loss: 28.232268048571303\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.217118960521262, Test Loss: 34.70923317252816\n",
      "Epoch [20/100], Train Loss: 26.595602241891328, Test Loss: 30.235624635374393\n",
      "Epoch [30/100], Train Loss: 26.731285545474194, Test Loss: 31.089270678433504\n",
      "Epoch [40/100], Train Loss: 25.59726725719014, Test Loss: 33.06148590979638\n",
      "Epoch [50/100], Train Loss: 25.8618441347216, Test Loss: 28.729570215398613\n",
      "Epoch [60/100], Train Loss: 25.215249433673797, Test Loss: 28.436657422548766\n",
      "Epoch [70/100], Train Loss: 25.858562788416126, Test Loss: 29.473991369272206\n",
      "Epoch [80/100], Train Loss: 25.037056406990427, Test Loss: 29.532053712126498\n",
      "Epoch [90/100], Train Loss: 24.84901972911397, Test Loss: 27.95152371889585\n",
      "Epoch [100/100], Train Loss: 25.026288273295417, Test Loss: 28.43693004954945\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.89385726803639, Test Loss: 35.30334324031681\n",
      "Epoch [20/100], Train Loss: 27.369652976364385, Test Loss: 31.572107438917286\n",
      "Epoch [30/100], Train Loss: 26.668931979820375, Test Loss: 33.43355047547972\n",
      "Epoch [40/100], Train Loss: 25.62150047177174, Test Loss: 28.511790164105303\n",
      "Epoch [50/100], Train Loss: 25.37149187932249, Test Loss: 28.497604518741756\n",
      "Epoch [60/100], Train Loss: 25.72364399394051, Test Loss: 28.61787476477685\n",
      "Epoch [70/100], Train Loss: 25.288807134159274, Test Loss: 27.88197121062836\n",
      "Epoch [80/100], Train Loss: 24.575331747336467, Test Loss: 28.024612278133244\n",
      "Epoch [90/100], Train Loss: 24.42819941161109, Test Loss: 28.20296661575119\n",
      "Epoch [100/100], Train Loss: 24.815663171987065, Test Loss: 28.30252526023171\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.994919730014487, Test Loss: 39.99839267483005\n",
      "Epoch [20/100], Train Loss: 26.156251313256437, Test Loss: 31.47526689009233\n",
      "Epoch [30/100], Train Loss: 25.743676739051693, Test Loss: 39.25046068662173\n",
      "Epoch [40/100], Train Loss: 25.096775617755828, Test Loss: 29.976913724626815\n",
      "Epoch [50/100], Train Loss: 24.89932806921787, Test Loss: 33.1696726563689\n",
      "Epoch [60/100], Train Loss: 25.519760675899317, Test Loss: 29.23219720419351\n",
      "Epoch [70/100], Train Loss: 24.73672731743484, Test Loss: 31.14956437148057\n",
      "Epoch [80/100], Train Loss: 23.87347897388896, Test Loss: 28.986351359974254\n",
      "Epoch [90/100], Train Loss: 24.223727179355308, Test Loss: 28.196386560217128\n",
      "Epoch [100/100], Train Loss: 24.311727298674036, Test Loss: 29.15628247446828\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.071354087454374, Test Loss: 32.77064283792075\n",
      "Epoch [20/100], Train Loss: 29.113398429995677, Test Loss: 29.06737117024211\n",
      "Epoch [30/100], Train Loss: 25.875693468187677, Test Loss: 29.688096925809784\n",
      "Epoch [40/100], Train Loss: 25.65436719675533, Test Loss: 31.63807891251205\n",
      "Epoch [50/100], Train Loss: 27.959151239864163, Test Loss: 29.321459014694412\n",
      "Epoch [60/100], Train Loss: 27.27009045335113, Test Loss: 28.145313262939453\n",
      "Epoch [70/100], Train Loss: 24.233775992471664, Test Loss: 28.703955712256494\n",
      "Epoch [80/100], Train Loss: 24.900364666297786, Test Loss: 28.76535636109191\n",
      "Epoch [90/100], Train Loss: 24.907759288099946, Test Loss: 28.281553714306323\n",
      "Epoch [100/100], Train Loss: 25.219315000440254, Test Loss: 28.639731692029283\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.54384304109167, Test Loss: 32.36326351413479\n",
      "Epoch [20/100], Train Loss: 27.773588818409404, Test Loss: 31.39910873809418\n",
      "Epoch [30/100], Train Loss: 26.776890926673765, Test Loss: 28.43252689807446\n",
      "Epoch [40/100], Train Loss: 25.48488202329542, Test Loss: 36.061214967207476\n",
      "Epoch [50/100], Train Loss: 25.158195426815848, Test Loss: 28.829364454591428\n",
      "Epoch [60/100], Train Loss: 25.0251826489558, Test Loss: 28.43858446393694\n",
      "Epoch [70/100], Train Loss: 26.510971901065012, Test Loss: 29.743326013738457\n",
      "Epoch [80/100], Train Loss: 24.932616712226242, Test Loss: 29.126360484531947\n",
      "Epoch [90/100], Train Loss: 25.555045381139536, Test Loss: 27.860237914246397\n",
      "Epoch [100/100], Train Loss: 24.85210972770316, Test Loss: 28.02085965639585\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.20670618151055, Test Loss: 32.04959844614004\n",
      "Epoch [20/100], Train Loss: 27.262302261102395, Test Loss: 30.210842107797596\n",
      "Epoch [30/100], Train Loss: 27.825981846793752, Test Loss: 32.600094857153955\n",
      "Epoch [40/100], Train Loss: 25.973010385231895, Test Loss: 29.248457945786512\n",
      "Epoch [50/100], Train Loss: 25.3705174555544, Test Loss: 29.972604578191582\n",
      "Epoch [60/100], Train Loss: 25.27405691928551, Test Loss: 30.292553369100993\n",
      "Epoch [70/100], Train Loss: 26.454996759383405, Test Loss: 28.602410997663224\n",
      "Epoch [80/100], Train Loss: 25.610373493882477, Test Loss: 28.527486231419946\n",
      "Epoch [90/100], Train Loss: 25.31026433725826, Test Loss: 28.06403110553692\n",
      "Epoch [100/100], Train Loss: 24.785462470132796, Test Loss: 33.939144828102805\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.3279402373267, Test Loss: 35.349889259833795\n",
      "Epoch [20/100], Train Loss: 26.761117478667714, Test Loss: 30.90235199866357\n",
      "Epoch [30/100], Train Loss: 26.572071763335682, Test Loss: 29.991950493354302\n",
      "Epoch [40/100], Train Loss: 27.3999819145828, Test Loss: 30.27776829608075\n",
      "Epoch [50/100], Train Loss: 26.03937276621334, Test Loss: 32.9411712993275\n",
      "Epoch [60/100], Train Loss: 28.071217458756244, Test Loss: 33.9469498721036\n",
      "Epoch [70/100], Train Loss: 25.560352481779503, Test Loss: 28.766349817251232\n",
      "Epoch [80/100], Train Loss: 25.492449475898116, Test Loss: 31.94064120503215\n",
      "Epoch [90/100], Train Loss: 24.515222705778527, Test Loss: 31.996793177220727\n",
      "Epoch [100/100], Train Loss: 25.06328511472608, Test Loss: 57.835803688346566\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.219915321225027, Test Loss: 36.47279362864309\n",
      "Epoch [20/100], Train Loss: 25.94613483616563, Test Loss: 32.349233751173145\n",
      "Epoch [30/100], Train Loss: 25.458462987180617, Test Loss: 35.44077219281878\n",
      "Epoch [40/100], Train Loss: 27.26744846281458, Test Loss: 27.92072194582456\n",
      "Epoch [50/100], Train Loss: 26.377697960275118, Test Loss: 29.17610767909459\n",
      "Epoch [60/100], Train Loss: 26.19748441352219, Test Loss: 28.410102893779804\n",
      "Epoch [70/100], Train Loss: 25.727347345821194, Test Loss: 27.345978328159877\n",
      "Epoch [80/100], Train Loss: 25.291214427009958, Test Loss: 28.089706371356915\n",
      "Epoch [90/100], Train Loss: 26.15077118170066, Test Loss: 28.41911387753177\n",
      "Epoch [100/100], Train Loss: 23.986967405725697, Test Loss: 27.652463541402444\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.453858159800046, Test Loss: 34.45267783821403\n",
      "Epoch [20/100], Train Loss: 28.500239462930647, Test Loss: 37.07265616082526\n",
      "Epoch [30/100], Train Loss: 29.740141371429942, Test Loss: 32.494836658626404\n",
      "Epoch [40/100], Train Loss: 27.57777150888912, Test Loss: 31.07648567100624\n",
      "Epoch [50/100], Train Loss: 26.821841762104974, Test Loss: 28.46633341405299\n",
      "Epoch [60/100], Train Loss: 25.986809320918848, Test Loss: 37.78929940756265\n",
      "Epoch [70/100], Train Loss: 26.53765239402896, Test Loss: 30.178316710831282\n",
      "Epoch [80/100], Train Loss: 25.719418422511367, Test Loss: 28.636678745220234\n",
      "Epoch [90/100], Train Loss: 26.362128229610256, Test Loss: 28.786118693165964\n",
      "Epoch [100/100], Train Loss: 26.08959677023966, Test Loss: 43.965692743078456\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.68324007128106, Test Loss: 36.51221451201996\n",
      "Epoch [20/100], Train Loss: 27.614393378085776, Test Loss: 48.95169562798042\n",
      "Epoch [30/100], Train Loss: 26.67507661913262, Test Loss: 29.973998627105317\n",
      "Epoch [40/100], Train Loss: 27.003768614471937, Test Loss: 29.366832386363637\n",
      "Epoch [50/100], Train Loss: 27.90808127043677, Test Loss: 28.65732289599134\n",
      "Epoch [60/100], Train Loss: 25.26169035239298, Test Loss: 29.99902039069634\n",
      "Epoch [70/100], Train Loss: 26.76438347238009, Test Loss: 31.936791853471235\n",
      "Epoch [80/100], Train Loss: 27.560623869348746, Test Loss: 30.598242326216265\n",
      "Epoch [90/100], Train Loss: 25.8705506121526, Test Loss: 30.8657528022667\n",
      "Epoch [100/100], Train Loss: 25.657793232652008, Test Loss: 31.21969711006462\n",
      "Training model with hidden size 256, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.913862028278288, Test Loss: 36.417218988591976\n",
      "Epoch [20/100], Train Loss: 29.07535734958336, Test Loss: 31.387287635307807\n",
      "Epoch [30/100], Train Loss: 27.808358276867477, Test Loss: 31.973118274242847\n",
      "Epoch [40/100], Train Loss: 26.629885107571962, Test Loss: 31.669926730069246\n",
      "Epoch [50/100], Train Loss: 27.041057730502768, Test Loss: 29.983886198564008\n",
      "Epoch [60/100], Train Loss: 26.5031225986168, Test Loss: 33.98932598163555\n",
      "Epoch [70/100], Train Loss: 26.90411337555432, Test Loss: 31.1587147403073\n",
      "Epoch [80/100], Train Loss: 25.936123294517643, Test Loss: 33.36280612202434\n",
      "Epoch [90/100], Train Loss: 26.07815396043121, Test Loss: 30.17476938916491\n",
      "Epoch [100/100], Train Loss: 25.15042173667032, Test Loss: 30.537720618309912\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.58731454317687, Test Loss: 50.699581146240234\n",
      "Epoch [20/100], Train Loss: 41.7977587215236, Test Loss: 41.04121780395508\n",
      "Epoch [30/100], Train Loss: 37.143608931244394, Test Loss: 37.01826095581055\n",
      "Epoch [40/100], Train Loss: 33.37132737206631, Test Loss: 35.09535598754883\n",
      "Epoch [50/100], Train Loss: 30.002558561231268, Test Loss: 30.536396026611328\n",
      "Epoch [60/100], Train Loss: 27.383634761122405, Test Loss: 29.40300941467285\n",
      "Epoch [70/100], Train Loss: 26.570931250150085, Test Loss: 29.76380729675293\n",
      "Epoch [80/100], Train Loss: 24.59558220222348, Test Loss: 28.864927291870117\n",
      "Epoch [90/100], Train Loss: 24.263265778588465, Test Loss: 29.44944953918457\n",
      "Epoch [100/100], Train Loss: 23.90301100934138, Test Loss: 28.85979652404785\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.20937096017306, Test Loss: 48.8099365234375\n",
      "Epoch [20/100], Train Loss: 42.35356047583408, Test Loss: 46.086830139160156\n",
      "Epoch [30/100], Train Loss: 38.19977537061347, Test Loss: 37.707271575927734\n",
      "Epoch [40/100], Train Loss: 34.340772241060854, Test Loss: 34.68410873413086\n",
      "Epoch [50/100], Train Loss: 31.139297091374633, Test Loss: 31.979175567626953\n",
      "Epoch [60/100], Train Loss: 28.306085880467148, Test Loss: 31.997310638427734\n",
      "Epoch [70/100], Train Loss: 26.583341267069834, Test Loss: 29.452455520629883\n",
      "Epoch [80/100], Train Loss: 25.88299650598745, Test Loss: 28.655120849609375\n",
      "Epoch [90/100], Train Loss: 24.540745932156923, Test Loss: 28.067180633544922\n",
      "Epoch [100/100], Train Loss: 23.95509654185811, Test Loss: 28.82210350036621\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.30579430001681, Test Loss: 46.84619903564453\n",
      "Epoch [20/100], Train Loss: 42.49765034659964, Test Loss: 41.95907211303711\n",
      "Epoch [30/100], Train Loss: 38.468556025770845, Test Loss: 38.661537170410156\n",
      "Epoch [40/100], Train Loss: 34.71018676757812, Test Loss: 34.08890914916992\n",
      "Epoch [50/100], Train Loss: 31.38666543178871, Test Loss: 31.922008514404297\n",
      "Epoch [60/100], Train Loss: 29.152065977503042, Test Loss: 31.712779998779297\n",
      "Epoch [70/100], Train Loss: 28.010917194554064, Test Loss: 30.63506317138672\n",
      "Epoch [80/100], Train Loss: 25.88422658951556, Test Loss: 28.1830997467041\n",
      "Epoch [90/100], Train Loss: 24.707975068639538, Test Loss: 28.854970932006836\n",
      "Epoch [100/100], Train Loss: 24.190444208364017, Test Loss: 28.841022491455078\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.441818562491996, Test Loss: 48.73622512817383\n",
      "Epoch [20/100], Train Loss: 42.86531530911805, Test Loss: 42.559608459472656\n",
      "Epoch [30/100], Train Loss: 38.457080778528436, Test Loss: 35.39704132080078\n",
      "Epoch [40/100], Train Loss: 34.67218306494541, Test Loss: 33.093196868896484\n",
      "Epoch [50/100], Train Loss: 31.982676915653418, Test Loss: 30.24254035949707\n",
      "Epoch [60/100], Train Loss: 30.184683865406473, Test Loss: 30.314119338989258\n",
      "Epoch [70/100], Train Loss: 26.882670699572955, Test Loss: 29.51624870300293\n",
      "Epoch [80/100], Train Loss: 25.996248095152808, Test Loss: 29.61358642578125\n",
      "Epoch [90/100], Train Loss: 24.761847592963548, Test Loss: 28.651592254638672\n",
      "Epoch [100/100], Train Loss: 24.109805716842903, Test Loss: 29.286052703857422\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.63306881013464, Test Loss: 49.87067794799805\n",
      "Epoch [20/100], Train Loss: 42.84426043150855, Test Loss: 44.75564193725586\n",
      "Epoch [30/100], Train Loss: 38.34268205986648, Test Loss: 36.96217727661133\n",
      "Epoch [40/100], Train Loss: 34.59666995689517, Test Loss: 37.23875427246094\n",
      "Epoch [50/100], Train Loss: 31.269175807765272, Test Loss: 34.0012321472168\n",
      "Epoch [60/100], Train Loss: 29.26507659036605, Test Loss: 32.740116119384766\n",
      "Epoch [70/100], Train Loss: 26.7322295767362, Test Loss: 29.608308792114258\n",
      "Epoch [80/100], Train Loss: 24.994981052836433, Test Loss: 29.035634994506836\n",
      "Epoch [90/100], Train Loss: 23.722492599487303, Test Loss: 27.91771125793457\n",
      "Epoch [100/100], Train Loss: 23.735962351814646, Test Loss: 28.00468635559082\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.09824960427206, Test Loss: 51.26210021972656\n",
      "Epoch [20/100], Train Loss: 42.42231831785108, Test Loss: 43.074363708496094\n",
      "Epoch [30/100], Train Loss: 37.884478209448645, Test Loss: 35.78062057495117\n",
      "Epoch [40/100], Train Loss: 34.101749270079566, Test Loss: 31.216854095458984\n",
      "Epoch [50/100], Train Loss: 30.9815649751757, Test Loss: 35.27119445800781\n",
      "Epoch [60/100], Train Loss: 28.782846269451206, Test Loss: 31.06410026550293\n",
      "Epoch [70/100], Train Loss: 26.68263279649078, Test Loss: 30.557435989379883\n",
      "Epoch [80/100], Train Loss: 27.134363568415406, Test Loss: 29.20282745361328\n",
      "Epoch [90/100], Train Loss: 24.393886791291784, Test Loss: 28.839561462402344\n",
      "Epoch [100/100], Train Loss: 24.287479531960408, Test Loss: 28.152395248413086\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.01554266507508, Test Loss: 51.130802154541016\n",
      "Epoch [20/100], Train Loss: 42.079315335633325, Test Loss: 43.45106887817383\n",
      "Epoch [30/100], Train Loss: 38.03098459712795, Test Loss: 38.595943450927734\n",
      "Epoch [40/100], Train Loss: 34.180069663876395, Test Loss: 33.22582244873047\n",
      "Epoch [50/100], Train Loss: 30.697377683295578, Test Loss: 30.965242385864258\n",
      "Epoch [60/100], Train Loss: 28.882653008132685, Test Loss: 32.211456298828125\n",
      "Epoch [70/100], Train Loss: 26.62771903491411, Test Loss: 28.429534912109375\n",
      "Epoch [80/100], Train Loss: 26.455267990612594, Test Loss: 29.766056060791016\n",
      "Epoch [90/100], Train Loss: 24.824753126550892, Test Loss: 30.02090072631836\n",
      "Epoch [100/100], Train Loss: 24.344494966600763, Test Loss: 30.360668182373047\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 46.62814050893314, Test Loss: 48.69578170776367\n",
      "Epoch [20/100], Train Loss: 41.65934941026031, Test Loss: 44.19346237182617\n",
      "Epoch [30/100], Train Loss: 37.41493315149526, Test Loss: 36.56917190551758\n",
      "Epoch [40/100], Train Loss: 33.69744270199635, Test Loss: 34.11678695678711\n",
      "Epoch [50/100], Train Loss: 31.166900447157563, Test Loss: 33.03242874145508\n",
      "Epoch [60/100], Train Loss: 28.953079223632812, Test Loss: 31.964570999145508\n",
      "Epoch [70/100], Train Loss: 26.364839822737896, Test Loss: 29.462446212768555\n",
      "Epoch [80/100], Train Loss: 26.715025254546617, Test Loss: 29.680519104003906\n",
      "Epoch [90/100], Train Loss: 24.71196327834833, Test Loss: 28.99288558959961\n",
      "Epoch [100/100], Train Loss: 25.535418457281395, Test Loss: 28.367250442504883\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 47.00983552776399, Test Loss: 47.60877227783203\n",
      "Epoch [20/100], Train Loss: 42.793244608894724, Test Loss: 35.769657135009766\n",
      "Epoch [30/100], Train Loss: 38.24808376499864, Test Loss: 38.82471466064453\n",
      "Epoch [40/100], Train Loss: 34.45269502733574, Test Loss: 34.45353317260742\n",
      "Epoch [50/100], Train Loss: 31.2301444381964, Test Loss: 32.51758575439453\n",
      "Epoch [60/100], Train Loss: 29.54985858729628, Test Loss: 28.969993591308594\n",
      "Epoch [70/100], Train Loss: 26.663488550655178, Test Loss: 29.38909339904785\n",
      "Epoch [80/100], Train Loss: 25.873655475553917, Test Loss: 29.337881088256836\n",
      "Epoch [90/100], Train Loss: 25.66406859100842, Test Loss: 29.706892013549805\n",
      "Epoch [100/100], Train Loss: 25.116609817254737, Test Loss: 27.947715759277344\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.510654912229445, Test Loss: 33.624916076660156\n",
      "Epoch [20/100], Train Loss: 24.59059141815686, Test Loss: 28.783620834350586\n",
      "Epoch [30/100], Train Loss: 24.968549165569367, Test Loss: 28.325674057006836\n",
      "Epoch [40/100], Train Loss: 24.07918571096952, Test Loss: 27.084678649902344\n",
      "Epoch [50/100], Train Loss: 23.698520041293786, Test Loss: 28.815746307373047\n",
      "Epoch [60/100], Train Loss: 23.52694296289663, Test Loss: 29.138591766357422\n",
      "Epoch [70/100], Train Loss: 24.037865798199764, Test Loss: 28.997478485107422\n",
      "Epoch [80/100], Train Loss: 23.37870840479116, Test Loss: 28.664554595947266\n",
      "Epoch [90/100], Train Loss: 23.910339649388046, Test Loss: 29.239486694335938\n",
      "Epoch [100/100], Train Loss: 22.30124306600602, Test Loss: 28.866994857788086\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.424392137371125, Test Loss: 31.568819046020508\n",
      "Epoch [20/100], Train Loss: 25.89488356543369, Test Loss: 33.0882568359375\n",
      "Epoch [30/100], Train Loss: 24.569218188426532, Test Loss: 28.69965934753418\n",
      "Epoch [40/100], Train Loss: 24.538612653388352, Test Loss: 27.363210678100586\n",
      "Epoch [50/100], Train Loss: 24.33930389529369, Test Loss: 27.181589126586914\n",
      "Epoch [60/100], Train Loss: 23.32856512226042, Test Loss: 30.629343032836914\n",
      "Epoch [70/100], Train Loss: 23.81990774185931, Test Loss: 28.198169708251953\n",
      "Epoch [80/100], Train Loss: 22.91552653078173, Test Loss: 28.670385360717773\n",
      "Epoch [90/100], Train Loss: 21.972640390865138, Test Loss: 29.242961883544922\n",
      "Epoch [100/100], Train Loss: 22.52883710392186, Test Loss: 28.966514587402344\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.299766065253586, Test Loss: 35.89767837524414\n",
      "Epoch [20/100], Train Loss: 26.537992783843492, Test Loss: 30.21367073059082\n",
      "Epoch [30/100], Train Loss: 24.449626960129034, Test Loss: 29.53330421447754\n",
      "Epoch [40/100], Train Loss: 23.717776733148295, Test Loss: 30.361614227294922\n",
      "Epoch [50/100], Train Loss: 24.54928159244725, Test Loss: 28.811580657958984\n",
      "Epoch [60/100], Train Loss: 24.40612241401047, Test Loss: 29.58922004699707\n",
      "Epoch [70/100], Train Loss: 23.272754450313382, Test Loss: 28.988988876342773\n",
      "Epoch [80/100], Train Loss: 23.09705116397045, Test Loss: 27.733930587768555\n",
      "Epoch [90/100], Train Loss: 22.27279707486512, Test Loss: 29.103803634643555\n",
      "Epoch [100/100], Train Loss: 22.839257068321352, Test Loss: 28.776071548461914\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.09484339229396, Test Loss: 39.28108596801758\n",
      "Epoch [20/100], Train Loss: 27.62627604750336, Test Loss: 29.760805130004883\n",
      "Epoch [30/100], Train Loss: 25.724771161939277, Test Loss: 29.29302978515625\n",
      "Epoch [40/100], Train Loss: 25.037730657858926, Test Loss: 28.893339157104492\n",
      "Epoch [50/100], Train Loss: 24.553705678220656, Test Loss: 30.147586822509766\n",
      "Epoch [60/100], Train Loss: 25.232647473694847, Test Loss: 29.658700942993164\n",
      "Epoch [70/100], Train Loss: 24.528479522955223, Test Loss: 30.14491081237793\n",
      "Epoch [80/100], Train Loss: 22.926200760387985, Test Loss: 29.473087310791016\n",
      "Epoch [90/100], Train Loss: 24.748020115836724, Test Loss: 27.090576171875\n",
      "Epoch [100/100], Train Loss: 23.300634227815223, Test Loss: 28.224924087524414\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.0309763236124, Test Loss: 38.0605354309082\n",
      "Epoch [20/100], Train Loss: 26.014600828827405, Test Loss: 31.106918334960938\n",
      "Epoch [30/100], Train Loss: 25.42601369013552, Test Loss: 30.16434097290039\n",
      "Epoch [40/100], Train Loss: 25.3042001255223, Test Loss: 31.91669464111328\n",
      "Epoch [50/100], Train Loss: 25.07651198965604, Test Loss: 27.327953338623047\n",
      "Epoch [60/100], Train Loss: 24.41538315444696, Test Loss: 28.92924690246582\n",
      "Epoch [70/100], Train Loss: 23.824332371696098, Test Loss: 29.05302619934082\n",
      "Epoch [80/100], Train Loss: 24.9406168640637, Test Loss: 30.337512969970703\n",
      "Epoch [90/100], Train Loss: 24.124024294243483, Test Loss: 28.122844696044922\n",
      "Epoch [100/100], Train Loss: 23.919656228237464, Test Loss: 28.15174102783203\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.101577402333746, Test Loss: 34.064456939697266\n",
      "Epoch [20/100], Train Loss: 25.954339205632444, Test Loss: 32.91054153442383\n",
      "Epoch [30/100], Train Loss: 24.985291778064166, Test Loss: 30.79502296447754\n",
      "Epoch [40/100], Train Loss: 25.080329157094486, Test Loss: 28.533666610717773\n",
      "Epoch [50/100], Train Loss: 24.673551259275342, Test Loss: 28.88409423828125\n",
      "Epoch [60/100], Train Loss: 24.32608669468614, Test Loss: 32.50630569458008\n",
      "Epoch [70/100], Train Loss: 23.403864557234968, Test Loss: 29.44896125793457\n",
      "Epoch [80/100], Train Loss: 24.55691987334705, Test Loss: 30.5034236907959\n",
      "Epoch [90/100], Train Loss: 23.990024122644645, Test Loss: 27.87181282043457\n",
      "Epoch [100/100], Train Loss: 23.0895839378482, Test Loss: 27.926143646240234\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.084040757476306, Test Loss: 38.50003433227539\n",
      "Epoch [20/100], Train Loss: 27.490568479944447, Test Loss: 34.1674690246582\n",
      "Epoch [30/100], Train Loss: 27.27135542572522, Test Loss: 35.36494064331055\n",
      "Epoch [40/100], Train Loss: 26.897535886920867, Test Loss: 31.52958869934082\n",
      "Epoch [50/100], Train Loss: 25.57127742454654, Test Loss: 30.549354553222656\n",
      "Epoch [60/100], Train Loss: 25.279470312399944, Test Loss: 30.656124114990234\n",
      "Epoch [70/100], Train Loss: 25.334209823608397, Test Loss: 30.719663619995117\n",
      "Epoch [80/100], Train Loss: 25.351719909417824, Test Loss: 31.248323440551758\n",
      "Epoch [90/100], Train Loss: 24.893937670598266, Test Loss: 33.040828704833984\n",
      "Epoch [100/100], Train Loss: 24.605808955333274, Test Loss: 28.664104461669922\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.504424142055825, Test Loss: 37.3277473449707\n",
      "Epoch [20/100], Train Loss: 28.964425384021197, Test Loss: 33.55270004272461\n",
      "Epoch [30/100], Train Loss: 26.23402349128098, Test Loss: 33.2010498046875\n",
      "Epoch [40/100], Train Loss: 26.600082897749104, Test Loss: 31.665128707885742\n",
      "Epoch [50/100], Train Loss: 26.813435463827165, Test Loss: 29.119455337524414\n",
      "Epoch [60/100], Train Loss: 25.365425960353164, Test Loss: 31.380691528320312\n",
      "Epoch [70/100], Train Loss: 25.71905900298572, Test Loss: 32.54342269897461\n",
      "Epoch [80/100], Train Loss: 25.307214524316006, Test Loss: 29.976720809936523\n",
      "Epoch [90/100], Train Loss: 24.486482739057696, Test Loss: 29.53165626525879\n",
      "Epoch [100/100], Train Loss: 24.914807660462426, Test Loss: 30.844036102294922\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.24808621015705, Test Loss: 38.240394592285156\n",
      "Epoch [20/100], Train Loss: 27.402612955062114, Test Loss: 33.69261932373047\n",
      "Epoch [30/100], Train Loss: 27.974741388539798, Test Loss: 33.58744430541992\n",
      "Epoch [40/100], Train Loss: 27.685084446140976, Test Loss: 29.58258628845215\n",
      "Epoch [50/100], Train Loss: 26.3588448196161, Test Loss: 31.24941635131836\n",
      "Epoch [60/100], Train Loss: 28.131582654108765, Test Loss: 29.604244232177734\n",
      "Epoch [70/100], Train Loss: 25.39637192272749, Test Loss: 29.976882934570312\n",
      "Epoch [80/100], Train Loss: 25.060887421154586, Test Loss: 29.308923721313477\n",
      "Epoch [90/100], Train Loss: 25.46111235071401, Test Loss: 31.781227111816406\n",
      "Epoch [100/100], Train Loss: 25.067757959834864, Test Loss: 32.966976165771484\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.64637442416832, Test Loss: 33.38840866088867\n",
      "Epoch [20/100], Train Loss: 27.46848874326612, Test Loss: 37.98157501220703\n",
      "Epoch [30/100], Train Loss: 26.357669336287703, Test Loss: 41.620723724365234\n",
      "Epoch [40/100], Train Loss: 25.872849974085074, Test Loss: 29.86753273010254\n",
      "Epoch [50/100], Train Loss: 25.10486934224113, Test Loss: 30.92466163635254\n",
      "Epoch [60/100], Train Loss: 25.128890647262825, Test Loss: 28.49854850769043\n",
      "Epoch [70/100], Train Loss: 27.853699530929816, Test Loss: 28.814729690551758\n",
      "Epoch [80/100], Train Loss: 24.73911488642458, Test Loss: 29.902128219604492\n",
      "Epoch [90/100], Train Loss: 24.56559106170154, Test Loss: 31.774169921875\n",
      "Epoch [100/100], Train Loss: 24.747833583394033, Test Loss: 29.16733741760254\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 26.710679388827966, Test Loss: 37.734493255615234\n",
      "Epoch [20/100], Train Loss: 27.319291361824412, Test Loss: 30.088449478149414\n",
      "Epoch [30/100], Train Loss: 26.475073373513144, Test Loss: 30.473392486572266\n",
      "Epoch [40/100], Train Loss: 25.632209859128857, Test Loss: 28.96516990661621\n",
      "Epoch [50/100], Train Loss: 25.340459116951365, Test Loss: 31.722043991088867\n",
      "Epoch [60/100], Train Loss: 25.59124291842101, Test Loss: 30.59788703918457\n",
      "Epoch [70/100], Train Loss: 25.731532850421843, Test Loss: 28.772815704345703\n",
      "Epoch [80/100], Train Loss: 25.088732766323403, Test Loss: 28.984235763549805\n",
      "Epoch [90/100], Train Loss: 25.126068440421683, Test Loss: 29.377565383911133\n",
      "Epoch [100/100], Train Loss: 24.355039827940892, Test Loss: 28.390226364135742\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.277234093087618, Test Loss: 29.97985076904297\n",
      "Epoch [20/100], Train Loss: 25.974258035128234, Test Loss: 42.55192947387695\n",
      "Epoch [30/100], Train Loss: 26.032678447785926, Test Loss: 34.19792556762695\n",
      "Epoch [40/100], Train Loss: 25.706186688532593, Test Loss: 29.259641647338867\n",
      "Epoch [50/100], Train Loss: 27.84441442645964, Test Loss: 29.781097412109375\n",
      "Epoch [60/100], Train Loss: 25.456647910446417, Test Loss: 37.603004455566406\n",
      "Epoch [70/100], Train Loss: 25.26239917942735, Test Loss: 30.692211151123047\n",
      "Epoch [80/100], Train Loss: 24.124880437381933, Test Loss: 28.382225036621094\n",
      "Epoch [90/100], Train Loss: 23.95267659171683, Test Loss: 28.704891204833984\n",
      "Epoch [100/100], Train Loss: 24.510796868996543, Test Loss: 29.90849494934082\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 28.378172014580397, Test Loss: 36.450225830078125\n",
      "Epoch [20/100], Train Loss: 28.44828100986168, Test Loss: 33.63441848754883\n",
      "Epoch [30/100], Train Loss: 28.2410749966981, Test Loss: 28.481698989868164\n",
      "Epoch [40/100], Train Loss: 25.851817052872455, Test Loss: 30.292327880859375\n",
      "Epoch [50/100], Train Loss: 26.376281037877817, Test Loss: 29.910907745361328\n",
      "Epoch [60/100], Train Loss: 25.78067406826332, Test Loss: 30.994977951049805\n",
      "Epoch [70/100], Train Loss: 24.994105379698706, Test Loss: 28.082963943481445\n",
      "Epoch [80/100], Train Loss: 26.06204623237985, Test Loss: 29.094552993774414\n",
      "Epoch [90/100], Train Loss: 24.704600224729443, Test Loss: 27.85040855407715\n",
      "Epoch [100/100], Train Loss: 25.77142508459873, Test Loss: 30.221891403198242\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 31.27664851829654, Test Loss: 35.741859436035156\n",
      "Epoch [20/100], Train Loss: 28.42491111130011, Test Loss: 32.248199462890625\n",
      "Epoch [30/100], Train Loss: 26.87620336814005, Test Loss: 30.551380157470703\n",
      "Epoch [40/100], Train Loss: 26.552921645367732, Test Loss: 30.88389778137207\n",
      "Epoch [50/100], Train Loss: 26.12572027737977, Test Loss: 32.414859771728516\n",
      "Epoch [60/100], Train Loss: 25.889621940988008, Test Loss: 30.138662338256836\n",
      "Epoch [70/100], Train Loss: 26.666766388689886, Test Loss: 30.656518936157227\n",
      "Epoch [80/100], Train Loss: 24.637130918659146, Test Loss: 29.581951141357422\n",
      "Epoch [90/100], Train Loss: 25.32542444448002, Test Loss: 31.423879623413086\n",
      "Epoch [100/100], Train Loss: 25.637211783987578, Test Loss: 29.760005950927734\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 27.15060022072714, Test Loss: 35.44330596923828\n",
      "Epoch [20/100], Train Loss: 28.700776859971345, Test Loss: 29.230493545532227\n",
      "Epoch [30/100], Train Loss: 26.625655602627113, Test Loss: 33.031715393066406\n",
      "Epoch [40/100], Train Loss: 27.709431563830766, Test Loss: 42.67192077636719\n",
      "Epoch [50/100], Train Loss: 26.61057475981165, Test Loss: 35.01948547363281\n",
      "Epoch [60/100], Train Loss: 25.16130204747935, Test Loss: 31.001779556274414\n",
      "Epoch [70/100], Train Loss: 25.53882583868308, Test Loss: 30.2285099029541\n",
      "Epoch [80/100], Train Loss: 24.67006967888504, Test Loss: 29.219364166259766\n",
      "Epoch [90/100], Train Loss: 26.338476549992794, Test Loss: 28.689132690429688\n",
      "Epoch [100/100], Train Loss: 25.13792062352915, Test Loss: 28.469806671142578\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 30.67569589458528, Test Loss: 36.805686950683594\n",
      "Epoch [20/100], Train Loss: 28.4093228574659, Test Loss: 35.341209411621094\n",
      "Epoch [30/100], Train Loss: 27.251348370411357, Test Loss: 42.031593322753906\n",
      "Epoch [40/100], Train Loss: 26.93089949185731, Test Loss: 36.0238037109375\n",
      "Epoch [50/100], Train Loss: 27.552618745897636, Test Loss: 34.899932861328125\n",
      "Epoch [60/100], Train Loss: 26.43683708691206, Test Loss: 31.410057067871094\n",
      "Epoch [70/100], Train Loss: 26.143114852905274, Test Loss: 31.676847457885742\n",
      "Epoch [80/100], Train Loss: 27.461244101602524, Test Loss: 31.597814559936523\n",
      "Epoch [90/100], Train Loss: 28.125806602102813, Test Loss: 38.99671936035156\n",
      "Epoch [100/100], Train Loss: 25.701854274312005, Test Loss: 29.83898162841797\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 29.42350646472368, Test Loss: 36.35502624511719\n",
      "Epoch [20/100], Train Loss: 28.9092927151039, Test Loss: 45.139747619628906\n",
      "Epoch [30/100], Train Loss: 29.386514751246718, Test Loss: 34.31290817260742\n",
      "Epoch [40/100], Train Loss: 26.56844297940614, Test Loss: 31.02149772644043\n",
      "Epoch [50/100], Train Loss: 27.202045521970653, Test Loss: 29.965839385986328\n",
      "Epoch [60/100], Train Loss: 26.94074535682553, Test Loss: 32.10450744628906\n",
      "Epoch [70/100], Train Loss: 26.430547051351578, Test Loss: 30.741018295288086\n",
      "Epoch [80/100], Train Loss: 26.31795231553375, Test Loss: 28.27984619140625\n",
      "Epoch [90/100], Train Loss: 25.93982660262311, Test Loss: 36.64061737060547\n",
      "Epoch [100/100], Train Loss: 25.99891293635134, Test Loss: 31.214887619018555\n",
      "Training model with hidden size 256, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 100\n",
      "Epoch [10/100], Train Loss: 31.897013217113056, Test Loss: 37.977787017822266\n",
      "Epoch [20/100], Train Loss: 29.672946617251537, Test Loss: 44.9268684387207\n",
      "Epoch [30/100], Train Loss: 28.30081286821209, Test Loss: 34.228431701660156\n",
      "Epoch [40/100], Train Loss: 27.07521382316214, Test Loss: 31.640499114990234\n",
      "Epoch [50/100], Train Loss: 26.25018222996446, Test Loss: 32.382144927978516\n",
      "Epoch [60/100], Train Loss: 27.26262041310795, Test Loss: 32.854949951171875\n",
      "Epoch [70/100], Train Loss: 27.145926334818856, Test Loss: 29.43714141845703\n",
      "Epoch [80/100], Train Loss: 26.48581690553759, Test Loss: 32.18183898925781\n",
      "Epoch [90/100], Train Loss: 25.708817291259766, Test Loss: 30.51025390625\n",
      "Epoch [100/100], Train Loss: 25.95861162279473, Test Loss: 29.019086837768555\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.113797372286434, Test Loss: 43.757041485278634\n",
      "Epoch [20/150], Train Loss: 41.66708155147365, Test Loss: 39.001968086539925\n",
      "Epoch [30/150], Train Loss: 37.60140464657643, Test Loss: 37.48222479882178\n",
      "Epoch [40/150], Train Loss: 34.636012505703285, Test Loss: 33.869209884049056\n",
      "Epoch [50/150], Train Loss: 32.16695975006604, Test Loss: 33.172145744422814\n",
      "Epoch [60/150], Train Loss: 29.427129614157757, Test Loss: 30.403306861976525\n",
      "Epoch [70/150], Train Loss: 27.675497399001824, Test Loss: 29.89893410422585\n",
      "Epoch [80/150], Train Loss: 27.83394500857494, Test Loss: 28.812232946420643\n",
      "Epoch [90/150], Train Loss: 26.930791560939102, Test Loss: 28.86216265195376\n",
      "Epoch [100/150], Train Loss: 26.234916743294136, Test Loss: 28.580688377479454\n",
      "Epoch [110/150], Train Loss: 24.32438437039735, Test Loss: 28.805637409160664\n",
      "Epoch [120/150], Train Loss: 23.896010789714875, Test Loss: 28.151837435635652\n",
      "Epoch [130/150], Train Loss: 23.824268072159565, Test Loss: 29.297849481756035\n",
      "Epoch [140/150], Train Loss: 23.370091363250232, Test Loss: 28.00875032102907\n",
      "Epoch [150/150], Train Loss: 23.936102482529936, Test Loss: 28.14634236422452\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.24934439737289, Test Loss: 41.72702174991756\n",
      "Epoch [20/150], Train Loss: 41.860780659659966, Test Loss: 40.322365054836524\n",
      "Epoch [30/150], Train Loss: 37.908683701812244, Test Loss: 36.90004279396751\n",
      "Epoch [40/150], Train Loss: 34.56321127219278, Test Loss: 34.575770415268934\n",
      "Epoch [50/150], Train Loss: 32.24855493013976, Test Loss: 33.056828486454954\n",
      "Epoch [60/150], Train Loss: 29.5547963126761, Test Loss: 32.52587974845589\n",
      "Epoch [70/150], Train Loss: 28.84367139222192, Test Loss: 29.65243307336584\n",
      "Epoch [80/150], Train Loss: 27.254321507938574, Test Loss: 28.7664179120745\n",
      "Epoch [90/150], Train Loss: 25.615781840340034, Test Loss: 28.214205704726183\n",
      "Epoch [100/150], Train Loss: 25.026518537177413, Test Loss: 28.759113113601487\n",
      "Epoch [110/150], Train Loss: 24.958359483812675, Test Loss: 28.493342833085492\n",
      "Epoch [120/150], Train Loss: 25.565842006245596, Test Loss: 28.469988686697825\n",
      "Epoch [130/150], Train Loss: 24.586273709281546, Test Loss: 28.581423747075068\n",
      "Epoch [140/150], Train Loss: 25.111892768984934, Test Loss: 28.157123862922965\n",
      "Epoch [150/150], Train Loss: 24.003949024638192, Test Loss: 28.112843798352525\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.639344962698516, Test Loss: 44.575501528653234\n",
      "Epoch [20/150], Train Loss: 42.304782066970574, Test Loss: 37.66241935630897\n",
      "Epoch [30/150], Train Loss: 38.6043610869861, Test Loss: 36.19852085856648\n",
      "Epoch [40/150], Train Loss: 35.3702133428855, Test Loss: 35.36755167973506\n",
      "Epoch [50/150], Train Loss: 32.60710816305192, Test Loss: 32.431968317403424\n",
      "Epoch [60/150], Train Loss: 30.10929739279825, Test Loss: 32.196844348659766\n",
      "Epoch [70/150], Train Loss: 28.737465442594935, Test Loss: 28.768130983625138\n",
      "Epoch [80/150], Train Loss: 27.544936595979284, Test Loss: 30.26699853872324\n",
      "Epoch [90/150], Train Loss: 26.481525683793866, Test Loss: 28.52769997832063\n",
      "Epoch [100/150], Train Loss: 24.89614605200095, Test Loss: 29.070560083760842\n",
      "Epoch [110/150], Train Loss: 24.677916436117204, Test Loss: 27.8312565939767\n",
      "Epoch [120/150], Train Loss: 24.247456794488624, Test Loss: 27.797211337399172\n",
      "Epoch [130/150], Train Loss: 24.103296564446122, Test Loss: 28.36310456015847\n",
      "Epoch [140/150], Train Loss: 23.947575603547644, Test Loss: 28.324460636485707\n",
      "Epoch [150/150], Train Loss: 24.214115561813603, Test Loss: 27.955344732705647\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 45.87903087178215, Test Loss: 43.00173568725586\n",
      "Epoch [20/150], Train Loss: 41.30340338534996, Test Loss: 40.527784620012554\n",
      "Epoch [30/150], Train Loss: 37.17511138916016, Test Loss: 37.591373939018744\n",
      "Epoch [40/150], Train Loss: 34.64200977262903, Test Loss: 32.968979501104975\n",
      "Epoch [50/150], Train Loss: 31.766973426693777, Test Loss: 31.941653239262568\n",
      "Epoch [60/150], Train Loss: 29.792046418737193, Test Loss: 32.32493348555131\n",
      "Epoch [70/150], Train Loss: 28.139478427073996, Test Loss: 29.232207013415053\n",
      "Epoch [80/150], Train Loss: 26.843944537053343, Test Loss: 28.369483947753906\n",
      "Epoch [90/150], Train Loss: 25.786985816330205, Test Loss: 28.918648187216228\n",
      "Epoch [100/150], Train Loss: 26.08030626265729, Test Loss: 27.74094532062481\n",
      "Epoch [110/150], Train Loss: 24.39398464140345, Test Loss: 28.08514020350072\n",
      "Epoch [120/150], Train Loss: 25.010972876626937, Test Loss: 27.68353945249087\n",
      "Epoch [130/150], Train Loss: 25.133140417005194, Test Loss: 27.59205996525752\n",
      "Epoch [140/150], Train Loss: 23.3928141265619, Test Loss: 27.11960316942884\n",
      "Epoch [150/150], Train Loss: 23.807144565269596, Test Loss: 28.029047458202808\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.15560337754547, Test Loss: 43.9527026089755\n",
      "Epoch [20/150], Train Loss: 41.668182948378266, Test Loss: 39.2800303867885\n",
      "Epoch [30/150], Train Loss: 37.46140880897397, Test Loss: 36.13309097290039\n",
      "Epoch [40/150], Train Loss: 34.149276758412846, Test Loss: 35.87443532572164\n",
      "Epoch [50/150], Train Loss: 31.775387498198963, Test Loss: 31.486760176621473\n",
      "Epoch [60/150], Train Loss: 28.669006328895446, Test Loss: 31.219749648849685\n",
      "Epoch [70/150], Train Loss: 28.83290410276319, Test Loss: 30.255746023995535\n",
      "Epoch [80/150], Train Loss: 26.24143596086346, Test Loss: 28.352957093870486\n",
      "Epoch [90/150], Train Loss: 26.221279951001776, Test Loss: 28.736088071550643\n",
      "Epoch [100/150], Train Loss: 24.88293176869877, Test Loss: 27.719809643633955\n",
      "Epoch [110/150], Train Loss: 25.14324821722312, Test Loss: 27.505710849514255\n",
      "Epoch [120/150], Train Loss: 23.915707872734696, Test Loss: 27.37315913609096\n",
      "Epoch [130/150], Train Loss: 23.661032848670835, Test Loss: 26.98690245987533\n",
      "Epoch [140/150], Train Loss: 23.355263000238136, Test Loss: 26.193481470083263\n",
      "Epoch [150/150], Train Loss: 25.215626432074874, Test Loss: 26.89881401557427\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.66689353692727, Test Loss: 43.11349611158495\n",
      "Epoch [20/150], Train Loss: 42.29866496852187, Test Loss: 40.81028598934025\n",
      "Epoch [30/150], Train Loss: 38.741169338538995, Test Loss: 39.330455581863205\n",
      "Epoch [40/150], Train Loss: 34.86923940689837, Test Loss: 35.982373968347325\n",
      "Epoch [50/150], Train Loss: 33.50015385111824, Test Loss: 33.32728254640257\n",
      "Epoch [60/150], Train Loss: 30.622150833880315, Test Loss: 30.368870301680133\n",
      "Epoch [70/150], Train Loss: 28.417084678274687, Test Loss: 30.89207664093414\n",
      "Epoch [80/150], Train Loss: 27.349515008144692, Test Loss: 29.493963613138572\n",
      "Epoch [90/150], Train Loss: 26.050674207093287, Test Loss: 28.673807094623516\n",
      "Epoch [100/150], Train Loss: 24.925072335415198, Test Loss: 28.50196288468002\n",
      "Epoch [110/150], Train Loss: 24.98050729720319, Test Loss: 28.708754799582742\n",
      "Epoch [120/150], Train Loss: 23.988550254946848, Test Loss: 28.241384580537872\n",
      "Epoch [130/150], Train Loss: 24.06212787315494, Test Loss: 28.181462151663645\n",
      "Epoch [140/150], Train Loss: 24.76067472989442, Test Loss: 27.982860342248692\n",
      "Epoch [150/150], Train Loss: 23.059628677368163, Test Loss: 28.40822814346908\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 45.89329944047771, Test Loss: 44.293054952249896\n",
      "Epoch [20/150], Train Loss: 41.61584385105821, Test Loss: 41.552600414721994\n",
      "Epoch [30/150], Train Loss: 37.5769226824651, Test Loss: 37.00439472941609\n",
      "Epoch [40/150], Train Loss: 34.89322883731029, Test Loss: 34.85649638981014\n",
      "Epoch [50/150], Train Loss: 31.8036931585093, Test Loss: 33.51881468141234\n",
      "Epoch [60/150], Train Loss: 29.95589871641065, Test Loss: 31.075756221622616\n",
      "Epoch [70/150], Train Loss: 28.929326848514744, Test Loss: 29.901204443597173\n",
      "Epoch [80/150], Train Loss: 26.461358798918177, Test Loss: 29.707095703521333\n",
      "Epoch [90/150], Train Loss: 25.78540759477459, Test Loss: 29.087273461478098\n",
      "Epoch [100/150], Train Loss: 25.911978931114323, Test Loss: 28.80050483307281\n",
      "Epoch [110/150], Train Loss: 25.418281505146965, Test Loss: 27.532894258375293\n",
      "Epoch [120/150], Train Loss: 25.017559758170705, Test Loss: 28.422677646983754\n",
      "Epoch [130/150], Train Loss: 24.837490188098343, Test Loss: 27.631723230535332\n",
      "Epoch [140/150], Train Loss: 23.96473247340468, Test Loss: 28.479642917583515\n",
      "Epoch [150/150], Train Loss: 23.364223855440734, Test Loss: 27.8970064683394\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 45.613456963711094, Test Loss: 40.742714374096366\n",
      "Epoch [20/150], Train Loss: 41.2981140887151, Test Loss: 39.122990843537565\n",
      "Epoch [30/150], Train Loss: 36.9866361524238, Test Loss: 36.25431870794915\n",
      "Epoch [40/150], Train Loss: 34.04558840892354, Test Loss: 32.82797605341131\n",
      "Epoch [50/150], Train Loss: 31.092576480302654, Test Loss: 31.68000570520178\n",
      "Epoch [60/150], Train Loss: 29.12577111916464, Test Loss: 31.222026750638886\n",
      "Epoch [70/150], Train Loss: 27.938361471207415, Test Loss: 29.380897868763316\n",
      "Epoch [80/150], Train Loss: 26.43605300403032, Test Loss: 28.869521252520673\n",
      "Epoch [90/150], Train Loss: 26.5457977607602, Test Loss: 28.68187966284814\n",
      "Epoch [100/150], Train Loss: 24.781521456358863, Test Loss: 29.374478525929636\n",
      "Epoch [110/150], Train Loss: 24.9051567828069, Test Loss: 27.610573434210444\n",
      "Epoch [120/150], Train Loss: 23.781069921274653, Test Loss: 27.752625948422914\n",
      "Epoch [130/150], Train Loss: 24.024833791764056, Test Loss: 27.003853958922548\n",
      "Epoch [140/150], Train Loss: 23.979407576263927, Test Loss: 27.385649235217603\n",
      "Epoch [150/150], Train Loss: 24.955825055231813, Test Loss: 27.75278916297021\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.48605149065862, Test Loss: 44.22381294547738\n",
      "Epoch [20/150], Train Loss: 42.119859776731396, Test Loss: 39.52293326637962\n",
      "Epoch [30/150], Train Loss: 38.33225631713867, Test Loss: 38.701946010837304\n",
      "Epoch [40/150], Train Loss: 35.164889632678424, Test Loss: 36.67715260889623\n",
      "Epoch [50/150], Train Loss: 32.77933764848553, Test Loss: 33.22762655282949\n",
      "Epoch [60/150], Train Loss: 30.477560900078444, Test Loss: 31.00406924161044\n",
      "Epoch [70/150], Train Loss: 28.27228602424997, Test Loss: 30.28390795224673\n",
      "Epoch [80/150], Train Loss: 27.024248179451362, Test Loss: 29.025995799473353\n",
      "Epoch [90/150], Train Loss: 25.975633596201412, Test Loss: 29.058297962337345\n",
      "Epoch [100/150], Train Loss: 25.44577287767754, Test Loss: 28.735237344518886\n",
      "Epoch [110/150], Train Loss: 25.13247109710193, Test Loss: 28.53021190692852\n",
      "Epoch [120/150], Train Loss: 25.22274284988153, Test Loss: 28.579471910154666\n",
      "Epoch [130/150], Train Loss: 24.680054386326525, Test Loss: 28.16627207669345\n",
      "Epoch [140/150], Train Loss: 24.946841399396053, Test Loss: 28.21642355485396\n",
      "Epoch [150/150], Train Loss: 24.982550286465003, Test Loss: 28.085034457120027\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.525921762185018, Test Loss: 29.913758562756822\n",
      "Epoch [20/150], Train Loss: 27.387434137062947, Test Loss: 28.6373503796466\n",
      "Epoch [30/150], Train Loss: 25.594718207687627, Test Loss: 28.87363391727596\n",
      "Epoch [40/150], Train Loss: 25.897184690881947, Test Loss: 31.586393009532582\n",
      "Epoch [50/150], Train Loss: 24.429415161883245, Test Loss: 28.106186383730407\n",
      "Epoch [60/150], Train Loss: 24.56885043284932, Test Loss: 33.34399480943556\n",
      "Epoch [70/150], Train Loss: 24.428563890300815, Test Loss: 28.26946815887055\n",
      "Epoch [80/150], Train Loss: 24.718330327018364, Test Loss: 27.934988789744192\n",
      "Epoch [90/150], Train Loss: 25.189915710199074, Test Loss: 28.12145384255942\n",
      "Epoch [100/150], Train Loss: 23.78392271448354, Test Loss: 29.658075109704747\n",
      "Epoch [110/150], Train Loss: 24.295015960443216, Test Loss: 29.741667858965986\n",
      "Epoch [120/150], Train Loss: 24.48674192584929, Test Loss: 27.634056636265345\n",
      "Epoch [130/150], Train Loss: 23.545998094902664, Test Loss: 28.245573093364765\n",
      "Epoch [140/150], Train Loss: 22.728135040158133, Test Loss: 30.48853363928857\n",
      "Epoch [150/150], Train Loss: 22.494010196748327, Test Loss: 28.90446444920131\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.295773065285605, Test Loss: 30.452353291697317\n",
      "Epoch [20/150], Train Loss: 26.618331377623512, Test Loss: 28.943710797792907\n",
      "Epoch [30/150], Train Loss: 24.7185655374996, Test Loss: 29.168489109386098\n",
      "Epoch [40/150], Train Loss: 25.189879132880538, Test Loss: 27.90092926520806\n",
      "Epoch [50/150], Train Loss: 25.100039022476945, Test Loss: 28.048187924669936\n",
      "Epoch [60/150], Train Loss: 23.910858491991387, Test Loss: 29.226450337992087\n",
      "Epoch [70/150], Train Loss: 24.665759358640578, Test Loss: 28.243735994611466\n",
      "Epoch [80/150], Train Loss: 23.86671145704926, Test Loss: 28.524684261966062\n",
      "Epoch [90/150], Train Loss: 23.479305823904568, Test Loss: 29.899887729000735\n",
      "Epoch [100/150], Train Loss: 25.387720839703668, Test Loss: 28.948678648317014\n",
      "Epoch [110/150], Train Loss: 24.231374646796557, Test Loss: 29.437531681803915\n",
      "Epoch [120/150], Train Loss: 23.906703354882413, Test Loss: 29.403610378116756\n",
      "Epoch [130/150], Train Loss: 23.26589168486048, Test Loss: 29.88809806031066\n",
      "Epoch [140/150], Train Loss: 22.701641314146947, Test Loss: 28.974159191181133\n",
      "Epoch [150/150], Train Loss: 23.38290102599097, Test Loss: 29.194143295288086\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.059465670976483, Test Loss: 29.884589306719892\n",
      "Epoch [20/150], Train Loss: 27.039619570872823, Test Loss: 28.840834927249265\n",
      "Epoch [30/150], Train Loss: 27.12133524535132, Test Loss: 28.815639768327987\n",
      "Epoch [40/150], Train Loss: 26.758856982872135, Test Loss: 27.984815622304943\n",
      "Epoch [50/150], Train Loss: 27.185025693549484, Test Loss: 27.977399355405336\n",
      "Epoch [60/150], Train Loss: 24.232632896548413, Test Loss: 28.45895140511649\n",
      "Epoch [70/150], Train Loss: 25.997162428058562, Test Loss: 28.33700856295499\n",
      "Epoch [80/150], Train Loss: 24.817575548515943, Test Loss: 28.151031518911388\n",
      "Epoch [90/150], Train Loss: 24.95939257887543, Test Loss: 31.877441158542386\n",
      "Epoch [100/150], Train Loss: 24.692940883949156, Test Loss: 27.553404845200575\n",
      "Epoch [110/150], Train Loss: 24.094622264924595, Test Loss: 28.41902207708978\n",
      "Epoch [120/150], Train Loss: 24.822407206550974, Test Loss: 28.820700905539773\n",
      "Epoch [130/150], Train Loss: 23.271201211898052, Test Loss: 28.715225913307883\n",
      "Epoch [140/150], Train Loss: 24.863551680768122, Test Loss: 27.961551666259766\n",
      "Epoch [150/150], Train Loss: 24.251786091288583, Test Loss: 28.221320734395608\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.396646330786535, Test Loss: 29.26965366710316\n",
      "Epoch [20/150], Train Loss: 26.675916321551213, Test Loss: 28.78081324193385\n",
      "Epoch [30/150], Train Loss: 26.936010845371936, Test Loss: 28.19746631770939\n",
      "Epoch [40/150], Train Loss: 25.489186815355644, Test Loss: 28.479414159601387\n",
      "Epoch [50/150], Train Loss: 24.80294831698058, Test Loss: 27.143192167405957\n",
      "Epoch [60/150], Train Loss: 25.863016547531377, Test Loss: 28.045065669270304\n",
      "Epoch [70/150], Train Loss: 25.310410746590037, Test Loss: 26.948250361851283\n",
      "Epoch [80/150], Train Loss: 25.12556993453229, Test Loss: 27.977541911137568\n",
      "Epoch [90/150], Train Loss: 25.522084345583057, Test Loss: 27.499504510458415\n",
      "Epoch [100/150], Train Loss: 24.509019557765274, Test Loss: 28.84349201251934\n",
      "Epoch [110/150], Train Loss: 23.81477285916688, Test Loss: 27.686855390474395\n",
      "Epoch [120/150], Train Loss: 23.799756109519084, Test Loss: 29.252794191434784\n",
      "Epoch [130/150], Train Loss: 24.22829418807733, Test Loss: 31.6416563306536\n",
      "Epoch [140/150], Train Loss: 25.786212327050382, Test Loss: 28.037171301903662\n",
      "Epoch [150/150], Train Loss: 22.436240036761173, Test Loss: 28.681822169910777\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.614258888119558, Test Loss: 31.426294747885173\n",
      "Epoch [20/150], Train Loss: 27.391405387002912, Test Loss: 28.52391104264693\n",
      "Epoch [30/150], Train Loss: 26.065956390881148, Test Loss: 28.558625729053052\n",
      "Epoch [40/150], Train Loss: 25.980751875580335, Test Loss: 29.612368447440012\n",
      "Epoch [50/150], Train Loss: 26.673148858742636, Test Loss: 28.3528170152144\n",
      "Epoch [60/150], Train Loss: 24.95714357720047, Test Loss: 28.277624353185878\n",
      "Epoch [70/150], Train Loss: 24.569430260580095, Test Loss: 27.161712374005997\n",
      "Epoch [80/150], Train Loss: 23.581470508262758, Test Loss: 27.93796881143149\n",
      "Epoch [90/150], Train Loss: 25.76500346699699, Test Loss: 30.13512187809139\n",
      "Epoch [100/150], Train Loss: 24.822153435378777, Test Loss: 28.01983256154246\n",
      "Epoch [110/150], Train Loss: 24.722323308225537, Test Loss: 27.815663746425084\n",
      "Epoch [120/150], Train Loss: 24.202974463290854, Test Loss: 28.024649434275442\n",
      "Epoch [130/150], Train Loss: 24.398382874785877, Test Loss: 28.60952768697367\n",
      "Epoch [140/150], Train Loss: 22.190378345426964, Test Loss: 28.45610910886294\n",
      "Epoch [150/150], Train Loss: 23.87621201687172, Test Loss: 27.846654074532644\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.42165603637695, Test Loss: 28.696320075493354\n",
      "Epoch [20/150], Train Loss: 26.35541797075115, Test Loss: 30.534361331493823\n",
      "Epoch [30/150], Train Loss: 25.85018066656394, Test Loss: 28.314852107654918\n",
      "Epoch [40/150], Train Loss: 24.83409818430416, Test Loss: 27.980543830178\n",
      "Epoch [50/150], Train Loss: 25.988939691762454, Test Loss: 28.103846587143934\n",
      "Epoch [60/150], Train Loss: 25.23277525354604, Test Loss: 27.05369384567459\n",
      "Epoch [70/150], Train Loss: 25.4372088447946, Test Loss: 27.578576496669225\n",
      "Epoch [80/150], Train Loss: 25.909516525268554, Test Loss: 27.737492474642668\n",
      "Epoch [90/150], Train Loss: 25.931880594472418, Test Loss: 27.11689030040394\n",
      "Epoch [100/150], Train Loss: 25.652485813078332, Test Loss: 30.312436314372274\n",
      "Epoch [110/150], Train Loss: 25.90703062463979, Test Loss: 27.16872839494185\n",
      "Epoch [120/150], Train Loss: 25.223100537159404, Test Loss: 27.51988106269341\n",
      "Epoch [130/150], Train Loss: 25.335166649740252, Test Loss: 27.820011138916016\n",
      "Epoch [140/150], Train Loss: 26.0446726126749, Test Loss: 28.15139470781599\n",
      "Epoch [150/150], Train Loss: 24.241186160728578, Test Loss: 31.694463283984692\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.648836729956454, Test Loss: 29.992355569616542\n",
      "Epoch [20/150], Train Loss: 27.254701458039833, Test Loss: 51.699084938346566\n",
      "Epoch [30/150], Train Loss: 25.808383416347816, Test Loss: 29.837082528448725\n",
      "Epoch [40/150], Train Loss: 26.358538468157658, Test Loss: 28.613971536809746\n",
      "Epoch [50/150], Train Loss: 26.529008627719566, Test Loss: 29.57833698817662\n",
      "Epoch [60/150], Train Loss: 25.662660605008483, Test Loss: 27.89630553010222\n",
      "Epoch [70/150], Train Loss: 25.700470658599354, Test Loss: 29.220333619551226\n",
      "Epoch [80/150], Train Loss: 25.663680601901696, Test Loss: 29.397313997342987\n",
      "Epoch [90/150], Train Loss: 24.96797415311219, Test Loss: 30.027263046859147\n",
      "Epoch [100/150], Train Loss: 25.705343102627115, Test Loss: 28.64157451282848\n",
      "Epoch [110/150], Train Loss: 25.30722952045378, Test Loss: 28.183895111083984\n",
      "Epoch [120/150], Train Loss: 25.055814361572267, Test Loss: 29.13186690095183\n",
      "Epoch [130/150], Train Loss: 25.17458475456863, Test Loss: 27.829062573321455\n",
      "Epoch [140/150], Train Loss: 24.934352530807747, Test Loss: 27.649630930516626\n",
      "Epoch [150/150], Train Loss: 26.17802379170402, Test Loss: 28.283787343409156\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.619629256451717, Test Loss: 30.759476376818373\n",
      "Epoch [20/150], Train Loss: 26.93314245255267, Test Loss: 28.610788048087777\n",
      "Epoch [30/150], Train Loss: 26.34940068604516, Test Loss: 29.148590385139762\n",
      "Epoch [40/150], Train Loss: 26.05494507961586, Test Loss: 28.52085812060864\n",
      "Epoch [50/150], Train Loss: 26.18551067915119, Test Loss: 29.60571489705668\n",
      "Epoch [60/150], Train Loss: 26.421813608388433, Test Loss: 27.5486046184193\n",
      "Epoch [70/150], Train Loss: 25.246402071343095, Test Loss: 29.214560917445592\n",
      "Epoch [80/150], Train Loss: 24.036387190271597, Test Loss: 27.744348476459454\n",
      "Epoch [90/150], Train Loss: 24.91215413828365, Test Loss: 27.77428339673327\n",
      "Epoch [100/150], Train Loss: 24.63325210946505, Test Loss: 27.357598837319905\n",
      "Epoch [110/150], Train Loss: 26.0472734607634, Test Loss: 29.82643679829387\n",
      "Epoch [120/150], Train Loss: 26.428978291495902, Test Loss: 28.072220368818805\n",
      "Epoch [130/150], Train Loss: 25.011888447745903, Test Loss: 27.075269922033534\n",
      "Epoch [140/150], Train Loss: 24.83504898196361, Test Loss: 29.21896505975104\n",
      "Epoch [150/150], Train Loss: 24.657143777315735, Test Loss: 27.47562685879794\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.588197620579454, Test Loss: 31.197797478019417\n",
      "Epoch [20/150], Train Loss: 27.32959143216493, Test Loss: 29.988472529820033\n",
      "Epoch [30/150], Train Loss: 26.740998909121654, Test Loss: 28.613616423173383\n",
      "Epoch [40/150], Train Loss: 26.353093994640915, Test Loss: 29.095938843566103\n",
      "Epoch [50/150], Train Loss: 25.638151750408234, Test Loss: 28.709155714357053\n",
      "Epoch [60/150], Train Loss: 26.05912040335233, Test Loss: 28.163735575490183\n",
      "Epoch [70/150], Train Loss: 26.128492843127642, Test Loss: 29.30919431711172\n",
      "Epoch [80/150], Train Loss: 25.901456213779138, Test Loss: 27.984818223234896\n",
      "Epoch [90/150], Train Loss: 26.05300222928407, Test Loss: 27.370672869991946\n",
      "Epoch [100/150], Train Loss: 26.0395809173584, Test Loss: 28.731688165045405\n",
      "Epoch [110/150], Train Loss: 26.71342218117636, Test Loss: 27.195178490180474\n",
      "Epoch [120/150], Train Loss: 25.66227409487865, Test Loss: 27.750877206975762\n",
      "Epoch [130/150], Train Loss: 24.947236633300783, Test Loss: 29.248330202969637\n",
      "Epoch [140/150], Train Loss: 24.806897673059684, Test Loss: 27.721987365128157\n",
      "Epoch [150/150], Train Loss: 24.511868586305713, Test Loss: 28.098858796156847\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.444274189433113, Test Loss: 28.789845751477525\n",
      "Epoch [20/150], Train Loss: 27.49927792158283, Test Loss: 30.17509465403371\n",
      "Epoch [30/150], Train Loss: 27.099762444417983, Test Loss: 26.906424534785284\n",
      "Epoch [40/150], Train Loss: 26.379431102314932, Test Loss: 27.833639120126698\n",
      "Epoch [50/150], Train Loss: 27.346683802370165, Test Loss: 28.33462261843991\n",
      "Epoch [60/150], Train Loss: 25.032718846055328, Test Loss: 29.800099162312296\n",
      "Epoch [70/150], Train Loss: 25.057307252727572, Test Loss: 30.647239387809456\n",
      "Epoch [80/150], Train Loss: 25.12339476288342, Test Loss: 28.989610300435647\n",
      "Epoch [90/150], Train Loss: 25.994234097590212, Test Loss: 28.960680280412948\n",
      "Epoch [100/150], Train Loss: 24.486616090868342, Test Loss: 27.74576727136389\n",
      "Epoch [110/150], Train Loss: 24.754969337338306, Test Loss: 27.54524530683245\n",
      "Epoch [120/150], Train Loss: 24.83450897091725, Test Loss: 29.42216546194894\n",
      "Epoch [130/150], Train Loss: 25.3069311861132, Test Loss: 27.937680603621843\n",
      "Epoch [140/150], Train Loss: 24.202397212044136, Test Loss: 27.864260983157468\n",
      "Epoch [150/150], Train Loss: 22.892970394697347, Test Loss: 28.555910110473633\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.26568916821089, Test Loss: 34.42648592862216\n",
      "Epoch [20/150], Train Loss: 26.993017234176886, Test Loss: 27.795264826192483\n",
      "Epoch [30/150], Train Loss: 25.51867118585305, Test Loss: 29.798467586566876\n",
      "Epoch [40/150], Train Loss: 25.514859802996526, Test Loss: 27.709765198942904\n",
      "Epoch [50/150], Train Loss: 26.31925337744541, Test Loss: 27.792765307736087\n",
      "Epoch [60/150], Train Loss: 26.32771342543305, Test Loss: 27.35431346645603\n",
      "Epoch [70/150], Train Loss: 24.998627578235062, Test Loss: 27.68429810660226\n",
      "Epoch [80/150], Train Loss: 26.49789053494813, Test Loss: 27.024117928046685\n",
      "Epoch [90/150], Train Loss: 24.407945439072908, Test Loss: 28.112096489249886\n",
      "Epoch [100/150], Train Loss: 25.540664072505763, Test Loss: 27.341012335442876\n",
      "Epoch [110/150], Train Loss: 24.179381261106396, Test Loss: 29.77145497210614\n",
      "Epoch [120/150], Train Loss: 23.91726483829686, Test Loss: 27.517701409079812\n",
      "Epoch [130/150], Train Loss: 24.70462945093874, Test Loss: 28.108801011915332\n",
      "Epoch [140/150], Train Loss: 23.609370547435322, Test Loss: 27.997101102556503\n",
      "Epoch [150/150], Train Loss: 24.542961883544923, Test Loss: 28.64184976552988\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.30433566609367, Test Loss: 29.440071328893886\n",
      "Epoch [20/150], Train Loss: 27.576955038602236, Test Loss: 28.790822363519048\n",
      "Epoch [30/150], Train Loss: 27.713000450759637, Test Loss: 28.819771531340365\n",
      "Epoch [40/150], Train Loss: 26.496827810318745, Test Loss: 28.551316992028966\n",
      "Epoch [50/150], Train Loss: 25.969853535636528, Test Loss: 31.11838811403745\n",
      "Epoch [60/150], Train Loss: 27.003516963270844, Test Loss: 29.245484736058618\n",
      "Epoch [70/150], Train Loss: 25.403449611976498, Test Loss: 28.631877899169922\n",
      "Epoch [80/150], Train Loss: 25.99983435458824, Test Loss: 27.738318728162096\n",
      "Epoch [90/150], Train Loss: 25.11022500210121, Test Loss: 28.166086816168452\n",
      "Epoch [100/150], Train Loss: 25.112376416315797, Test Loss: 27.671638117208108\n",
      "Epoch [110/150], Train Loss: 26.32698040321225, Test Loss: 29.307227345256063\n",
      "Epoch [120/150], Train Loss: 24.747137569990315, Test Loss: 34.25413565202193\n",
      "Epoch [130/150], Train Loss: 25.409484463050717, Test Loss: 27.285346043574346\n",
      "Epoch [140/150], Train Loss: 24.752132690929976, Test Loss: 27.396299931910132\n",
      "Epoch [150/150], Train Loss: 23.710493175318984, Test Loss: 27.5195917401995\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.064371215320023, Test Loss: 33.04892705942129\n",
      "Epoch [20/150], Train Loss: 26.756936232770077, Test Loss: 29.26484152558562\n",
      "Epoch [30/150], Train Loss: 26.86272268451628, Test Loss: 29.679794955563235\n",
      "Epoch [40/150], Train Loss: 25.58136932498119, Test Loss: 30.256223158402875\n",
      "Epoch [50/150], Train Loss: 25.844477525304576, Test Loss: 29.94934790475028\n",
      "Epoch [60/150], Train Loss: 25.895985787813782, Test Loss: 32.649618495594375\n",
      "Epoch [70/150], Train Loss: 25.968421460761398, Test Loss: 30.087275938554242\n",
      "Epoch [80/150], Train Loss: 24.904957055263832, Test Loss: 28.982049842933556\n",
      "Epoch [90/150], Train Loss: 26.725337869612897, Test Loss: 28.110873804463967\n",
      "Epoch [100/150], Train Loss: 27.05009361016946, Test Loss: 29.188653722985997\n",
      "Epoch [110/150], Train Loss: 25.684916862112576, Test Loss: 28.841343570065188\n",
      "Epoch [120/150], Train Loss: 25.996500402982118, Test Loss: 27.65956804349825\n",
      "Epoch [130/150], Train Loss: 26.150602009257334, Test Loss: 29.127245667692904\n",
      "Epoch [140/150], Train Loss: 25.4506715305516, Test Loss: 28.32266701041878\n",
      "Epoch [150/150], Train Loss: 24.96907476831655, Test Loss: 27.047994588876698\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.848740899758262, Test Loss: 31.84986590100573\n",
      "Epoch [20/150], Train Loss: 27.152907055714092, Test Loss: 32.856943576366874\n",
      "Epoch [30/150], Train Loss: 26.42742902099109, Test Loss: 32.487073179963346\n",
      "Epoch [40/150], Train Loss: 27.10014404547019, Test Loss: 30.433977820656516\n",
      "Epoch [50/150], Train Loss: 25.686947631835938, Test Loss: 27.293208555741742\n",
      "Epoch [60/150], Train Loss: 25.20243655345479, Test Loss: 27.64839306125393\n",
      "Epoch [70/150], Train Loss: 26.01448956473929, Test Loss: 27.473830681342584\n",
      "Epoch [80/150], Train Loss: 25.77622634387407, Test Loss: 27.827846626182655\n",
      "Epoch [90/150], Train Loss: 25.056326844262294, Test Loss: 28.22398938761129\n",
      "Epoch [100/150], Train Loss: 24.506805638797946, Test Loss: 27.615183743563566\n",
      "Epoch [110/150], Train Loss: 25.645928804991676, Test Loss: 28.667999936388686\n",
      "Epoch [120/150], Train Loss: 26.14301199991195, Test Loss: 26.556886846368965\n",
      "Epoch [130/150], Train Loss: 25.179654484107846, Test Loss: 27.12212309899268\n",
      "Epoch [140/150], Train Loss: 24.73787679203221, Test Loss: 28.371603086397247\n",
      "Epoch [150/150], Train Loss: 25.528227233886717, Test Loss: 27.562102429278486\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.060954209624743, Test Loss: 30.663854747623592\n",
      "Epoch [20/150], Train Loss: 27.15312506253602, Test Loss: 28.558949581988447\n",
      "Epoch [30/150], Train Loss: 26.676987219638512, Test Loss: 30.84484030983665\n",
      "Epoch [40/150], Train Loss: 26.14576636455098, Test Loss: 45.175954298539594\n",
      "Epoch [50/150], Train Loss: 25.42122212394339, Test Loss: 27.834667552601207\n",
      "Epoch [60/150], Train Loss: 26.06794831322842, Test Loss: 26.82917243164855\n",
      "Epoch [70/150], Train Loss: 25.62527051206495, Test Loss: 27.256215529008344\n",
      "Epoch [80/150], Train Loss: 25.553002047929606, Test Loss: 26.786288719672662\n",
      "Epoch [90/150], Train Loss: 26.25162057720247, Test Loss: 27.734031379996956\n",
      "Epoch [100/150], Train Loss: 25.54262930447938, Test Loss: 28.290715477683328\n",
      "Epoch [110/150], Train Loss: 26.921185540371255, Test Loss: 27.530273387958477\n",
      "Epoch [120/150], Train Loss: 25.677595682613184, Test Loss: 27.18744976489575\n",
      "Epoch [130/150], Train Loss: 25.42467710151047, Test Loss: 26.61962937689447\n",
      "Epoch [140/150], Train Loss: 25.85453182908355, Test Loss: 27.482182688527292\n",
      "Epoch [150/150], Train Loss: 25.036072752905675, Test Loss: 27.966950775740983\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.18543134595527, Test Loss: 31.08323047687481\n",
      "Epoch [20/150], Train Loss: 27.758454463520987, Test Loss: 32.98816983111493\n",
      "Epoch [30/150], Train Loss: 26.53689582699635, Test Loss: 33.40611381035347\n",
      "Epoch [40/150], Train Loss: 27.20451897793129, Test Loss: 29.105127780468433\n",
      "Epoch [50/150], Train Loss: 27.197872030539592, Test Loss: 28.928967686442586\n",
      "Epoch [60/150], Train Loss: 26.90607657510726, Test Loss: 32.2243717243145\n",
      "Epoch [70/150], Train Loss: 26.675993415957592, Test Loss: 31.1231344891833\n",
      "Epoch [80/150], Train Loss: 26.315992624251567, Test Loss: 28.74617266964603\n",
      "Epoch [90/150], Train Loss: 27.320750289666847, Test Loss: 28.22926020931888\n",
      "Epoch [100/150], Train Loss: 27.836211707943775, Test Loss: 28.392034902201072\n",
      "Epoch [110/150], Train Loss: 25.800019361152025, Test Loss: 28.48443301312335\n",
      "Epoch [120/150], Train Loss: 25.497714039536774, Test Loss: 33.14957645961216\n",
      "Epoch [130/150], Train Loss: 25.41826147485952, Test Loss: 31.304866617376153\n",
      "Epoch [140/150], Train Loss: 25.82740328429175, Test Loss: 30.709457224065606\n",
      "Epoch [150/150], Train Loss: 27.824147246313878, Test Loss: 31.397387170172358\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.75154223832928, Test Loss: 35.27671625707057\n",
      "Epoch [20/150], Train Loss: 28.530299102282914, Test Loss: 30.554567881992885\n",
      "Epoch [30/150], Train Loss: 27.400325625060034, Test Loss: 30.096970273302748\n",
      "Epoch [40/150], Train Loss: 27.448499085473234, Test Loss: 29.709663440654804\n",
      "Epoch [50/150], Train Loss: 26.24782666065654, Test Loss: 29.662989306759524\n",
      "Epoch [60/150], Train Loss: 27.040399995397348, Test Loss: 29.52011806933911\n",
      "Epoch [70/150], Train Loss: 26.706607849871524, Test Loss: 29.732477856920912\n",
      "Epoch [80/150], Train Loss: 27.504941727685146, Test Loss: 28.51369719071822\n",
      "Epoch [90/150], Train Loss: 26.889104624263577, Test Loss: 30.47133631520457\n",
      "Epoch [100/150], Train Loss: 25.46284640577973, Test Loss: 28.49866837340516\n",
      "Epoch [110/150], Train Loss: 25.869319277904072, Test Loss: 28.280023475746056\n",
      "Epoch [120/150], Train Loss: 26.79700643820841, Test Loss: 31.895217647800198\n",
      "Epoch [130/150], Train Loss: 25.60231014314245, Test Loss: 29.13872428993126\n",
      "Epoch [140/150], Train Loss: 25.314813864035685, Test Loss: 28.954092793650442\n",
      "Epoch [150/150], Train Loss: 26.091250885510053, Test Loss: 30.481002262660436\n",
      "Training model with hidden size 64, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.01033953682321, Test Loss: 32.875870271162555\n",
      "Epoch [20/150], Train Loss: 26.539361709844872, Test Loss: 30.078240976705178\n",
      "Epoch [30/150], Train Loss: 26.906206937696112, Test Loss: 29.722152784273224\n",
      "Epoch [40/150], Train Loss: 26.807836620143203, Test Loss: 28.582936398394697\n",
      "Epoch [50/150], Train Loss: 26.29069640988209, Test Loss: 30.386423977938566\n",
      "Epoch [60/150], Train Loss: 26.544032737857005, Test Loss: 29.54028724075912\n",
      "Epoch [70/150], Train Loss: 25.537116591656794, Test Loss: 29.407808130437676\n",
      "Epoch [80/150], Train Loss: 25.956245341066456, Test Loss: 28.8014622477742\n",
      "Epoch [90/150], Train Loss: 26.537572291639986, Test Loss: 30.2118886724695\n",
      "Epoch [100/150], Train Loss: 26.101308872660653, Test Loss: 32.17493505601759\n",
      "Epoch [110/150], Train Loss: 25.40675166395844, Test Loss: 28.477960314069474\n",
      "Epoch [120/150], Train Loss: 25.74039185320745, Test Loss: 28.53650169867974\n",
      "Epoch [130/150], Train Loss: 26.6936960439213, Test Loss: 28.32040311144544\n",
      "Epoch [140/150], Train Loss: 26.119944950791655, Test Loss: 28.804059313489244\n",
      "Epoch [150/150], Train Loss: 25.89375735423604, Test Loss: 28.885002507791892\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.04224098080494, Test Loss: 46.94412013462612\n",
      "Epoch [20/150], Train Loss: 45.527324451384, Test Loss: 41.92787234814136\n",
      "Epoch [30/150], Train Loss: 43.10170419411581, Test Loss: 40.38979141433518\n",
      "Epoch [40/150], Train Loss: 40.91481040579374, Test Loss: 37.93000689419833\n",
      "Epoch [50/150], Train Loss: 38.574365847228, Test Loss: 37.66835839407785\n",
      "Epoch [60/150], Train Loss: 36.54319788198002, Test Loss: 35.497552772621056\n",
      "Epoch [70/150], Train Loss: 34.81200777272709, Test Loss: 33.59811963663473\n",
      "Epoch [80/150], Train Loss: 32.7793225022613, Test Loss: 32.89299541324764\n",
      "Epoch [90/150], Train Loss: 31.469371545510214, Test Loss: 34.035112257127636\n",
      "Epoch [100/150], Train Loss: 29.94408374848913, Test Loss: 30.65589917170537\n",
      "Epoch [110/150], Train Loss: 28.85842881749888, Test Loss: 30.342859738832946\n",
      "Epoch [120/150], Train Loss: 27.592971301469646, Test Loss: 29.56353353525137\n",
      "Epoch [130/150], Train Loss: 26.4222430119749, Test Loss: 29.526133623990145\n",
      "Epoch [140/150], Train Loss: 25.409651077770796, Test Loss: 28.931759598967318\n",
      "Epoch [150/150], Train Loss: 25.318277290219168, Test Loss: 28.09338193125539\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.8382975218726, Test Loss: 44.924398645178066\n",
      "Epoch [20/150], Train Loss: 45.258163802350154, Test Loss: 43.99626565908457\n",
      "Epoch [30/150], Train Loss: 42.82266309144067, Test Loss: 42.47283524352235\n",
      "Epoch [40/150], Train Loss: 40.39315470711129, Test Loss: 38.00600106375558\n",
      "Epoch [50/150], Train Loss: 38.176953625288164, Test Loss: 37.744912828717915\n",
      "Epoch [60/150], Train Loss: 36.39643996191806, Test Loss: 35.09549111205262\n",
      "Epoch [70/150], Train Loss: 34.39494646416336, Test Loss: 37.34383436921355\n",
      "Epoch [80/150], Train Loss: 32.470401188584624, Test Loss: 34.40445394639845\n",
      "Epoch [90/150], Train Loss: 30.676958202924883, Test Loss: 32.831830606832135\n",
      "Epoch [100/150], Train Loss: 29.39272268326556, Test Loss: 30.789045804506774\n",
      "Epoch [110/150], Train Loss: 27.944448733720623, Test Loss: 30.30970375259201\n",
      "Epoch [120/150], Train Loss: 27.60311409371798, Test Loss: 29.7387712652033\n",
      "Epoch [130/150], Train Loss: 25.96133093286733, Test Loss: 28.731081826346262\n",
      "Epoch [140/150], Train Loss: 25.386447381191566, Test Loss: 29.16450634250393\n",
      "Epoch [150/150], Train Loss: 25.263445857313812, Test Loss: 29.088699241737267\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.241822277131625, Test Loss: 45.99190947297332\n",
      "Epoch [20/150], Train Loss: 45.99637414900983, Test Loss: 43.749541047331576\n",
      "Epoch [30/150], Train Loss: 43.6443814512159, Test Loss: 40.66961684784332\n",
      "Epoch [40/150], Train Loss: 41.478851480953026, Test Loss: 40.27978728653549\n",
      "Epoch [50/150], Train Loss: 39.25141430213803, Test Loss: 39.89793896365475\n",
      "Epoch [60/150], Train Loss: 37.47020138599834, Test Loss: 37.44987106323242\n",
      "Epoch [70/150], Train Loss: 35.298955073122116, Test Loss: 36.99453856728294\n",
      "Epoch [80/150], Train Loss: 33.41568584754819, Test Loss: 36.01194951441381\n",
      "Epoch [90/150], Train Loss: 32.00825308502698, Test Loss: 32.18867433225954\n",
      "Epoch [100/150], Train Loss: 30.397394399173923, Test Loss: 32.66760184548118\n",
      "Epoch [110/150], Train Loss: 29.152451687171812, Test Loss: 31.781758172171457\n",
      "Epoch [120/150], Train Loss: 27.821084569712156, Test Loss: 31.13042690227558\n",
      "Epoch [130/150], Train Loss: 26.98798350975162, Test Loss: 29.212673583588042\n",
      "Epoch [140/150], Train Loss: 25.621245487400742, Test Loss: 29.40136872328721\n",
      "Epoch [150/150], Train Loss: 25.242398928032546, Test Loss: 29.816389950838957\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.94643364577997, Test Loss: 46.60462941751852\n",
      "Epoch [20/150], Train Loss: 45.36957608832687, Test Loss: 43.84179538875431\n",
      "Epoch [30/150], Train Loss: 43.042899084872886, Test Loss: 42.78828915682706\n",
      "Epoch [40/150], Train Loss: 40.67615604087955, Test Loss: 41.22620961573217\n",
      "Epoch [50/150], Train Loss: 38.575546001997154, Test Loss: 37.32985843311656\n",
      "Epoch [60/150], Train Loss: 36.278420545234056, Test Loss: 36.93309508980094\n",
      "Epoch [70/150], Train Loss: 34.55268971177398, Test Loss: 35.75768067000748\n",
      "Epoch [80/150], Train Loss: 32.93320306246398, Test Loss: 34.01168085073496\n",
      "Epoch [90/150], Train Loss: 31.30868770411757, Test Loss: 32.04652744144588\n",
      "Epoch [100/150], Train Loss: 29.412634139764503, Test Loss: 32.26850143036285\n",
      "Epoch [110/150], Train Loss: 28.72482072798932, Test Loss: 30.834141223461597\n",
      "Epoch [120/150], Train Loss: 27.357746017956345, Test Loss: 29.96338653564453\n",
      "Epoch [130/150], Train Loss: 25.72384301482654, Test Loss: 29.233381940172865\n",
      "Epoch [140/150], Train Loss: 26.32499479074947, Test Loss: 28.581149683370217\n",
      "Epoch [150/150], Train Loss: 24.974716730586817, Test Loss: 29.27635485166079\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.185480511774784, Test Loss: 46.01707077026367\n",
      "Epoch [20/150], Train Loss: 45.72556910280321, Test Loss: 43.22732340825068\n",
      "Epoch [30/150], Train Loss: 43.41880993452229, Test Loss: 42.56294686453683\n",
      "Epoch [40/150], Train Loss: 41.12256124527728, Test Loss: 41.25203273822735\n",
      "Epoch [50/150], Train Loss: 38.880906983672595, Test Loss: 37.669459256258875\n",
      "Epoch [60/150], Train Loss: 36.69710880342077, Test Loss: 34.98455027790813\n",
      "Epoch [70/150], Train Loss: 34.80966863163182, Test Loss: 35.78210397200151\n",
      "Epoch [80/150], Train Loss: 33.01399232833112, Test Loss: 33.14507670216746\n",
      "Epoch [90/150], Train Loss: 31.870985737784963, Test Loss: 31.933542102962345\n",
      "Epoch [100/150], Train Loss: 29.90741671577829, Test Loss: 30.650694686096983\n",
      "Epoch [110/150], Train Loss: 28.7654170239558, Test Loss: 30.442510902107536\n",
      "Epoch [120/150], Train Loss: 27.167735509403418, Test Loss: 30.266005850457525\n",
      "Epoch [130/150], Train Loss: 26.756565319123816, Test Loss: 29.60683334647835\n",
      "Epoch [140/150], Train Loss: 25.735451107337827, Test Loss: 28.54706585871709\n",
      "Epoch [150/150], Train Loss: 25.716412391037238, Test Loss: 29.63548231744147\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.366230223608795, Test Loss: 45.88932869651101\n",
      "Epoch [20/150], Train Loss: 46.036183854400136, Test Loss: 43.120232371540816\n",
      "Epoch [30/150], Train Loss: 43.77525054431352, Test Loss: 41.301530020577566\n",
      "Epoch [40/150], Train Loss: 41.582798692046616, Test Loss: 39.901333400181365\n",
      "Epoch [50/150], Train Loss: 39.26865969798604, Test Loss: 39.1217226796336\n",
      "Epoch [60/150], Train Loss: 37.257882740458506, Test Loss: 37.158263739053304\n",
      "Epoch [70/150], Train Loss: 35.64855085279121, Test Loss: 37.56708147618678\n",
      "Epoch [80/150], Train Loss: 33.92702018862865, Test Loss: 33.81376360608386\n",
      "Epoch [90/150], Train Loss: 32.17790263441742, Test Loss: 33.17389778038124\n",
      "Epoch [100/150], Train Loss: 30.4791823465316, Test Loss: 31.775155500932172\n",
      "Epoch [110/150], Train Loss: 29.62471953845415, Test Loss: 31.747298847545277\n",
      "Epoch [120/150], Train Loss: 27.717575598544762, Test Loss: 31.174974862631267\n",
      "Epoch [130/150], Train Loss: 26.865052614055696, Test Loss: 30.093439126943615\n",
      "Epoch [140/150], Train Loss: 25.778801827352556, Test Loss: 28.363090465595196\n",
      "Epoch [150/150], Train Loss: 26.1833505098937, Test Loss: 28.485614590830618\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.029785694059775, Test Loss: 48.114611836222856\n",
      "Epoch [20/150], Train Loss: 45.52135394987513, Test Loss: 43.68370928083147\n",
      "Epoch [30/150], Train Loss: 43.199337155701684, Test Loss: 42.92466929051783\n",
      "Epoch [40/150], Train Loss: 40.72859931070297, Test Loss: 39.84286305811498\n",
      "Epoch [50/150], Train Loss: 38.46276327664735, Test Loss: 38.160747676700744\n",
      "Epoch [60/150], Train Loss: 36.91526372940814, Test Loss: 35.69143193727964\n",
      "Epoch [70/150], Train Loss: 34.93744993366179, Test Loss: 34.523483449762516\n",
      "Epoch [80/150], Train Loss: 33.190283559580315, Test Loss: 33.714265922447304\n",
      "Epoch [90/150], Train Loss: 32.00066623375064, Test Loss: 30.865970215240083\n",
      "Epoch [100/150], Train Loss: 30.180867016901736, Test Loss: 31.503763917204623\n",
      "Epoch [110/150], Train Loss: 28.426843311747568, Test Loss: 28.578045411543414\n",
      "Epoch [120/150], Train Loss: 27.93565993387191, Test Loss: 29.448808050774907\n",
      "Epoch [130/150], Train Loss: 26.444208332749664, Test Loss: 28.637913146576324\n",
      "Epoch [140/150], Train Loss: 25.5728165923572, Test Loss: 27.922132318670098\n",
      "Epoch [150/150], Train Loss: 25.089479674667608, Test Loss: 28.644283691009917\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.668138147573, Test Loss: 46.07959791901824\n",
      "Epoch [20/150], Train Loss: 45.11971141627578, Test Loss: 39.383915641091086\n",
      "Epoch [30/150], Train Loss: 42.71573555117748, Test Loss: 41.22047944502397\n",
      "Epoch [40/150], Train Loss: 40.62462335805424, Test Loss: 38.037129687024404\n",
      "Epoch [50/150], Train Loss: 38.621377838635055, Test Loss: 35.301769999714644\n",
      "Epoch [60/150], Train Loss: 36.20833321868396, Test Loss: 35.10326246781783\n",
      "Epoch [70/150], Train Loss: 34.2699981189165, Test Loss: 34.31664724473829\n",
      "Epoch [80/150], Train Loss: 32.69366219942687, Test Loss: 34.54450698951622\n",
      "Epoch [90/150], Train Loss: 31.14634885944304, Test Loss: 33.23458381751915\n",
      "Epoch [100/150], Train Loss: 29.393729888415727, Test Loss: 31.649741928298752\n",
      "Epoch [110/150], Train Loss: 28.730681660136238, Test Loss: 30.324228806929156\n",
      "Epoch [120/150], Train Loss: 27.207729295824397, Test Loss: 28.85914185759309\n",
      "Epoch [130/150], Train Loss: 26.66707218357774, Test Loss: 27.832988268369203\n",
      "Epoch [140/150], Train Loss: 26.36540083963363, Test Loss: 28.489855729140245\n",
      "Epoch [150/150], Train Loss: 24.59679443484447, Test Loss: 28.503745264821237\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.282020743948515, Test Loss: 45.86603600638254\n",
      "Epoch [20/150], Train Loss: 45.95773424242363, Test Loss: 43.73639059686042\n",
      "Epoch [30/150], Train Loss: 43.69818683061443, Test Loss: 41.86677090533368\n",
      "Epoch [40/150], Train Loss: 41.45578336872038, Test Loss: 41.053469472117236\n",
      "Epoch [50/150], Train Loss: 39.15811996459961, Test Loss: 39.5605946330281\n",
      "Epoch [60/150], Train Loss: 37.44822460237096, Test Loss: 36.577532508156516\n",
      "Epoch [70/150], Train Loss: 35.75441571220023, Test Loss: 36.87770704789595\n",
      "Epoch [80/150], Train Loss: 33.950321185002565, Test Loss: 36.422578985040836\n",
      "Epoch [90/150], Train Loss: 31.966488885097817, Test Loss: 33.68220349101277\n",
      "Epoch [100/150], Train Loss: 30.550751514122133, Test Loss: 30.51699668091613\n",
      "Epoch [110/150], Train Loss: 29.51297583658187, Test Loss: 28.767754046947925\n",
      "Epoch [120/150], Train Loss: 27.84238121157787, Test Loss: 31.204951571179674\n",
      "Epoch [130/150], Train Loss: 26.898013143070408, Test Loss: 29.821719231543604\n",
      "Epoch [140/150], Train Loss: 25.92146342543305, Test Loss: 30.02026567830668\n",
      "Epoch [150/150], Train Loss: 25.32697669482622, Test Loss: 29.962495110251687\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.014931831985223, Test Loss: 31.1439709106049\n",
      "Epoch [20/150], Train Loss: 26.045531613709496, Test Loss: 28.785005792394863\n",
      "Epoch [30/150], Train Loss: 24.817153192739017, Test Loss: 28.81122148191774\n",
      "Epoch [40/150], Train Loss: 24.225695181674645, Test Loss: 28.125228559816037\n",
      "Epoch [50/150], Train Loss: 23.750745179223234, Test Loss: 29.091565094984972\n",
      "Epoch [60/150], Train Loss: 23.209312920492202, Test Loss: 28.623869685383585\n",
      "Epoch [70/150], Train Loss: 22.684620841604765, Test Loss: 28.592585402649718\n",
      "Epoch [80/150], Train Loss: 22.600702923634014, Test Loss: 28.806656750765715\n",
      "Epoch [90/150], Train Loss: 23.289540856783507, Test Loss: 28.21349270312817\n",
      "Epoch [100/150], Train Loss: 22.17103009458448, Test Loss: 29.640345090395446\n",
      "Epoch [110/150], Train Loss: 22.19027820962374, Test Loss: 28.668659631308024\n",
      "Epoch [120/150], Train Loss: 22.518177582787686, Test Loss: 28.614901852298093\n",
      "Epoch [130/150], Train Loss: 22.249151917754627, Test Loss: 29.454590710726652\n",
      "Epoch [140/150], Train Loss: 21.464689955164175, Test Loss: 31.79639345639712\n",
      "Epoch [150/150], Train Loss: 21.300494572373687, Test Loss: 29.551490957086738\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.11913380857374, Test Loss: 28.988667302317435\n",
      "Epoch [20/150], Train Loss: 25.719657260081807, Test Loss: 27.63800980208756\n",
      "Epoch [30/150], Train Loss: 24.981161586573865, Test Loss: 27.70794781771573\n",
      "Epoch [40/150], Train Loss: 25.329675030317464, Test Loss: 27.838547694218622\n",
      "Epoch [50/150], Train Loss: 24.18940836171635, Test Loss: 28.51598729715719\n",
      "Epoch [60/150], Train Loss: 23.484234012541222, Test Loss: 29.588240858796354\n",
      "Epoch [70/150], Train Loss: 23.463642689439116, Test Loss: 28.456954435868695\n",
      "Epoch [80/150], Train Loss: 23.780692672729494, Test Loss: 29.37220028468541\n",
      "Epoch [90/150], Train Loss: 22.50797453082976, Test Loss: 28.293090176272703\n",
      "Epoch [100/150], Train Loss: 22.3559874737849, Test Loss: 29.793567360221566\n",
      "Epoch [110/150], Train Loss: 22.23184862605861, Test Loss: 29.113637453549867\n",
      "Epoch [120/150], Train Loss: 23.625027747232405, Test Loss: 29.44172291941457\n",
      "Epoch [130/150], Train Loss: 22.160159589423507, Test Loss: 30.41904001112108\n",
      "Epoch [140/150], Train Loss: 22.26687952260502, Test Loss: 29.66252780270267\n",
      "Epoch [150/150], Train Loss: 22.560851100233734, Test Loss: 28.981822967529297\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.20099868774414, Test Loss: 29.819465562894745\n",
      "Epoch [20/150], Train Loss: 27.36339958691206, Test Loss: 30.56405084783381\n",
      "Epoch [30/150], Train Loss: 25.337092277651927, Test Loss: 28.39706651266519\n",
      "Epoch [40/150], Train Loss: 25.31848340894355, Test Loss: 31.589405109355976\n",
      "Epoch [50/150], Train Loss: 24.317832133809073, Test Loss: 28.930707782893986\n",
      "Epoch [60/150], Train Loss: 23.531831703811395, Test Loss: 28.895763867861266\n",
      "Epoch [70/150], Train Loss: 23.550568196030913, Test Loss: 28.95642934526716\n",
      "Epoch [80/150], Train Loss: 23.310763456000657, Test Loss: 28.196843655078442\n",
      "Epoch [90/150], Train Loss: 24.348576736450195, Test Loss: 28.812573816869165\n",
      "Epoch [100/150], Train Loss: 22.818913306564582, Test Loss: 29.24487691111379\n",
      "Epoch [110/150], Train Loss: 22.773645401000977, Test Loss: 29.703086431924397\n",
      "Epoch [120/150], Train Loss: 21.27441771460361, Test Loss: 28.15882227018282\n",
      "Epoch [130/150], Train Loss: 21.485182759019196, Test Loss: 29.954127844277913\n",
      "Epoch [140/150], Train Loss: 22.121721536605083, Test Loss: 30.386114194795685\n",
      "Epoch [150/150], Train Loss: 21.548825667334384, Test Loss: 28.59765300503025\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.333007431030275, Test Loss: 27.540666406804863\n",
      "Epoch [20/150], Train Loss: 25.75869074336818, Test Loss: 27.408227425116998\n",
      "Epoch [30/150], Train Loss: 24.723620955670466, Test Loss: 27.965900842245524\n",
      "Epoch [40/150], Train Loss: 24.17256344654521, Test Loss: 28.158651599636325\n",
      "Epoch [50/150], Train Loss: 24.627006042980756, Test Loss: 27.55240316514845\n",
      "Epoch [60/150], Train Loss: 25.58045811262287, Test Loss: 28.02784236065753\n",
      "Epoch [70/150], Train Loss: 23.591877596495582, Test Loss: 28.754560643976387\n",
      "Epoch [80/150], Train Loss: 23.927791595458984, Test Loss: 28.489652980457652\n",
      "Epoch [90/150], Train Loss: 24.176652927086, Test Loss: 28.27979060581752\n",
      "Epoch [100/150], Train Loss: 23.611609083707215, Test Loss: 28.236922846212014\n",
      "Epoch [110/150], Train Loss: 23.196076902796012, Test Loss: 30.131761798610935\n",
      "Epoch [120/150], Train Loss: 22.561397802634318, Test Loss: 28.671637175919173\n",
      "Epoch [130/150], Train Loss: 22.137038490420483, Test Loss: 29.9038819399747\n",
      "Epoch [140/150], Train Loss: 20.8552101385398, Test Loss: 30.125429376379238\n",
      "Epoch [150/150], Train Loss: 21.234588222816342, Test Loss: 29.933327513855772\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.698703328116995, Test Loss: 30.006569453648158\n",
      "Epoch [20/150], Train Loss: 26.37753698630411, Test Loss: 26.724290203738523\n",
      "Epoch [30/150], Train Loss: 24.293202209472657, Test Loss: 27.520134962998426\n",
      "Epoch [40/150], Train Loss: 24.77331829383725, Test Loss: 28.511294748876\n",
      "Epoch [50/150], Train Loss: 24.49366321876401, Test Loss: 27.543533027946175\n",
      "Epoch [60/150], Train Loss: 25.375955075123272, Test Loss: 27.193020510983157\n",
      "Epoch [70/150], Train Loss: 24.6128023178851, Test Loss: 27.427071038778728\n",
      "Epoch [80/150], Train Loss: 23.919009974745453, Test Loss: 27.497131743988433\n",
      "Epoch [90/150], Train Loss: 23.063344073686444, Test Loss: 28.24087569001433\n",
      "Epoch [100/150], Train Loss: 22.741998584934922, Test Loss: 27.551832966990286\n",
      "Epoch [110/150], Train Loss: 23.650099757460296, Test Loss: 28.78869747805905\n",
      "Epoch [120/150], Train Loss: 23.143613271244238, Test Loss: 28.463582076035536\n",
      "Epoch [130/150], Train Loss: 23.157515128714138, Test Loss: 30.54938742402312\n",
      "Epoch [140/150], Train Loss: 22.5098602607602, Test Loss: 27.968185177097073\n",
      "Epoch [150/150], Train Loss: 22.900619281706263, Test Loss: 26.596211272400694\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.541872850011607, Test Loss: 32.64626285008022\n",
      "Epoch [20/150], Train Loss: 26.73861213433938, Test Loss: 31.242899560309077\n",
      "Epoch [30/150], Train Loss: 24.29870791826092, Test Loss: 27.60521349968848\n",
      "Epoch [40/150], Train Loss: 24.172536874989994, Test Loss: 28.808790652782886\n",
      "Epoch [50/150], Train Loss: 23.68790377007156, Test Loss: 28.320468853046368\n",
      "Epoch [60/150], Train Loss: 24.28237073304223, Test Loss: 28.351432032399362\n",
      "Epoch [70/150], Train Loss: 24.40304657357638, Test Loss: 29.419552741112646\n",
      "Epoch [80/150], Train Loss: 23.378408876012582, Test Loss: 27.73200681612089\n",
      "Epoch [90/150], Train Loss: 22.125945413308067, Test Loss: 28.06804424137264\n",
      "Epoch [100/150], Train Loss: 23.556779742631758, Test Loss: 29.26267839406992\n",
      "Epoch [110/150], Train Loss: 22.98718731989626, Test Loss: 28.47879954746791\n",
      "Epoch [120/150], Train Loss: 22.727650176501665, Test Loss: 28.756057986965427\n",
      "Epoch [130/150], Train Loss: 22.05525827876857, Test Loss: 29.22307116025454\n",
      "Epoch [140/150], Train Loss: 21.67864659418825, Test Loss: 29.45322542066698\n",
      "Epoch [150/150], Train Loss: 22.091180188538598, Test Loss: 29.08541845346426\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.20286159828061, Test Loss: 34.18010934606775\n",
      "Epoch [20/150], Train Loss: 25.794894771888607, Test Loss: 32.177271062677555\n",
      "Epoch [30/150], Train Loss: 26.084647156762294, Test Loss: 28.243880284297003\n",
      "Epoch [40/150], Train Loss: 25.03814489020676, Test Loss: 28.58524988843249\n",
      "Epoch [50/150], Train Loss: 24.914803683171506, Test Loss: 29.178810070087383\n",
      "Epoch [60/150], Train Loss: 24.757108481985625, Test Loss: 28.821404419936144\n",
      "Epoch [70/150], Train Loss: 25.619615367201508, Test Loss: 28.376168907462777\n",
      "Epoch [80/150], Train Loss: 24.762601933713817, Test Loss: 28.037468105167537\n",
      "Epoch [90/150], Train Loss: 24.045292894957495, Test Loss: 28.384575385551948\n",
      "Epoch [100/150], Train Loss: 23.630034399814292, Test Loss: 28.687963510488537\n",
      "Epoch [110/150], Train Loss: 23.907133934146067, Test Loss: 28.0775613165521\n",
      "Epoch [120/150], Train Loss: 23.49745505286045, Test Loss: 29.35794216007381\n",
      "Epoch [130/150], Train Loss: 22.798296212368324, Test Loss: 29.038012318796927\n",
      "Epoch [140/150], Train Loss: 23.949476861172034, Test Loss: 29.95578570180125\n",
      "Epoch [150/150], Train Loss: 22.61362157727851, Test Loss: 29.285582752970907\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.22439607088683, Test Loss: 33.68277138549012\n",
      "Epoch [20/150], Train Loss: 27.32154055736104, Test Loss: 28.842497317821948\n",
      "Epoch [30/150], Train Loss: 25.158773985065398, Test Loss: 29.03736330007578\n",
      "Epoch [40/150], Train Loss: 26.119193805632044, Test Loss: 28.805473946905757\n",
      "Epoch [50/150], Train Loss: 25.7974989156254, Test Loss: 29.09131520754331\n",
      "Epoch [60/150], Train Loss: 24.53391185823034, Test Loss: 28.58318769776976\n",
      "Epoch [70/150], Train Loss: 25.455596861292104, Test Loss: 28.14425951474673\n",
      "Epoch [80/150], Train Loss: 24.538115523291417, Test Loss: 28.23635381227964\n",
      "Epoch [90/150], Train Loss: 24.97000817470863, Test Loss: 27.64583377095012\n",
      "Epoch [100/150], Train Loss: 24.398619948840533, Test Loss: 27.96421786717006\n",
      "Epoch [110/150], Train Loss: 24.75204662885822, Test Loss: 28.982571639023817\n",
      "Epoch [120/150], Train Loss: 23.77073428669914, Test Loss: 28.95960792937836\n",
      "Epoch [130/150], Train Loss: 24.395983029975266, Test Loss: 29.540357391555588\n",
      "Epoch [140/150], Train Loss: 23.55603160545474, Test Loss: 33.62154348794516\n",
      "Epoch [150/150], Train Loss: 23.464896067634957, Test Loss: 29.261598636577656\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.437604997978834, Test Loss: 33.348347379015635\n",
      "Epoch [20/150], Train Loss: 25.180275695050348, Test Loss: 30.49242027084549\n",
      "Epoch [30/150], Train Loss: 25.28256749012431, Test Loss: 28.669074789270176\n",
      "Epoch [40/150], Train Loss: 25.264208571637262, Test Loss: 30.94017927987235\n",
      "Epoch [50/150], Train Loss: 25.673599224403258, Test Loss: 27.917947843477325\n",
      "Epoch [60/150], Train Loss: 25.097827454864003, Test Loss: 28.726368148605545\n",
      "Epoch [70/150], Train Loss: 25.308195539380684, Test Loss: 28.59479027289849\n",
      "Epoch [80/150], Train Loss: 24.536761437087762, Test Loss: 29.80443875201337\n",
      "Epoch [90/150], Train Loss: 24.12451906673244, Test Loss: 28.463010540256253\n",
      "Epoch [100/150], Train Loss: 24.69036792692591, Test Loss: 27.52797859984559\n",
      "Epoch [110/150], Train Loss: 23.822848792154282, Test Loss: 30.807438193977653\n",
      "Epoch [120/150], Train Loss: 23.468808971467567, Test Loss: 27.464028024054194\n",
      "Epoch [130/150], Train Loss: 23.90804693503458, Test Loss: 27.630581583295548\n",
      "Epoch [140/150], Train Loss: 23.158633904378924, Test Loss: 27.345986601594205\n",
      "Epoch [150/150], Train Loss: 23.86150008029625, Test Loss: 28.3834326855548\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.345089077558672, Test Loss: 31.92211274976854\n",
      "Epoch [20/150], Train Loss: 25.3066089817735, Test Loss: 31.416004849718764\n",
      "Epoch [30/150], Train Loss: 24.77541184972544, Test Loss: 28.628998199066558\n",
      "Epoch [40/150], Train Loss: 25.052271839829743, Test Loss: 27.456908263169325\n",
      "Epoch [50/150], Train Loss: 26.268598925481076, Test Loss: 27.921450602543818\n",
      "Epoch [60/150], Train Loss: 24.33348199187732, Test Loss: 28.260251082383192\n",
      "Epoch [70/150], Train Loss: 24.416099785976723, Test Loss: 26.80130049470183\n",
      "Epoch [80/150], Train Loss: 24.559396681238393, Test Loss: 28.08781809621043\n",
      "Epoch [90/150], Train Loss: 23.596695046346696, Test Loss: 28.57362965175084\n",
      "Epoch [100/150], Train Loss: 22.819334480410717, Test Loss: 35.014281186190516\n",
      "Epoch [110/150], Train Loss: 23.31274486604284, Test Loss: 28.044358290635145\n",
      "Epoch [120/150], Train Loss: 24.341844064681258, Test Loss: 28.05214654005967\n",
      "Epoch [130/150], Train Loss: 22.322332407216557, Test Loss: 27.934609648469205\n",
      "Epoch [140/150], Train Loss: 22.277826828253072, Test Loss: 27.381378817867922\n",
      "Epoch [150/150], Train Loss: 22.339773684642353, Test Loss: 27.12998491757876\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.868153068667553, Test Loss: 35.49487426064231\n",
      "Epoch [20/150], Train Loss: 26.088978989397894, Test Loss: 27.8265901540781\n",
      "Epoch [30/150], Train Loss: 26.201969503183832, Test Loss: 28.52361976326286\n",
      "Epoch [40/150], Train Loss: 26.21662995385342, Test Loss: 28.459586354045125\n",
      "Epoch [50/150], Train Loss: 25.408513960291128, Test Loss: 27.7436310656659\n",
      "Epoch [60/150], Train Loss: 25.644775378117796, Test Loss: 27.74402041249461\n",
      "Epoch [70/150], Train Loss: 24.54268581828133, Test Loss: 28.007617801814884\n",
      "Epoch [80/150], Train Loss: 23.845308153746558, Test Loss: 27.249446249627447\n",
      "Epoch [90/150], Train Loss: 24.479683797867573, Test Loss: 35.91343565111036\n",
      "Epoch [100/150], Train Loss: 24.160033229139984, Test Loss: 27.662290399724785\n",
      "Epoch [110/150], Train Loss: 24.80602852868252, Test Loss: 27.193770817347936\n",
      "Epoch [120/150], Train Loss: 23.92609902678943, Test Loss: 30.614843517154842\n",
      "Epoch [130/150], Train Loss: 22.87520625630363, Test Loss: 30.06840450732739\n",
      "Epoch [140/150], Train Loss: 23.45340868215092, Test Loss: 33.35984797291941\n",
      "Epoch [150/150], Train Loss: 23.389409787537623, Test Loss: 28.377757159146395\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.03425404282867, Test Loss: 42.195439574006315\n",
      "Epoch [20/150], Train Loss: 26.857993541780065, Test Loss: 28.725464833247198\n",
      "Epoch [30/150], Train Loss: 26.177816853757765, Test Loss: 32.64317789944735\n",
      "Epoch [40/150], Train Loss: 25.25096897062708, Test Loss: 28.78227729301948\n",
      "Epoch [50/150], Train Loss: 25.008233792664576, Test Loss: 29.98209316699536\n",
      "Epoch [60/150], Train Loss: 25.15144895960073, Test Loss: 28.470703546102946\n",
      "Epoch [70/150], Train Loss: 24.631555863677477, Test Loss: 29.54659637847504\n",
      "Epoch [80/150], Train Loss: 23.36895210391185, Test Loss: 27.776152028665916\n",
      "Epoch [90/150], Train Loss: 24.713156246748127, Test Loss: 29.13025088124461\n",
      "Epoch [100/150], Train Loss: 24.33930898572578, Test Loss: 27.84447989525733\n",
      "Epoch [110/150], Train Loss: 23.841195609921314, Test Loss: 27.057231655368557\n",
      "Epoch [120/150], Train Loss: 23.86945481222184, Test Loss: 29.449925707532213\n",
      "Epoch [130/150], Train Loss: 22.681339889276224, Test Loss: 29.51382166379458\n",
      "Epoch [140/150], Train Loss: 23.611033599103084, Test Loss: 30.675245062097325\n",
      "Epoch [150/150], Train Loss: 22.025347275030416, Test Loss: 32.04735508212796\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.87889696340092, Test Loss: 29.9224850295426\n",
      "Epoch [20/150], Train Loss: 26.40079882887543, Test Loss: 28.62001894666003\n",
      "Epoch [30/150], Train Loss: 25.761365296410734, Test Loss: 28.940157506373023\n",
      "Epoch [40/150], Train Loss: 25.59887134989754, Test Loss: 28.4527138549012\n",
      "Epoch [50/150], Train Loss: 24.979064991435067, Test Loss: 27.924490148370918\n",
      "Epoch [60/150], Train Loss: 24.582443806382475, Test Loss: 29.705529299649324\n",
      "Epoch [70/150], Train Loss: 25.592292110255507, Test Loss: 27.83816077492454\n",
      "Epoch [80/150], Train Loss: 24.34886412073354, Test Loss: 28.508543336546268\n",
      "Epoch [90/150], Train Loss: 24.23381734128858, Test Loss: 27.905526148808466\n",
      "Epoch [100/150], Train Loss: 24.36355709638752, Test Loss: 30.208134366320326\n",
      "Epoch [110/150], Train Loss: 23.823775188258438, Test Loss: 28.31295072877562\n",
      "Epoch [120/150], Train Loss: 24.77329862000512, Test Loss: 28.786151019009676\n",
      "Epoch [130/150], Train Loss: 25.045537898579582, Test Loss: 28.564515299611276\n",
      "Epoch [140/150], Train Loss: 22.913378912503603, Test Loss: 27.565856537261567\n",
      "Epoch [150/150], Train Loss: 25.02206085080006, Test Loss: 28.49784402723436\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.967948450807665, Test Loss: 30.257895457280146\n",
      "Epoch [20/150], Train Loss: 26.264926497662653, Test Loss: 29.4782897404262\n",
      "Epoch [30/150], Train Loss: 25.822590849829503, Test Loss: 32.319862613430274\n",
      "Epoch [40/150], Train Loss: 26.34330174805688, Test Loss: 29.22340217194\n",
      "Epoch [50/150], Train Loss: 25.608015873393075, Test Loss: 27.44283762845126\n",
      "Epoch [60/150], Train Loss: 24.667946574727043, Test Loss: 28.066870255903765\n",
      "Epoch [70/150], Train Loss: 24.334349929309283, Test Loss: 28.399172176014293\n",
      "Epoch [80/150], Train Loss: 26.100276465494126, Test Loss: 27.855146779642478\n",
      "Epoch [90/150], Train Loss: 25.268008885618116, Test Loss: 28.904837695035067\n",
      "Epoch [100/150], Train Loss: 23.975884165529344, Test Loss: 29.942953629927203\n",
      "Epoch [110/150], Train Loss: 25.332116624175526, Test Loss: 28.76853712503012\n",
      "Epoch [120/150], Train Loss: 23.676390832369446, Test Loss: 28.470259109100738\n",
      "Epoch [130/150], Train Loss: 24.19082492140473, Test Loss: 28.75365604053844\n",
      "Epoch [140/150], Train Loss: 23.112804106415297, Test Loss: 29.005603691200157\n",
      "Epoch [150/150], Train Loss: 23.453601599521324, Test Loss: 29.685507761967646\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.89704097685267, Test Loss: 29.181057347879783\n",
      "Epoch [20/150], Train Loss: 26.880517646914623, Test Loss: 28.652622668774097\n",
      "Epoch [30/150], Train Loss: 26.75290380384101, Test Loss: 29.417399171110873\n",
      "Epoch [40/150], Train Loss: 26.267396451606125, Test Loss: 33.09119375649985\n",
      "Epoch [50/150], Train Loss: 25.464921913772333, Test Loss: 28.858343074848126\n",
      "Epoch [60/150], Train Loss: 25.044968451828254, Test Loss: 29.170630170153334\n",
      "Epoch [70/150], Train Loss: 24.72544136047363, Test Loss: 29.208756310599192\n",
      "Epoch [80/150], Train Loss: 25.094394421186603, Test Loss: 27.173248142391056\n",
      "Epoch [90/150], Train Loss: 24.741581225786053, Test Loss: 29.726379419302013\n",
      "Epoch [100/150], Train Loss: 24.561572340668224, Test Loss: 27.90354094567237\n",
      "Epoch [110/150], Train Loss: 25.06500355454742, Test Loss: 28.121358499898538\n",
      "Epoch [120/150], Train Loss: 24.41761782286597, Test Loss: 27.587094851902553\n",
      "Epoch [130/150], Train Loss: 24.99097550814269, Test Loss: 27.44764994336413\n",
      "Epoch [140/150], Train Loss: 24.352508094662525, Test Loss: 30.962141185611873\n",
      "Epoch [150/150], Train Loss: 24.607725387323097, Test Loss: 29.326562410825257\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.432108994780993, Test Loss: 32.41286899517109\n",
      "Epoch [20/150], Train Loss: 27.694215962144195, Test Loss: 28.774556370524618\n",
      "Epoch [30/150], Train Loss: 26.470384422677462, Test Loss: 28.873892301088805\n",
      "Epoch [40/150], Train Loss: 25.628847822595816, Test Loss: 28.161301600468622\n",
      "Epoch [50/150], Train Loss: 25.87192024481101, Test Loss: 27.012152684199346\n",
      "Epoch [60/150], Train Loss: 25.986594547209194, Test Loss: 29.876958252547624\n",
      "Epoch [70/150], Train Loss: 26.073878028744556, Test Loss: 28.5017120064079\n",
      "Epoch [80/150], Train Loss: 25.61665392391017, Test Loss: 28.35207901991807\n",
      "Epoch [90/150], Train Loss: 26.04505970438973, Test Loss: 29.00999319398558\n",
      "Epoch [100/150], Train Loss: 25.204224733446466, Test Loss: 27.44094028720608\n",
      "Epoch [110/150], Train Loss: 25.33912743115034, Test Loss: 27.642339805503944\n",
      "Epoch [120/150], Train Loss: 25.30163970071761, Test Loss: 30.110438061999037\n",
      "Epoch [130/150], Train Loss: 25.612550322735895, Test Loss: 30.013880073250114\n",
      "Epoch [140/150], Train Loss: 25.09611852051782, Test Loss: 27.961615550053583\n",
      "Epoch [150/150], Train Loss: 24.7928928000028, Test Loss: 28.42428918318315\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.244478150664783, Test Loss: 33.784878941325395\n",
      "Epoch [20/150], Train Loss: 27.059310293979333, Test Loss: 30.6131561576546\n",
      "Epoch [30/150], Train Loss: 26.516647495207238, Test Loss: 30.708477218429763\n",
      "Epoch [40/150], Train Loss: 25.88334156724273, Test Loss: 30.222704453901812\n",
      "Epoch [50/150], Train Loss: 25.371122916800076, Test Loss: 31.170458087673435\n",
      "Epoch [60/150], Train Loss: 26.50057005335073, Test Loss: 29.16923139002416\n",
      "Epoch [70/150], Train Loss: 25.580879855546794, Test Loss: 29.148949016224254\n",
      "Epoch [80/150], Train Loss: 26.59705412817783, Test Loss: 28.740079111867136\n",
      "Epoch [90/150], Train Loss: 25.461502150238537, Test Loss: 27.775484233707576\n",
      "Epoch [100/150], Train Loss: 25.110251154665086, Test Loss: 30.182350604565112\n",
      "Epoch [110/150], Train Loss: 26.216538426133454, Test Loss: 30.38397149915819\n",
      "Epoch [120/150], Train Loss: 25.657926415615396, Test Loss: 31.76254653930664\n",
      "Epoch [130/150], Train Loss: 26.66576310454822, Test Loss: 28.564934024563083\n",
      "Epoch [140/150], Train Loss: 25.485517114107726, Test Loss: 30.245093258944426\n",
      "Epoch [150/150], Train Loss: 25.59202256749888, Test Loss: 26.99831387903783\n",
      "Training model with hidden size 64, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.702961349487303, Test Loss: 36.79091577406054\n",
      "Epoch [20/150], Train Loss: 26.8595865843726, Test Loss: 33.1276469787994\n",
      "Epoch [30/150], Train Loss: 26.72908444639112, Test Loss: 29.468235065410664\n",
      "Epoch [40/150], Train Loss: 26.31068178395756, Test Loss: 29.12612870451692\n",
      "Epoch [50/150], Train Loss: 25.970142014300237, Test Loss: 29.596208076972466\n",
      "Epoch [60/150], Train Loss: 26.03614170512215, Test Loss: 29.898276514821237\n",
      "Epoch [70/150], Train Loss: 25.08564853980893, Test Loss: 28.829676491873606\n",
      "Epoch [80/150], Train Loss: 25.977618808433657, Test Loss: 28.85776846749442\n",
      "Epoch [90/150], Train Loss: 26.33167790897557, Test Loss: 28.695375095714223\n",
      "Epoch [100/150], Train Loss: 26.44312605310659, Test Loss: 29.089116852004807\n",
      "Epoch [110/150], Train Loss: 25.055756478231462, Test Loss: 28.475673155351117\n",
      "Epoch [120/150], Train Loss: 25.644384021446353, Test Loss: 28.332851657619724\n",
      "Epoch [130/150], Train Loss: 24.888891195078365, Test Loss: 27.513126175124924\n",
      "Epoch [140/150], Train Loss: 26.192948288214012, Test Loss: 28.9031907861883\n",
      "Epoch [150/150], Train Loss: 25.91216525718814, Test Loss: 27.941707759708553\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.69820379038326, Test Loss: 47.98693084716797\n",
      "Epoch [20/150], Train Loss: 47.1798773718662, Test Loss: 46.46221923828125\n",
      "Epoch [30/150], Train Loss: 45.66476311605485, Test Loss: 44.14840316772461\n",
      "Epoch [40/150], Train Loss: 44.12884353887839, Test Loss: 40.59037780761719\n",
      "Epoch [50/150], Train Loss: 42.79830376046603, Test Loss: 42.10039138793945\n",
      "Epoch [60/150], Train Loss: 41.354232819353946, Test Loss: 40.86692810058594\n",
      "Epoch [70/150], Train Loss: 39.868726211297705, Test Loss: 40.358123779296875\n",
      "Epoch [80/150], Train Loss: 38.52475347049901, Test Loss: 37.80699157714844\n",
      "Epoch [90/150], Train Loss: 37.420595112784966, Test Loss: 39.00350570678711\n",
      "Epoch [100/150], Train Loss: 35.866980168076815, Test Loss: 36.06480407714844\n",
      "Epoch [110/150], Train Loss: 34.823621793653146, Test Loss: 38.58957290649414\n",
      "Epoch [120/150], Train Loss: 33.50144245585457, Test Loss: 35.67022705078125\n",
      "Epoch [130/150], Train Loss: 32.24319566195128, Test Loss: 37.5844612121582\n",
      "Epoch [140/150], Train Loss: 31.350589145597866, Test Loss: 33.13539505004883\n",
      "Epoch [150/150], Train Loss: 30.233723512243053, Test Loss: 32.659095764160156\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.71993005471151, Test Loss: 49.13396453857422\n",
      "Epoch [20/150], Train Loss: 47.163722604220034, Test Loss: 45.753501892089844\n",
      "Epoch [30/150], Train Loss: 45.627687710621316, Test Loss: 38.23255920410156\n",
      "Epoch [40/150], Train Loss: 44.13170197283635, Test Loss: 41.96654510498047\n",
      "Epoch [50/150], Train Loss: 42.5873046875, Test Loss: 40.73768997192383\n",
      "Epoch [60/150], Train Loss: 41.200530393006375, Test Loss: 40.155479431152344\n",
      "Epoch [70/150], Train Loss: 39.744131682349035, Test Loss: 39.15996170043945\n",
      "Epoch [80/150], Train Loss: 38.43339355969038, Test Loss: 37.07495880126953\n",
      "Epoch [90/150], Train Loss: 37.04857611734359, Test Loss: 36.78322982788086\n",
      "Epoch [100/150], Train Loss: 35.92870213242828, Test Loss: 35.86425018310547\n",
      "Epoch [110/150], Train Loss: 34.40971181900775, Test Loss: 33.661720275878906\n",
      "Epoch [120/150], Train Loss: 33.502075733122275, Test Loss: 34.31758117675781\n",
      "Epoch [130/150], Train Loss: 32.65611954986072, Test Loss: 31.98676872253418\n",
      "Epoch [140/150], Train Loss: 31.23003605076524, Test Loss: 31.43389129638672\n",
      "Epoch [150/150], Train Loss: 29.919448933835888, Test Loss: 30.136205673217773\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 49.05716488947634, Test Loss: 46.7227668762207\n",
      "Epoch [20/150], Train Loss: 47.649925819772186, Test Loss: 45.80643844604492\n",
      "Epoch [30/150], Train Loss: 46.16069732415872, Test Loss: 43.52732467651367\n",
      "Epoch [40/150], Train Loss: 44.88852674140305, Test Loss: 43.90647888183594\n",
      "Epoch [50/150], Train Loss: 43.4471752104212, Test Loss: 40.4161491394043\n",
      "Epoch [60/150], Train Loss: 41.916567874345624, Test Loss: 38.51541519165039\n",
      "Epoch [70/150], Train Loss: 40.67326053556849, Test Loss: 40.18136978149414\n",
      "Epoch [80/150], Train Loss: 39.60746309249127, Test Loss: 36.513587951660156\n",
      "Epoch [90/150], Train Loss: 38.196416160708566, Test Loss: 36.5556526184082\n",
      "Epoch [100/150], Train Loss: 36.86239980478756, Test Loss: 35.65304183959961\n",
      "Epoch [110/150], Train Loss: 35.503748859342984, Test Loss: 37.181644439697266\n",
      "Epoch [120/150], Train Loss: 34.46114550731221, Test Loss: 33.77588653564453\n",
      "Epoch [130/150], Train Loss: 33.47743586555856, Test Loss: 33.92936325073242\n",
      "Epoch [140/150], Train Loss: 32.083588847175974, Test Loss: 33.295814514160156\n",
      "Epoch [150/150], Train Loss: 31.728477790707448, Test Loss: 30.533809661865234\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.69850646472368, Test Loss: 49.004051208496094\n",
      "Epoch [20/150], Train Loss: 47.13696105206599, Test Loss: 46.3137092590332\n",
      "Epoch [30/150], Train Loss: 45.59524090876345, Test Loss: 44.75896072387695\n",
      "Epoch [40/150], Train Loss: 44.10602657130507, Test Loss: 42.805824279785156\n",
      "Epoch [50/150], Train Loss: 42.591848817418835, Test Loss: 42.69047927856445\n",
      "Epoch [60/150], Train Loss: 41.130140836121605, Test Loss: 39.41250991821289\n",
      "Epoch [70/150], Train Loss: 39.741789108026225, Test Loss: 37.67523193359375\n",
      "Epoch [80/150], Train Loss: 38.4073836029553, Test Loss: 38.39439010620117\n",
      "Epoch [90/150], Train Loss: 37.11782424176326, Test Loss: 37.09860610961914\n",
      "Epoch [100/150], Train Loss: 36.18614265566966, Test Loss: 38.05056381225586\n",
      "Epoch [110/150], Train Loss: 34.9110966416656, Test Loss: 35.29132080078125\n",
      "Epoch [120/150], Train Loss: 33.53930817901111, Test Loss: 34.25770950317383\n",
      "Epoch [130/150], Train Loss: 32.2809739034684, Test Loss: 32.96125793457031\n",
      "Epoch [140/150], Train Loss: 31.695445107631997, Test Loss: 31.47229766845703\n",
      "Epoch [150/150], Train Loss: 30.538955088130763, Test Loss: 30.833324432373047\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.97366665699443, Test Loss: 48.29600524902344\n",
      "Epoch [20/150], Train Loss: 47.46189797823546, Test Loss: 46.023040771484375\n",
      "Epoch [30/150], Train Loss: 45.94679183959961, Test Loss: 42.33277130126953\n",
      "Epoch [40/150], Train Loss: 44.58143440621798, Test Loss: 41.77138137817383\n",
      "Epoch [50/150], Train Loss: 42.94942977154841, Test Loss: 39.552162170410156\n",
      "Epoch [60/150], Train Loss: 41.567425036821206, Test Loss: 38.30323028564453\n",
      "Epoch [70/150], Train Loss: 40.18780790235176, Test Loss: 37.60425567626953\n",
      "Epoch [80/150], Train Loss: 38.729668163862385, Test Loss: 38.10721206665039\n",
      "Epoch [90/150], Train Loss: 37.608907805896195, Test Loss: 35.95563507080078\n",
      "Epoch [100/150], Train Loss: 36.17199785826636, Test Loss: 36.76747131347656\n",
      "Epoch [110/150], Train Loss: 35.24106774251969, Test Loss: 35.43536376953125\n",
      "Epoch [120/150], Train Loss: 34.15836851401407, Test Loss: 34.344295501708984\n",
      "Epoch [130/150], Train Loss: 32.66593624177526, Test Loss: 33.53411865234375\n",
      "Epoch [140/150], Train Loss: 31.49056071297067, Test Loss: 33.74795150756836\n",
      "Epoch [150/150], Train Loss: 30.302654478979893, Test Loss: 31.542381286621094\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.90186925168897, Test Loss: 47.803768157958984\n",
      "Epoch [20/150], Train Loss: 47.56844632508325, Test Loss: 44.65614318847656\n",
      "Epoch [30/150], Train Loss: 46.14649748254995, Test Loss: 44.00828170776367\n",
      "Epoch [40/150], Train Loss: 44.66948396026111, Test Loss: 41.511436462402344\n",
      "Epoch [50/150], Train Loss: 43.401719377861646, Test Loss: 44.534507751464844\n",
      "Epoch [60/150], Train Loss: 41.913582692380814, Test Loss: 39.04096221923828\n",
      "Epoch [70/150], Train Loss: 40.54849124345623, Test Loss: 37.77227783203125\n",
      "Epoch [80/150], Train Loss: 39.007959359591126, Test Loss: 38.66362762451172\n",
      "Epoch [90/150], Train Loss: 37.8746747626633, Test Loss: 36.47191619873047\n",
      "Epoch [100/150], Train Loss: 37.048764025578734, Test Loss: 36.42439270019531\n",
      "Epoch [110/150], Train Loss: 35.71411386708744, Test Loss: 33.17575454711914\n",
      "Epoch [120/150], Train Loss: 34.522945103879835, Test Loss: 34.56987380981445\n",
      "Epoch [130/150], Train Loss: 33.36873113288254, Test Loss: 31.858354568481445\n",
      "Epoch [140/150], Train Loss: 32.363114566490296, Test Loss: 32.15334701538086\n",
      "Epoch [150/150], Train Loss: 31.021906930892193, Test Loss: 32.433109283447266\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.74287766003218, Test Loss: 47.901268005371094\n",
      "Epoch [20/150], Train Loss: 47.241207723148534, Test Loss: 46.72460174560547\n",
      "Epoch [30/150], Train Loss: 45.72083990378458, Test Loss: 43.76188659667969\n",
      "Epoch [40/150], Train Loss: 44.29454020515817, Test Loss: 39.831138610839844\n",
      "Epoch [50/150], Train Loss: 42.92392874545738, Test Loss: 39.627777099609375\n",
      "Epoch [60/150], Train Loss: 41.291531559678376, Test Loss: 40.81206512451172\n",
      "Epoch [70/150], Train Loss: 39.8771963525991, Test Loss: 39.909690856933594\n",
      "Epoch [80/150], Train Loss: 38.51439066402248, Test Loss: 38.49616622924805\n",
      "Epoch [90/150], Train Loss: 37.40778847366083, Test Loss: 38.83719253540039\n",
      "Epoch [100/150], Train Loss: 36.270464812732136, Test Loss: 35.987003326416016\n",
      "Epoch [110/150], Train Loss: 34.97802297873575, Test Loss: 36.295326232910156\n",
      "Epoch [120/150], Train Loss: 33.7893762682305, Test Loss: 34.380821228027344\n",
      "Epoch [130/150], Train Loss: 32.683263147072715, Test Loss: 34.58189392089844\n",
      "Epoch [140/150], Train Loss: 31.702346195158412, Test Loss: 35.139564514160156\n",
      "Epoch [150/150], Train Loss: 31.059848047475345, Test Loss: 34.794944763183594\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.80668877773598, Test Loss: 48.28727340698242\n",
      "Epoch [20/150], Train Loss: 47.25723465466108, Test Loss: 46.77169418334961\n",
      "Epoch [30/150], Train Loss: 45.72949926657755, Test Loss: 42.56755447387695\n",
      "Epoch [40/150], Train Loss: 44.279285405893795, Test Loss: 40.93053436279297\n",
      "Epoch [50/150], Train Loss: 42.75839910038182, Test Loss: 41.4913444519043\n",
      "Epoch [60/150], Train Loss: 41.29665777487833, Test Loss: 41.93291473388672\n",
      "Epoch [70/150], Train Loss: 39.87418804481381, Test Loss: 39.994468688964844\n",
      "Epoch [80/150], Train Loss: 38.69943778241267, Test Loss: 36.71064758300781\n",
      "Epoch [90/150], Train Loss: 37.29713825163294, Test Loss: 37.49155807495117\n",
      "Epoch [100/150], Train Loss: 36.01222625482278, Test Loss: 37.847251892089844\n",
      "Epoch [110/150], Train Loss: 35.49227083550125, Test Loss: 34.607810974121094\n",
      "Epoch [120/150], Train Loss: 33.74670557741259, Test Loss: 35.32014846801758\n",
      "Epoch [130/150], Train Loss: 32.76408246149782, Test Loss: 35.81813049316406\n",
      "Epoch [140/150], Train Loss: 31.994648867747824, Test Loss: 31.055530548095703\n",
      "Epoch [150/150], Train Loss: 30.83866430814149, Test Loss: 31.431909561157227\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 49.11333518106429, Test Loss: 46.8709716796875\n",
      "Epoch [20/150], Train Loss: 47.767082589571594, Test Loss: 46.345924377441406\n",
      "Epoch [30/150], Train Loss: 46.346401739902184, Test Loss: 43.60693359375\n",
      "Epoch [40/150], Train Loss: 44.98095466738842, Test Loss: 41.823890686035156\n",
      "Epoch [50/150], Train Loss: 43.48065650814869, Test Loss: 41.097103118896484\n",
      "Epoch [60/150], Train Loss: 41.989720554039124, Test Loss: 41.902774810791016\n",
      "Epoch [70/150], Train Loss: 40.71958301731797, Test Loss: 41.58768844604492\n",
      "Epoch [80/150], Train Loss: 39.50625155089332, Test Loss: 38.86195755004883\n",
      "Epoch [90/150], Train Loss: 38.244056401487256, Test Loss: 39.68218994140625\n",
      "Epoch [100/150], Train Loss: 36.90958861053967, Test Loss: 38.423858642578125\n",
      "Epoch [110/150], Train Loss: 35.65438162381532, Test Loss: 35.88705825805664\n",
      "Epoch [120/150], Train Loss: 34.816398833227936, Test Loss: 36.60761642456055\n",
      "Epoch [130/150], Train Loss: 33.583293677158046, Test Loss: 36.81936264038086\n",
      "Epoch [140/150], Train Loss: 32.59990129314485, Test Loss: 33.40264129638672\n",
      "Epoch [150/150], Train Loss: 31.533009413422132, Test Loss: 34.911109924316406\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.13266110029377, Test Loss: 33.49910354614258\n",
      "Epoch [20/150], Train Loss: 25.349939946659276, Test Loss: 34.399227142333984\n",
      "Epoch [30/150], Train Loss: 24.628008964413503, Test Loss: 28.426441192626953\n",
      "Epoch [40/150], Train Loss: 24.302647162265465, Test Loss: 28.13163948059082\n",
      "Epoch [50/150], Train Loss: 23.530045731341254, Test Loss: 28.95330238342285\n",
      "Epoch [60/150], Train Loss: 23.510761035856653, Test Loss: 28.919343948364258\n",
      "Epoch [70/150], Train Loss: 23.385820545133996, Test Loss: 27.730836868286133\n",
      "Epoch [80/150], Train Loss: 23.124574273531554, Test Loss: 28.832843780517578\n",
      "Epoch [90/150], Train Loss: 22.908470428967085, Test Loss: 28.003225326538086\n",
      "Epoch [100/150], Train Loss: 22.679604114469935, Test Loss: 28.020627975463867\n",
      "Epoch [110/150], Train Loss: 21.704426656003857, Test Loss: 28.890432357788086\n",
      "Epoch [120/150], Train Loss: 21.76349790604388, Test Loss: 29.783103942871094\n",
      "Epoch [130/150], Train Loss: 21.430761699989194, Test Loss: 28.67324447631836\n",
      "Epoch [140/150], Train Loss: 22.00039753523029, Test Loss: 28.866350173950195\n",
      "Epoch [150/150], Train Loss: 22.090675866799277, Test Loss: 29.33279800415039\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.464580291998193, Test Loss: 35.037532806396484\n",
      "Epoch [20/150], Train Loss: 25.30471540357246, Test Loss: 29.80078125\n",
      "Epoch [30/150], Train Loss: 25.47902551869877, Test Loss: 27.819982528686523\n",
      "Epoch [40/150], Train Loss: 24.252307860577694, Test Loss: 32.7581672668457\n",
      "Epoch [50/150], Train Loss: 23.59857150218526, Test Loss: 31.222972869873047\n",
      "Epoch [60/150], Train Loss: 23.89269021456359, Test Loss: 29.593921661376953\n",
      "Epoch [70/150], Train Loss: 23.03596658550325, Test Loss: 28.735456466674805\n",
      "Epoch [80/150], Train Loss: 23.023900841884927, Test Loss: 28.34453010559082\n",
      "Epoch [90/150], Train Loss: 22.294513145821995, Test Loss: 30.09973907470703\n",
      "Epoch [100/150], Train Loss: 22.38038001764016, Test Loss: 28.57265853881836\n",
      "Epoch [110/150], Train Loss: 23.03604552472224, Test Loss: 33.058349609375\n",
      "Epoch [120/150], Train Loss: 21.663664201830255, Test Loss: 27.904943466186523\n",
      "Epoch [130/150], Train Loss: 21.5381778654505, Test Loss: 30.2646484375\n",
      "Epoch [140/150], Train Loss: 21.848180526983544, Test Loss: 30.81365394592285\n",
      "Epoch [150/150], Train Loss: 21.125218006821928, Test Loss: 29.212800979614258\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.046084563458553, Test Loss: 37.74852752685547\n",
      "Epoch [20/150], Train Loss: 25.842128547293242, Test Loss: 34.02016067504883\n",
      "Epoch [30/150], Train Loss: 24.65157525734823, Test Loss: 30.92717933654785\n",
      "Epoch [40/150], Train Loss: 24.226338239575995, Test Loss: 30.03523826599121\n",
      "Epoch [50/150], Train Loss: 23.935060601156266, Test Loss: 31.102916717529297\n",
      "Epoch [60/150], Train Loss: 24.511641774412062, Test Loss: 27.785581588745117\n",
      "Epoch [70/150], Train Loss: 23.376890270045546, Test Loss: 31.144948959350586\n",
      "Epoch [80/150], Train Loss: 22.827921413984456, Test Loss: 31.27470588684082\n",
      "Epoch [90/150], Train Loss: 24.6281123364558, Test Loss: 28.14975929260254\n",
      "Epoch [100/150], Train Loss: 22.702387856655434, Test Loss: 28.82347297668457\n",
      "Epoch [110/150], Train Loss: 22.69266851456439, Test Loss: 29.739177703857422\n",
      "Epoch [120/150], Train Loss: 21.763169923375866, Test Loss: 32.282325744628906\n",
      "Epoch [130/150], Train Loss: 21.544597794579676, Test Loss: 29.32289695739746\n",
      "Epoch [140/150], Train Loss: 21.739057178184634, Test Loss: 27.978933334350586\n",
      "Epoch [150/150], Train Loss: 21.235086697437723, Test Loss: 28.82895278930664\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.85796901515273, Test Loss: 34.09911346435547\n",
      "Epoch [20/150], Train Loss: 26.799095841704823, Test Loss: 29.94459342956543\n",
      "Epoch [30/150], Train Loss: 24.60490761428583, Test Loss: 31.034006118774414\n",
      "Epoch [40/150], Train Loss: 24.51537215436091, Test Loss: 28.5778865814209\n",
      "Epoch [50/150], Train Loss: 23.85985478885838, Test Loss: 28.14799690246582\n",
      "Epoch [60/150], Train Loss: 23.810629753988298, Test Loss: 30.0446834564209\n",
      "Epoch [70/150], Train Loss: 22.78132031550173, Test Loss: 32.00497055053711\n",
      "Epoch [80/150], Train Loss: 23.138201110089412, Test Loss: 29.138561248779297\n",
      "Epoch [90/150], Train Loss: 22.44211759723601, Test Loss: 28.895174026489258\n",
      "Epoch [100/150], Train Loss: 22.98970006723873, Test Loss: 29.254457473754883\n",
      "Epoch [110/150], Train Loss: 22.973007639900583, Test Loss: 28.732473373413086\n",
      "Epoch [120/150], Train Loss: 22.28519661074779, Test Loss: 29.998483657836914\n",
      "Epoch [130/150], Train Loss: 23.22522645543833, Test Loss: 29.095861434936523\n",
      "Epoch [140/150], Train Loss: 21.67936005514176, Test Loss: 29.32726287841797\n",
      "Epoch [150/150], Train Loss: 21.17060236070977, Test Loss: 35.79199981689453\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.126619426539687, Test Loss: 38.692161560058594\n",
      "Epoch [20/150], Train Loss: 25.098799771168192, Test Loss: 28.466915130615234\n",
      "Epoch [30/150], Train Loss: 25.090033890771085, Test Loss: 36.06412124633789\n",
      "Epoch [40/150], Train Loss: 24.368656002107215, Test Loss: 28.00873565673828\n",
      "Epoch [50/150], Train Loss: 24.366100029867205, Test Loss: 27.605144500732422\n",
      "Epoch [60/150], Train Loss: 24.263599702178453, Test Loss: 28.22705078125\n",
      "Epoch [70/150], Train Loss: 23.854437649836306, Test Loss: 29.007619857788086\n",
      "Epoch [80/150], Train Loss: 22.802964244905066, Test Loss: 28.886220932006836\n",
      "Epoch [90/150], Train Loss: 23.952831555976243, Test Loss: 27.822092056274414\n",
      "Epoch [100/150], Train Loss: 22.937138698140128, Test Loss: 32.02751159667969\n",
      "Epoch [110/150], Train Loss: 22.065662515358845, Test Loss: 28.96671485900879\n",
      "Epoch [120/150], Train Loss: 21.563417809908508, Test Loss: 29.510210037231445\n",
      "Epoch [130/150], Train Loss: 21.527657962236248, Test Loss: 28.06590461730957\n",
      "Epoch [140/150], Train Loss: 21.063618769411182, Test Loss: 31.001461029052734\n",
      "Epoch [150/150], Train Loss: 22.827844557214956, Test Loss: 29.435962677001953\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.963848095252867, Test Loss: 35.4601936340332\n",
      "Epoch [20/150], Train Loss: 26.373467554811572, Test Loss: 30.226530075073242\n",
      "Epoch [30/150], Train Loss: 24.70859974095079, Test Loss: 38.56141662597656\n",
      "Epoch [40/150], Train Loss: 24.68869028560451, Test Loss: 28.883316040039062\n",
      "Epoch [50/150], Train Loss: 23.99097400102459, Test Loss: 29.101390838623047\n",
      "Epoch [60/150], Train Loss: 23.435428425523103, Test Loss: 28.391563415527344\n",
      "Epoch [70/150], Train Loss: 24.083756943999745, Test Loss: 29.94375228881836\n",
      "Epoch [80/150], Train Loss: 23.437791055147766, Test Loss: 28.394855499267578\n",
      "Epoch [90/150], Train Loss: 23.424218768760806, Test Loss: 29.70885467529297\n",
      "Epoch [100/150], Train Loss: 23.085680239317846, Test Loss: 28.601285934448242\n",
      "Epoch [110/150], Train Loss: 21.696424922005075, Test Loss: 28.828460693359375\n",
      "Epoch [120/150], Train Loss: 21.325244034313766, Test Loss: 27.84339141845703\n",
      "Epoch [130/150], Train Loss: 22.610988360545676, Test Loss: 29.213531494140625\n",
      "Epoch [140/150], Train Loss: 21.912980964535574, Test Loss: 28.92656707763672\n",
      "Epoch [150/150], Train Loss: 21.744901944770188, Test Loss: 29.862707138061523\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.324860375826475, Test Loss: 38.29533767700195\n",
      "Epoch [20/150], Train Loss: 26.065267081338853, Test Loss: 33.00682067871094\n",
      "Epoch [30/150], Train Loss: 24.97369113359295, Test Loss: 29.747262954711914\n",
      "Epoch [40/150], Train Loss: 26.22905873157939, Test Loss: 28.3876953125\n",
      "Epoch [50/150], Train Loss: 24.750317526645347, Test Loss: 29.03131103515625\n",
      "Epoch [60/150], Train Loss: 25.316096146380314, Test Loss: 28.575115203857422\n",
      "Epoch [70/150], Train Loss: 23.93819133570937, Test Loss: 28.865053176879883\n",
      "Epoch [80/150], Train Loss: 24.21009689706271, Test Loss: 29.847291946411133\n",
      "Epoch [90/150], Train Loss: 24.545459028150216, Test Loss: 29.52837371826172\n",
      "Epoch [100/150], Train Loss: 24.064748226228307, Test Loss: 28.79047393798828\n",
      "Epoch [110/150], Train Loss: 23.670445288986457, Test Loss: 28.8056697845459\n",
      "Epoch [120/150], Train Loss: 24.61690924597568, Test Loss: 30.668155670166016\n",
      "Epoch [130/150], Train Loss: 23.5649118642338, Test Loss: 29.350236892700195\n",
      "Epoch [140/150], Train Loss: 25.17102213374904, Test Loss: 30.288013458251953\n",
      "Epoch [150/150], Train Loss: 23.48926823412786, Test Loss: 28.88803482055664\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.344439847352074, Test Loss: 38.91302490234375\n",
      "Epoch [20/150], Train Loss: 26.440466508709015, Test Loss: 35.051353454589844\n",
      "Epoch [30/150], Train Loss: 25.195715801051406, Test Loss: 29.389484405517578\n",
      "Epoch [40/150], Train Loss: 25.045373435098618, Test Loss: 30.780508041381836\n",
      "Epoch [50/150], Train Loss: 25.041171045772366, Test Loss: 28.611661911010742\n",
      "Epoch [60/150], Train Loss: 24.72102334694784, Test Loss: 29.32284927368164\n",
      "Epoch [70/150], Train Loss: 25.43539361172035, Test Loss: 28.367259979248047\n",
      "Epoch [80/150], Train Loss: 25.771139426309553, Test Loss: 29.500680923461914\n",
      "Epoch [90/150], Train Loss: 24.92243534776031, Test Loss: 31.194215774536133\n",
      "Epoch [100/150], Train Loss: 23.98695700598545, Test Loss: 27.934221267700195\n",
      "Epoch [110/150], Train Loss: 23.687141387189023, Test Loss: 31.466825485229492\n",
      "Epoch [120/150], Train Loss: 23.941064903384348, Test Loss: 27.0263614654541\n",
      "Epoch [130/150], Train Loss: 24.151631314637232, Test Loss: 28.00205421447754\n",
      "Epoch [140/150], Train Loss: 23.530233189317048, Test Loss: 28.918968200683594\n",
      "Epoch [150/150], Train Loss: 23.78808647530978, Test Loss: 30.03582763671875\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.31898826223905, Test Loss: 35.53668975830078\n",
      "Epoch [20/150], Train Loss: 26.08256188064325, Test Loss: 31.358877182006836\n",
      "Epoch [30/150], Train Loss: 25.915888902007556, Test Loss: 29.707618713378906\n",
      "Epoch [40/150], Train Loss: 25.933940493474243, Test Loss: 29.96227264404297\n",
      "Epoch [50/150], Train Loss: 25.537983403440382, Test Loss: 29.571216583251953\n",
      "Epoch [60/150], Train Loss: 24.99945442324779, Test Loss: 29.635164260864258\n",
      "Epoch [70/150], Train Loss: 25.30363181692655, Test Loss: 28.914636611938477\n",
      "Epoch [80/150], Train Loss: 24.87945704225634, Test Loss: 28.230709075927734\n",
      "Epoch [90/150], Train Loss: 24.073949100932136, Test Loss: 28.227130889892578\n",
      "Epoch [100/150], Train Loss: 23.814627569229877, Test Loss: 28.570598602294922\n",
      "Epoch [110/150], Train Loss: 24.176757756217583, Test Loss: 28.26110076904297\n",
      "Epoch [120/150], Train Loss: 24.108260501799037, Test Loss: 28.186960220336914\n",
      "Epoch [130/150], Train Loss: 24.35164248982414, Test Loss: 28.45553207397461\n",
      "Epoch [140/150], Train Loss: 23.301063193649544, Test Loss: 29.13475227355957\n",
      "Epoch [150/150], Train Loss: 26.268808120977685, Test Loss: 28.062131881713867\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.3271695371534, Test Loss: 31.96580696105957\n",
      "Epoch [20/150], Train Loss: 27.26851959853876, Test Loss: 29.296823501586914\n",
      "Epoch [30/150], Train Loss: 25.18984266187324, Test Loss: 28.71249771118164\n",
      "Epoch [40/150], Train Loss: 28.501670599765465, Test Loss: 28.948806762695312\n",
      "Epoch [50/150], Train Loss: 25.028811683029424, Test Loss: 29.618244171142578\n",
      "Epoch [60/150], Train Loss: 24.4319055588519, Test Loss: 27.88732147216797\n",
      "Epoch [70/150], Train Loss: 23.48507737331703, Test Loss: 27.44500160217285\n",
      "Epoch [80/150], Train Loss: 23.422555717093047, Test Loss: 34.62513732910156\n",
      "Epoch [90/150], Train Loss: 24.067314867113456, Test Loss: 27.76967430114746\n",
      "Epoch [100/150], Train Loss: 24.44268009623543, Test Loss: 30.975845336914062\n",
      "Epoch [110/150], Train Loss: 22.431336962590454, Test Loss: 28.185880661010742\n",
      "Epoch [120/150], Train Loss: 23.565310500098057, Test Loss: 31.259178161621094\n",
      "Epoch [130/150], Train Loss: 22.296088922219198, Test Loss: 30.654338836669922\n",
      "Epoch [140/150], Train Loss: 21.83971094850634, Test Loss: 28.514347076416016\n",
      "Epoch [150/150], Train Loss: 21.83250886885846, Test Loss: 29.096248626708984\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.165962406846344, Test Loss: 37.010826110839844\n",
      "Epoch [20/150], Train Loss: 26.019024533131084, Test Loss: 28.65691566467285\n",
      "Epoch [30/150], Train Loss: 25.541645437772157, Test Loss: 34.16823959350586\n",
      "Epoch [40/150], Train Loss: 24.956632682925363, Test Loss: 30.486276626586914\n",
      "Epoch [50/150], Train Loss: 24.954948600393827, Test Loss: 29.62709617614746\n",
      "Epoch [60/150], Train Loss: 24.286195948866546, Test Loss: 33.18134307861328\n",
      "Epoch [70/150], Train Loss: 25.328531884365393, Test Loss: 32.215293884277344\n",
      "Epoch [80/150], Train Loss: 24.019250988569414, Test Loss: 31.22130584716797\n",
      "Epoch [90/150], Train Loss: 23.917816255913404, Test Loss: 29.221330642700195\n",
      "Epoch [100/150], Train Loss: 23.63858474356229, Test Loss: 28.684871673583984\n",
      "Epoch [110/150], Train Loss: 23.84659131159548, Test Loss: 31.359350204467773\n",
      "Epoch [120/150], Train Loss: 23.691911215860337, Test Loss: 27.040803909301758\n",
      "Epoch [130/150], Train Loss: 23.136079200369412, Test Loss: 37.456565856933594\n",
      "Epoch [140/150], Train Loss: 23.274244708702213, Test Loss: 28.87078857421875\n",
      "Epoch [150/150], Train Loss: 24.215817310771005, Test Loss: 27.936763763427734\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.475311673273804, Test Loss: 46.21755599975586\n",
      "Epoch [20/150], Train Loss: 27.250981409041607, Test Loss: 32.49954605102539\n",
      "Epoch [30/150], Train Loss: 25.442873751530882, Test Loss: 29.562623977661133\n",
      "Epoch [40/150], Train Loss: 27.27500768567695, Test Loss: 29.02099609375\n",
      "Epoch [50/150], Train Loss: 24.445101096981862, Test Loss: 28.07830810546875\n",
      "Epoch [60/150], Train Loss: 25.539237550829277, Test Loss: 27.992961883544922\n",
      "Epoch [70/150], Train Loss: 24.304760341956968, Test Loss: 28.171098709106445\n",
      "Epoch [80/150], Train Loss: 24.121222711781986, Test Loss: 27.777423858642578\n",
      "Epoch [90/150], Train Loss: 23.54182261482614, Test Loss: 28.088211059570312\n",
      "Epoch [100/150], Train Loss: 23.96725285014168, Test Loss: 30.407398223876953\n",
      "Epoch [110/150], Train Loss: 22.9758113986156, Test Loss: 28.72180938720703\n",
      "Epoch [120/150], Train Loss: 23.027521715007843, Test Loss: 30.04737663269043\n",
      "Epoch [130/150], Train Loss: 22.861028658757444, Test Loss: 31.173511505126953\n",
      "Epoch [140/150], Train Loss: 22.866450750632364, Test Loss: 29.540695190429688\n",
      "Epoch [150/150], Train Loss: 22.743550306851745, Test Loss: 29.386159896850586\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.656288559710394, Test Loss: 34.565242767333984\n",
      "Epoch [20/150], Train Loss: 27.658925966356623, Test Loss: 29.04058074951172\n",
      "Epoch [30/150], Train Loss: 25.37775282312612, Test Loss: 29.36313819885254\n",
      "Epoch [40/150], Train Loss: 25.993321340592182, Test Loss: 29.549081802368164\n",
      "Epoch [50/150], Train Loss: 25.1174141618072, Test Loss: 34.99496078491211\n",
      "Epoch [60/150], Train Loss: 24.670432594174244, Test Loss: 29.809024810791016\n",
      "Epoch [70/150], Train Loss: 25.001236662317496, Test Loss: 28.58379364013672\n",
      "Epoch [80/150], Train Loss: 25.150138786190848, Test Loss: 27.68321990966797\n",
      "Epoch [90/150], Train Loss: 24.574339150600746, Test Loss: 27.505523681640625\n",
      "Epoch [100/150], Train Loss: 23.9979383687504, Test Loss: 28.222145080566406\n",
      "Epoch [110/150], Train Loss: 24.195233567034613, Test Loss: 28.609298706054688\n",
      "Epoch [120/150], Train Loss: 23.99629017564117, Test Loss: 27.505237579345703\n",
      "Epoch [130/150], Train Loss: 23.80572135800221, Test Loss: 29.32193946838379\n",
      "Epoch [140/150], Train Loss: 24.466694084542695, Test Loss: 28.245634078979492\n",
      "Epoch [150/150], Train Loss: 24.824052548017658, Test Loss: 28.471330642700195\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.0662558508701, Test Loss: 33.94411849975586\n",
      "Epoch [20/150], Train Loss: 26.860817349543336, Test Loss: 34.7360954284668\n",
      "Epoch [30/150], Train Loss: 26.393612845999296, Test Loss: 32.24365997314453\n",
      "Epoch [40/150], Train Loss: 25.945023758685004, Test Loss: 33.49219512939453\n",
      "Epoch [50/150], Train Loss: 24.926257499319608, Test Loss: 29.403602600097656\n",
      "Epoch [60/150], Train Loss: 25.655823735721775, Test Loss: 28.696475982666016\n",
      "Epoch [70/150], Train Loss: 24.625620344818614, Test Loss: 28.987224578857422\n",
      "Epoch [80/150], Train Loss: 25.5278591593758, Test Loss: 28.914527893066406\n",
      "Epoch [90/150], Train Loss: 24.497762411148823, Test Loss: 27.96074676513672\n",
      "Epoch [100/150], Train Loss: 24.360430883188716, Test Loss: 28.839948654174805\n",
      "Epoch [110/150], Train Loss: 24.070556778204246, Test Loss: 30.028181076049805\n",
      "Epoch [120/150], Train Loss: 24.56601912701716, Test Loss: 28.933738708496094\n",
      "Epoch [130/150], Train Loss: 24.883740590830318, Test Loss: 28.3774356842041\n",
      "Epoch [140/150], Train Loss: 24.868499561997712, Test Loss: 28.602941513061523\n",
      "Epoch [150/150], Train Loss: 24.147537087612466, Test Loss: 29.058382034301758\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.724420047197185, Test Loss: 41.48975372314453\n",
      "Epoch [20/150], Train Loss: 27.995977783203124, Test Loss: 34.123016357421875\n",
      "Epoch [30/150], Train Loss: 26.133140538950435, Test Loss: 31.097631454467773\n",
      "Epoch [40/150], Train Loss: 25.57163317946137, Test Loss: 29.213315963745117\n",
      "Epoch [50/150], Train Loss: 24.79311489668049, Test Loss: 29.211076736450195\n",
      "Epoch [60/150], Train Loss: 25.990315734362994, Test Loss: 28.331998825073242\n",
      "Epoch [70/150], Train Loss: 24.57128156443111, Test Loss: 34.20032501220703\n",
      "Epoch [80/150], Train Loss: 24.90767667801654, Test Loss: 31.915311813354492\n",
      "Epoch [90/150], Train Loss: 24.671034422077117, Test Loss: 29.349754333496094\n",
      "Epoch [100/150], Train Loss: 24.197821864143748, Test Loss: 29.93697738647461\n",
      "Epoch [110/150], Train Loss: 23.939598020960073, Test Loss: 30.116531372070312\n",
      "Epoch [120/150], Train Loss: 23.3388270956571, Test Loss: 28.89501953125\n",
      "Epoch [130/150], Train Loss: 24.27411057519131, Test Loss: 31.380874633789062\n",
      "Epoch [140/150], Train Loss: 24.34444800204918, Test Loss: 27.626096725463867\n",
      "Epoch [150/150], Train Loss: 22.910994995617475, Test Loss: 29.59617805480957\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.505980920009925, Test Loss: 51.545738220214844\n",
      "Epoch [20/150], Train Loss: 27.355170909693985, Test Loss: 40.462074279785156\n",
      "Epoch [30/150], Train Loss: 26.93533447140553, Test Loss: 37.22391128540039\n",
      "Epoch [40/150], Train Loss: 26.045287410548475, Test Loss: 32.05671691894531\n",
      "Epoch [50/150], Train Loss: 25.897294566670404, Test Loss: 40.95388412475586\n",
      "Epoch [60/150], Train Loss: 26.579360480386704, Test Loss: 31.05164337158203\n",
      "Epoch [70/150], Train Loss: 25.361571377613505, Test Loss: 33.09291458129883\n",
      "Epoch [80/150], Train Loss: 26.322136788290056, Test Loss: 68.07478332519531\n",
      "Epoch [90/150], Train Loss: 24.692102188360497, Test Loss: 29.387752532958984\n",
      "Epoch [100/150], Train Loss: 24.981115372454532, Test Loss: 29.08562660217285\n",
      "Epoch [110/150], Train Loss: 25.490668875272156, Test Loss: 29.06524658203125\n",
      "Epoch [120/150], Train Loss: 25.554242743820442, Test Loss: 28.92445182800293\n",
      "Epoch [130/150], Train Loss: 24.772775956450914, Test Loss: 27.622879028320312\n",
      "Epoch [140/150], Train Loss: 25.196457878488008, Test Loss: 27.823049545288086\n",
      "Epoch [150/150], Train Loss: 25.373711370249264, Test Loss: 28.563011169433594\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.123706948952595, Test Loss: 45.59511184692383\n",
      "Epoch [20/150], Train Loss: 27.595022476696577, Test Loss: 36.32484817504883\n",
      "Epoch [30/150], Train Loss: 26.52351840285004, Test Loss: 44.02412414550781\n",
      "Epoch [40/150], Train Loss: 26.008219265546956, Test Loss: 31.407278060913086\n",
      "Epoch [50/150], Train Loss: 27.480238598682842, Test Loss: 30.727018356323242\n",
      "Epoch [60/150], Train Loss: 26.054372837504403, Test Loss: 28.799795150756836\n",
      "Epoch [70/150], Train Loss: 25.20138653614482, Test Loss: 30.839441299438477\n",
      "Epoch [80/150], Train Loss: 25.946425559872488, Test Loss: 29.688697814941406\n",
      "Epoch [90/150], Train Loss: 24.835629353757763, Test Loss: 30.041732788085938\n",
      "Epoch [100/150], Train Loss: 26.60077015610992, Test Loss: 28.039274215698242\n",
      "Epoch [110/150], Train Loss: 24.587678290195154, Test Loss: 26.945589065551758\n",
      "Epoch [120/150], Train Loss: 24.827886049864723, Test Loss: 28.509809494018555\n",
      "Epoch [130/150], Train Loss: 25.3392448612901, Test Loss: 28.431306838989258\n",
      "Epoch [140/150], Train Loss: 24.625642770235658, Test Loss: 29.49054527282715\n",
      "Epoch [150/150], Train Loss: 24.33387997111336, Test Loss: 28.53705596923828\n",
      "Training model with hidden size 64, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.157158998583185, Test Loss: 37.138668060302734\n",
      "Epoch [20/150], Train Loss: 27.88984869659924, Test Loss: 37.049537658691406\n",
      "Epoch [30/150], Train Loss: 26.518830133656987, Test Loss: 34.47876739501953\n",
      "Epoch [40/150], Train Loss: 25.742614433413646, Test Loss: 31.850332260131836\n",
      "Epoch [50/150], Train Loss: 26.26892107353836, Test Loss: 30.263925552368164\n",
      "Epoch [60/150], Train Loss: 26.323023955548397, Test Loss: 28.606924057006836\n",
      "Epoch [70/150], Train Loss: 25.92805669815814, Test Loss: 28.833324432373047\n",
      "Epoch [80/150], Train Loss: 25.780160297331264, Test Loss: 36.78520202636719\n",
      "Epoch [90/150], Train Loss: 25.063892727210874, Test Loss: 29.973670959472656\n",
      "Epoch [100/150], Train Loss: 25.100044688240427, Test Loss: 29.233064651489258\n",
      "Epoch [110/150], Train Loss: 25.47946045672307, Test Loss: 29.12249183654785\n",
      "Epoch [120/150], Train Loss: 25.2024537696213, Test Loss: 29.31332778930664\n",
      "Epoch [130/150], Train Loss: 24.628951507318217, Test Loss: 29.44117546081543\n",
      "Epoch [140/150], Train Loss: 25.078219560716974, Test Loss: 29.018861770629883\n",
      "Epoch [150/150], Train Loss: 24.81867833372022, Test Loss: 28.155454635620117\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 42.77142621650071, Test Loss: 41.95122562755238\n",
      "Epoch [20/150], Train Loss: 36.07804296524798, Test Loss: 35.39581192313851\n",
      "Epoch [30/150], Train Loss: 31.631689115430486, Test Loss: 31.9202830079314\n",
      "Epoch [40/150], Train Loss: 28.748806224885534, Test Loss: 30.308277724625228\n",
      "Epoch [50/150], Train Loss: 26.987903594970703, Test Loss: 29.0468097538143\n",
      "Epoch [60/150], Train Loss: 26.94815288606237, Test Loss: 28.06333824256798\n",
      "Epoch [70/150], Train Loss: 26.398113175689197, Test Loss: 28.189776284354075\n",
      "Epoch [80/150], Train Loss: 24.810940432939372, Test Loss: 28.34047384385939\n",
      "Epoch [90/150], Train Loss: 24.112913569466013, Test Loss: 28.202565255103174\n",
      "Epoch [100/150], Train Loss: 22.823729612006517, Test Loss: 27.306537628173828\n",
      "Epoch [110/150], Train Loss: 22.802854994476817, Test Loss: 28.350070978139904\n",
      "Epoch [120/150], Train Loss: 25.985816911791193, Test Loss: 28.129888485004376\n",
      "Epoch [130/150], Train Loss: 24.511161316418256, Test Loss: 28.2975661537864\n",
      "Epoch [140/150], Train Loss: 22.26611807776279, Test Loss: 27.65953579196682\n",
      "Epoch [150/150], Train Loss: 24.270330985647732, Test Loss: 27.84894309105811\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.57254468573898, Test Loss: 41.25376892089844\n",
      "Epoch [20/150], Train Loss: 36.6369200909724, Test Loss: 35.503989355904714\n",
      "Epoch [30/150], Train Loss: 31.257909318267323, Test Loss: 35.054751210398486\n",
      "Epoch [40/150], Train Loss: 27.958848471719712, Test Loss: 29.571955668461786\n",
      "Epoch [50/150], Train Loss: 26.381312883095664, Test Loss: 29.576875463708653\n",
      "Epoch [60/150], Train Loss: 25.831744441048045, Test Loss: 28.8640166939079\n",
      "Epoch [70/150], Train Loss: 25.121092661873238, Test Loss: 28.203372781926934\n",
      "Epoch [80/150], Train Loss: 24.82814664371678, Test Loss: 27.93264547570959\n",
      "Epoch [90/150], Train Loss: 24.275032750114065, Test Loss: 28.339203871689833\n",
      "Epoch [100/150], Train Loss: 23.938568928202645, Test Loss: 27.971698438966428\n",
      "Epoch [110/150], Train Loss: 22.81722198236184, Test Loss: 27.732936215091062\n",
      "Epoch [120/150], Train Loss: 23.86502172751505, Test Loss: 28.751768607597846\n",
      "Epoch [130/150], Train Loss: 23.092301465644212, Test Loss: 28.676273816591735\n",
      "Epoch [140/150], Train Loss: 23.223385282422676, Test Loss: 28.433826768553104\n",
      "Epoch [150/150], Train Loss: 23.37622826372991, Test Loss: 28.45972561526608\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.972084233018215, Test Loss: 41.25135481202757\n",
      "Epoch [20/150], Train Loss: 36.91463340384061, Test Loss: 35.94565463375736\n",
      "Epoch [30/150], Train Loss: 32.29438781112921, Test Loss: 33.1886611789852\n",
      "Epoch [40/150], Train Loss: 29.18960283623367, Test Loss: 30.15758397981718\n",
      "Epoch [50/150], Train Loss: 26.432371958748238, Test Loss: 28.421335889147475\n",
      "Epoch [60/150], Train Loss: 27.616999782499718, Test Loss: 28.1963800207361\n",
      "Epoch [70/150], Train Loss: 24.931030836261687, Test Loss: 27.823598118571493\n",
      "Epoch [80/150], Train Loss: 24.983086076329965, Test Loss: 27.235508163253982\n",
      "Epoch [90/150], Train Loss: 24.041304985421604, Test Loss: 27.35984656098601\n",
      "Epoch [100/150], Train Loss: 26.322794967401222, Test Loss: 27.833699536013913\n",
      "Epoch [110/150], Train Loss: 24.485628440731862, Test Loss: 28.3944304577716\n",
      "Epoch [120/150], Train Loss: 22.970376862072555, Test Loss: 28.280969718834022\n",
      "Epoch [130/150], Train Loss: 23.310490411226866, Test Loss: 28.287308086048473\n",
      "Epoch [140/150], Train Loss: 24.473130923411887, Test Loss: 27.677015923834468\n",
      "Epoch [150/150], Train Loss: 24.123654712614467, Test Loss: 29.174003625844982\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 42.96738715249984, Test Loss: 41.39148172155603\n",
      "Epoch [20/150], Train Loss: 35.789090628702134, Test Loss: 36.5439476904931\n",
      "Epoch [30/150], Train Loss: 31.09225866599161, Test Loss: 31.343294094135235\n",
      "Epoch [40/150], Train Loss: 28.67025951322962, Test Loss: 30.30845597502473\n",
      "Epoch [50/150], Train Loss: 26.662877236037957, Test Loss: 28.59558593452751\n",
      "Epoch [60/150], Train Loss: 25.604669921124568, Test Loss: 27.824478322809394\n",
      "Epoch [70/150], Train Loss: 24.69536515532947, Test Loss: 29.998259185196517\n",
      "Epoch [80/150], Train Loss: 24.351331091708825, Test Loss: 28.722495264821237\n",
      "Epoch [90/150], Train Loss: 24.35912131012463, Test Loss: 28.169600028496284\n",
      "Epoch [100/150], Train Loss: 23.50971064958416, Test Loss: 28.779093432736087\n",
      "Epoch [110/150], Train Loss: 22.838571995594464, Test Loss: 28.870579162201324\n",
      "Epoch [120/150], Train Loss: 23.700896141177317, Test Loss: 28.661876975715934\n",
      "Epoch [130/150], Train Loss: 22.25564944157835, Test Loss: 27.66591371808733\n",
      "Epoch [140/150], Train Loss: 21.733050418290937, Test Loss: 27.950550772927024\n",
      "Epoch [150/150], Train Loss: 22.490638395215644, Test Loss: 29.041457015198546\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 42.96476280337475, Test Loss: 40.337712523225065\n",
      "Epoch [20/150], Train Loss: 35.80157962861608, Test Loss: 36.184520647123264\n",
      "Epoch [30/150], Train Loss: 31.242640510934297, Test Loss: 32.49583449921051\n",
      "Epoch [40/150], Train Loss: 28.54524486729356, Test Loss: 30.248587496868975\n",
      "Epoch [50/150], Train Loss: 27.122053171376713, Test Loss: 28.728097246838853\n",
      "Epoch [60/150], Train Loss: 25.45165090717253, Test Loss: 27.759144522927024\n",
      "Epoch [70/150], Train Loss: 25.0688123921879, Test Loss: 27.653036018470665\n",
      "Epoch [80/150], Train Loss: 25.25065721605645, Test Loss: 27.746689833603895\n",
      "Epoch [90/150], Train Loss: 24.33460753393955, Test Loss: 28.122038085739334\n",
      "Epoch [100/150], Train Loss: 23.934145893034387, Test Loss: 27.989620951863078\n",
      "Epoch [110/150], Train Loss: 23.5316107890645, Test Loss: 28.509103601629082\n",
      "Epoch [120/150], Train Loss: 22.91883364818135, Test Loss: 28.243410630659625\n",
      "Epoch [130/150], Train Loss: 23.61047957373447, Test Loss: 28.604950619982436\n",
      "Epoch [140/150], Train Loss: 23.987072466240555, Test Loss: 27.84169083137017\n",
      "Epoch [150/150], Train Loss: 22.778198767490075, Test Loss: 28.757213344821682\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.911264413302064, Test Loss: 40.538417915245155\n",
      "Epoch [20/150], Train Loss: 37.282217807457094, Test Loss: 37.927834498417845\n",
      "Epoch [30/150], Train Loss: 32.86861909334777, Test Loss: 34.122145863322466\n",
      "Epoch [40/150], Train Loss: 29.679611193547483, Test Loss: 30.821115518545177\n",
      "Epoch [50/150], Train Loss: 27.57152159643955, Test Loss: 29.84059068754122\n",
      "Epoch [60/150], Train Loss: 25.253173002649525, Test Loss: 29.718790822214896\n",
      "Epoch [70/150], Train Loss: 25.745490352443007, Test Loss: 28.0194139356737\n",
      "Epoch [80/150], Train Loss: 25.195777424046252, Test Loss: 28.02719497680664\n",
      "Epoch [90/150], Train Loss: 23.682155640398868, Test Loss: 28.308238388656022\n",
      "Epoch [100/150], Train Loss: 25.18693687564037, Test Loss: 28.161111732581993\n",
      "Epoch [110/150], Train Loss: 23.01523099180128, Test Loss: 28.208869339583757\n",
      "Epoch [120/150], Train Loss: 23.570912758248753, Test Loss: 28.134256784017985\n",
      "Epoch [130/150], Train Loss: 23.711924080770522, Test Loss: 28.631734947105507\n",
      "Epoch [140/150], Train Loss: 24.765381447213596, Test Loss: 27.954584196016388\n",
      "Epoch [150/150], Train Loss: 24.281734429031122, Test Loss: 28.6709846397499\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.02911687131788, Test Loss: 42.85455421348671\n",
      "Epoch [20/150], Train Loss: 36.501935045836404, Test Loss: 36.511312187492074\n",
      "Epoch [30/150], Train Loss: 31.35988501877081, Test Loss: 35.75549923289906\n",
      "Epoch [40/150], Train Loss: 28.164067821815365, Test Loss: 29.793869588282202\n",
      "Epoch [50/150], Train Loss: 26.840596289712874, Test Loss: 27.349470336715896\n",
      "Epoch [60/150], Train Loss: 25.110253881235593, Test Loss: 27.78269525007768\n",
      "Epoch [70/150], Train Loss: 26.12177659331775, Test Loss: 28.361676649613813\n",
      "Epoch [80/150], Train Loss: 25.08465492373607, Test Loss: 27.9766034460687\n",
      "Epoch [90/150], Train Loss: 25.361358974019033, Test Loss: 27.734831227884666\n",
      "Epoch [100/150], Train Loss: 24.162456181010263, Test Loss: 27.69793500528707\n",
      "Epoch [110/150], Train Loss: 27.196039606313235, Test Loss: 28.34773502102146\n",
      "Epoch [120/150], Train Loss: 24.97357033276167, Test Loss: 27.96912673851112\n",
      "Epoch [130/150], Train Loss: 23.736837999937965, Test Loss: 27.846477037900453\n",
      "Epoch [140/150], Train Loss: 25.043366801152462, Test Loss: 28.23682874208921\n",
      "Epoch [150/150], Train Loss: 23.413169266747648, Test Loss: 27.2357441295277\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.057801393602716, Test Loss: 40.003151385815116\n",
      "Epoch [20/150], Train Loss: 36.45596323482326, Test Loss: 36.3455756546615\n",
      "Epoch [30/150], Train Loss: 31.810786713146772, Test Loss: 32.12329906612248\n",
      "Epoch [40/150], Train Loss: 28.533225719264298, Test Loss: 30.13763192412141\n",
      "Epoch [50/150], Train Loss: 27.950768905389506, Test Loss: 28.13645513955649\n",
      "Epoch [60/150], Train Loss: 26.17962483890721, Test Loss: 27.96242178879775\n",
      "Epoch [70/150], Train Loss: 25.82121919725762, Test Loss: 27.804516978078073\n",
      "Epoch [80/150], Train Loss: 25.770983786661116, Test Loss: 28.721435621187286\n",
      "Epoch [90/150], Train Loss: 24.83389958553627, Test Loss: 28.851018212058328\n",
      "Epoch [100/150], Train Loss: 24.463903183233544, Test Loss: 28.152966338318663\n",
      "Epoch [110/150], Train Loss: 23.657436896152184, Test Loss: 28.121488075751763\n",
      "Epoch [120/150], Train Loss: 23.55627004904825, Test Loss: 27.30862283087396\n",
      "Epoch [130/150], Train Loss: 23.81288426508669, Test Loss: 27.369564576582476\n",
      "Epoch [140/150], Train Loss: 22.9262824887135, Test Loss: 28.221584592546737\n",
      "Epoch [150/150], Train Loss: 24.566296799456488, Test Loss: 28.85267921546837\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 43.637674400454664, Test Loss: 42.15139711057985\n",
      "Epoch [20/150], Train Loss: 37.6063660418401, Test Loss: 34.34020200952307\n",
      "Epoch [30/150], Train Loss: 32.673797032090484, Test Loss: 32.43592730435458\n",
      "Epoch [40/150], Train Loss: 29.208187828689326, Test Loss: 30.3753195428229\n",
      "Epoch [50/150], Train Loss: 27.216530721695698, Test Loss: 29.310257626818373\n",
      "Epoch [60/150], Train Loss: 26.688295220547037, Test Loss: 28.8150695206283\n",
      "Epoch [70/150], Train Loss: 24.73988120907643, Test Loss: 28.18389540833312\n",
      "Epoch [80/150], Train Loss: 24.975296058029425, Test Loss: 28.106835848325257\n",
      "Epoch [90/150], Train Loss: 24.18597683515705, Test Loss: 28.385636044787123\n",
      "Epoch [100/150], Train Loss: 24.664265185496845, Test Loss: 28.239380972726003\n",
      "Epoch [110/150], Train Loss: 23.877846520846006, Test Loss: 28.059941725297406\n",
      "Epoch [120/150], Train Loss: 24.212018929153192, Test Loss: 28.839293145514155\n",
      "Epoch [130/150], Train Loss: 23.141944403726548, Test Loss: 27.896428516932897\n",
      "Epoch [140/150], Train Loss: 22.968084235269515, Test Loss: 27.877322655219537\n",
      "Epoch [150/150], Train Loss: 23.93718852684146, Test Loss: 28.56782514398748\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.174620487650888, Test Loss: 33.417543237859554\n",
      "Epoch [20/150], Train Loss: 26.066849692923125, Test Loss: 28.31541866451115\n",
      "Epoch [30/150], Train Loss: 24.967000042024207, Test Loss: 28.743009146157796\n",
      "Epoch [40/150], Train Loss: 25.93221737595855, Test Loss: 27.287157455048003\n",
      "Epoch [50/150], Train Loss: 24.768736942478867, Test Loss: 30.646171594594982\n",
      "Epoch [60/150], Train Loss: 25.804632174382444, Test Loss: 28.508345888806627\n",
      "Epoch [70/150], Train Loss: 24.476655284693983, Test Loss: 28.811440901322797\n",
      "Epoch [80/150], Train Loss: 24.596912424681616, Test Loss: 30.671777130721452\n",
      "Epoch [90/150], Train Loss: 24.489196696046925, Test Loss: 28.497581036059888\n",
      "Epoch [100/150], Train Loss: 24.10999210545274, Test Loss: 30.039770497904197\n",
      "Epoch [110/150], Train Loss: 25.215208885317942, Test Loss: 27.81252756985751\n",
      "Epoch [120/150], Train Loss: 26.29399689220991, Test Loss: 27.49560326415223\n",
      "Epoch [130/150], Train Loss: 23.74121229453165, Test Loss: 30.817570351935053\n",
      "Epoch [140/150], Train Loss: 22.71915774110888, Test Loss: 30.17522214914297\n",
      "Epoch [150/150], Train Loss: 23.097333839291434, Test Loss: 28.334406840336786\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.89673090450099, Test Loss: 31.19399779183524\n",
      "Epoch [20/150], Train Loss: 27.171478459092437, Test Loss: 29.891980753316506\n",
      "Epoch [30/150], Train Loss: 26.09161497022285, Test Loss: 28.32438632420131\n",
      "Epoch [40/150], Train Loss: 24.97074938914815, Test Loss: 28.669115834421927\n",
      "Epoch [50/150], Train Loss: 24.914269725611952, Test Loss: 28.711784338022206\n",
      "Epoch [60/150], Train Loss: 24.651805790135118, Test Loss: 28.217312602253703\n",
      "Epoch [70/150], Train Loss: 25.696815815909964, Test Loss: 28.27268697069837\n",
      "Epoch [80/150], Train Loss: 24.863312693111233, Test Loss: 27.961186446152723\n",
      "Epoch [90/150], Train Loss: 23.913285871411933, Test Loss: 27.773073716597125\n",
      "Epoch [100/150], Train Loss: 24.213613209958936, Test Loss: 29.239074806114296\n",
      "Epoch [110/150], Train Loss: 23.14835362043537, Test Loss: 28.53974339869115\n",
      "Epoch [120/150], Train Loss: 24.771242667026208, Test Loss: 28.04489098586045\n",
      "Epoch [130/150], Train Loss: 24.28427670588259, Test Loss: 27.468253544398717\n",
      "Epoch [140/150], Train Loss: 23.613733279118772, Test Loss: 27.980064540714412\n",
      "Epoch [150/150], Train Loss: 22.82151316033035, Test Loss: 28.28631324272651\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.66992756577789, Test Loss: 30.069469749153434\n",
      "Epoch [20/150], Train Loss: 25.768265821112962, Test Loss: 28.486884501073266\n",
      "Epoch [30/150], Train Loss: 25.815578385650134, Test Loss: 29.592812575303114\n",
      "Epoch [40/150], Train Loss: 25.23238504128378, Test Loss: 28.89096512732568\n",
      "Epoch [50/150], Train Loss: 25.16974591114482, Test Loss: 28.396898690756267\n",
      "Epoch [60/150], Train Loss: 24.560437655839763, Test Loss: 28.052269650744154\n",
      "Epoch [70/150], Train Loss: 24.02277696953445, Test Loss: 28.04959978376116\n",
      "Epoch [80/150], Train Loss: 25.932086863283253, Test Loss: 27.95226508301574\n",
      "Epoch [90/150], Train Loss: 23.964819038891402, Test Loss: 28.789153582089906\n",
      "Epoch [100/150], Train Loss: 23.69926734048812, Test Loss: 28.84291222807649\n",
      "Epoch [110/150], Train Loss: 25.24220325907723, Test Loss: 28.927819660731725\n",
      "Epoch [120/150], Train Loss: 25.31140998465116, Test Loss: 29.4705056524896\n",
      "Epoch [130/150], Train Loss: 23.529170045696322, Test Loss: 28.028982806515383\n",
      "Epoch [140/150], Train Loss: 24.06748600944144, Test Loss: 29.997251783098495\n",
      "Epoch [150/150], Train Loss: 23.44132152619909, Test Loss: 28.469452028150684\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.587408809974544, Test Loss: 31.00460245702174\n",
      "Epoch [20/150], Train Loss: 25.949243089019276, Test Loss: 29.39520110093154\n",
      "Epoch [30/150], Train Loss: 25.80861638491271, Test Loss: 27.72453687098119\n",
      "Epoch [40/150], Train Loss: 25.354282960735382, Test Loss: 28.516257818643147\n",
      "Epoch [50/150], Train Loss: 25.999388822962025, Test Loss: 28.331046612231763\n",
      "Epoch [60/150], Train Loss: 24.74729082701636, Test Loss: 28.196939369300743\n",
      "Epoch [70/150], Train Loss: 24.58107500545314, Test Loss: 27.219515924329883\n",
      "Epoch [80/150], Train Loss: 24.174105109543095, Test Loss: 27.436728935737115\n",
      "Epoch [90/150], Train Loss: 26.52362952310531, Test Loss: 29.781296891051454\n",
      "Epoch [100/150], Train Loss: 24.898227591592757, Test Loss: 27.233145404171633\n",
      "Epoch [110/150], Train Loss: 24.37931068920698, Test Loss: 31.094127605487774\n",
      "Epoch [120/150], Train Loss: 24.649565262090963, Test Loss: 28.652907681155515\n",
      "Epoch [130/150], Train Loss: 23.547855915007045, Test Loss: 28.56543236274224\n",
      "Epoch [140/150], Train Loss: 23.143335236095993, Test Loss: 28.329893681910132\n",
      "Epoch [150/150], Train Loss: 23.941087391337412, Test Loss: 28.740503955197024\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.93758339178367, Test Loss: 31.074509633051886\n",
      "Epoch [20/150], Train Loss: 27.307956620513416, Test Loss: 34.51217136135349\n",
      "Epoch [30/150], Train Loss: 25.954998891861713, Test Loss: 29.26407628245168\n",
      "Epoch [40/150], Train Loss: 26.567915444295913, Test Loss: 29.521140432977056\n",
      "Epoch [50/150], Train Loss: 25.05586784863081, Test Loss: 27.599804890620245\n",
      "Epoch [60/150], Train Loss: 25.823522004924836, Test Loss: 27.836940517673245\n",
      "Epoch [70/150], Train Loss: 24.58106737918541, Test Loss: 29.70866094316755\n",
      "Epoch [80/150], Train Loss: 25.316569812962268, Test Loss: 28.82564403484394\n",
      "Epoch [90/150], Train Loss: 23.862853234713196, Test Loss: 28.964472733534777\n",
      "Epoch [100/150], Train Loss: 25.200516929001104, Test Loss: 27.848834743747464\n",
      "Epoch [110/150], Train Loss: 23.958921238633454, Test Loss: 28.73515661660727\n",
      "Epoch [120/150], Train Loss: 24.85295428917056, Test Loss: 27.716642627468357\n",
      "Epoch [130/150], Train Loss: 25.017173285562485, Test Loss: 29.169650238829774\n",
      "Epoch [140/150], Train Loss: 25.151091716328605, Test Loss: 27.31823511247511\n",
      "Epoch [150/150], Train Loss: 24.946555788008894, Test Loss: 29.431400299072266\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.610029864702067, Test Loss: 27.92165027965199\n",
      "Epoch [20/150], Train Loss: 25.9678843513864, Test Loss: 29.062544389204547\n",
      "Epoch [30/150], Train Loss: 26.81344392495077, Test Loss: 28.71247403033368\n",
      "Epoch [40/150], Train Loss: 25.925780662161404, Test Loss: 28.84467969621931\n",
      "Epoch [50/150], Train Loss: 27.71451593617924, Test Loss: 27.759163621184115\n",
      "Epoch [60/150], Train Loss: 25.738016378684122, Test Loss: 28.14736710585557\n",
      "Epoch [70/150], Train Loss: 26.879554198218173, Test Loss: 27.45248368498567\n",
      "Epoch [80/150], Train Loss: 25.529856272212793, Test Loss: 28.345032506174856\n",
      "Epoch [90/150], Train Loss: 24.861269791399845, Test Loss: 27.91779644458325\n",
      "Epoch [100/150], Train Loss: 25.06382067320777, Test Loss: 28.43573369608297\n",
      "Epoch [110/150], Train Loss: 24.84092312171811, Test Loss: 27.5573240007673\n",
      "Epoch [120/150], Train Loss: 23.7765088503478, Test Loss: 27.7810822276326\n",
      "Epoch [130/150], Train Loss: 25.903654855196592, Test Loss: 27.05512983148748\n",
      "Epoch [140/150], Train Loss: 23.53385987203629, Test Loss: 27.953701316536247\n",
      "Epoch [150/150], Train Loss: 24.649805431678647, Test Loss: 28.84540166483297\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.657103278988696, Test Loss: 33.68212097960633\n",
      "Epoch [20/150], Train Loss: 26.901708396536407, Test Loss: 29.59709187297078\n",
      "Epoch [30/150], Train Loss: 26.24729097710281, Test Loss: 42.03368139886237\n",
      "Epoch [40/150], Train Loss: 26.494299472746302, Test Loss: 31.46531221464083\n",
      "Epoch [50/150], Train Loss: 27.16302857946177, Test Loss: 30.456171927514013\n",
      "Epoch [60/150], Train Loss: 25.510633843844055, Test Loss: 28.580988339015416\n",
      "Epoch [70/150], Train Loss: 25.4320776767418, Test Loss: 28.07879383533032\n",
      "Epoch [80/150], Train Loss: 26.33913863760526, Test Loss: 27.677557610846186\n",
      "Epoch [90/150], Train Loss: 25.894241333007812, Test Loss: 30.462500683673017\n",
      "Epoch [100/150], Train Loss: 24.985675192660974, Test Loss: 30.08610757604822\n",
      "Epoch [110/150], Train Loss: 25.105919734767227, Test Loss: 34.548987772557645\n",
      "Epoch [120/150], Train Loss: 24.793391993788422, Test Loss: 32.839684919877485\n",
      "Epoch [130/150], Train Loss: 24.087821385117827, Test Loss: 30.513137321967584\n",
      "Epoch [140/150], Train Loss: 24.87458190917969, Test Loss: 29.213350741894214\n",
      "Epoch [150/150], Train Loss: 24.10803455290247, Test Loss: 30.01435676178375\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.699914050493085, Test Loss: 62.721960389768924\n",
      "Epoch [20/150], Train Loss: 27.40156693380387, Test Loss: 40.00504738943918\n",
      "Epoch [30/150], Train Loss: 26.48933452543665, Test Loss: 28.770976574389966\n",
      "Epoch [40/150], Train Loss: 26.08929928638896, Test Loss: 30.314802541361228\n",
      "Epoch [50/150], Train Loss: 25.768222408607357, Test Loss: 30.224572119774756\n",
      "Epoch [60/150], Train Loss: 27.78786030753714, Test Loss: 27.66774066082843\n",
      "Epoch [70/150], Train Loss: 26.886247659902104, Test Loss: 27.841671312010135\n",
      "Epoch [80/150], Train Loss: 25.88205400685795, Test Loss: 30.430310980066075\n",
      "Epoch [90/150], Train Loss: 26.197190412927846, Test Loss: 27.629208453289873\n",
      "Epoch [100/150], Train Loss: 25.53747658026023, Test Loss: 28.51172940142743\n",
      "Epoch [110/150], Train Loss: 24.855475672737498, Test Loss: 30.37414704359971\n",
      "Epoch [120/150], Train Loss: 24.376889238201205, Test Loss: 29.315733426577086\n",
      "Epoch [130/150], Train Loss: 24.732055263831967, Test Loss: 29.049127826442966\n",
      "Epoch [140/150], Train Loss: 25.219991890328828, Test Loss: 29.10496511087789\n",
      "Epoch [150/150], Train Loss: 24.578147775618756, Test Loss: 29.058629543750317\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.15189880621238, Test Loss: 31.10755915456004\n",
      "Epoch [20/150], Train Loss: 27.14763632602379, Test Loss: 32.12049328197133\n",
      "Epoch [30/150], Train Loss: 26.20619773239386, Test Loss: 30.608344189532392\n",
      "Epoch [40/150], Train Loss: 27.531984910808625, Test Loss: 31.026951133430778\n",
      "Epoch [50/150], Train Loss: 26.140600748531153, Test Loss: 28.779336012803114\n",
      "Epoch [60/150], Train Loss: 25.659703639296236, Test Loss: 28.945124960564947\n",
      "Epoch [70/150], Train Loss: 25.723267833522108, Test Loss: 28.846379193392668\n",
      "Epoch [80/150], Train Loss: 25.98673131974017, Test Loss: 28.15576159489619\n",
      "Epoch [90/150], Train Loss: 25.01393739043689, Test Loss: 28.8905948539833\n",
      "Epoch [100/150], Train Loss: 25.672968423561972, Test Loss: 27.720931759128323\n",
      "Epoch [110/150], Train Loss: 26.40492178494813, Test Loss: 28.61948518629198\n",
      "Epoch [120/150], Train Loss: 24.661497078567255, Test Loss: 28.46479745344682\n",
      "Epoch [130/150], Train Loss: 26.398485896626458, Test Loss: 27.016283580235072\n",
      "Epoch [140/150], Train Loss: 25.34184305785132, Test Loss: 28.055614000791078\n",
      "Epoch [150/150], Train Loss: 25.778230229362112, Test Loss: 30.618284671337573\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.985016356921587, Test Loss: 30.313388527213753\n",
      "Epoch [20/150], Train Loss: 27.65149811291304, Test Loss: 31.118960838813287\n",
      "Epoch [30/150], Train Loss: 25.385484976846662, Test Loss: 31.803692309887378\n",
      "Epoch [40/150], Train Loss: 26.425340058373624, Test Loss: 30.755112288834212\n",
      "Epoch [50/150], Train Loss: 25.43639814032883, Test Loss: 27.885325097418452\n",
      "Epoch [60/150], Train Loss: 26.670548373363058, Test Loss: 27.752881235890573\n",
      "Epoch [70/150], Train Loss: 25.49873335791416, Test Loss: 28.836255804284825\n",
      "Epoch [80/150], Train Loss: 25.814525929435355, Test Loss: 28.758444748915636\n",
      "Epoch [90/150], Train Loss: 25.273386389310243, Test Loss: 28.61018819932814\n",
      "Epoch [100/150], Train Loss: 24.07403600724017, Test Loss: 28.34366751336432\n",
      "Epoch [110/150], Train Loss: 25.736823641667602, Test Loss: 28.868907358739282\n",
      "Epoch [120/150], Train Loss: 24.985075878706134, Test Loss: 28.839273427988026\n",
      "Epoch [130/150], Train Loss: 24.916236633550927, Test Loss: 27.5912805037065\n",
      "Epoch [140/150], Train Loss: 23.898373050377018, Test Loss: 29.334400870583274\n",
      "Epoch [150/150], Train Loss: 25.28538399368036, Test Loss: 27.337858447780857\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.66601990871742, Test Loss: 30.591184442693535\n",
      "Epoch [20/150], Train Loss: 27.240737170860417, Test Loss: 30.90413373476499\n",
      "Epoch [30/150], Train Loss: 25.49042191427262, Test Loss: 27.379519673136922\n",
      "Epoch [40/150], Train Loss: 25.2010033841993, Test Loss: 27.493713800009196\n",
      "Epoch [50/150], Train Loss: 25.8835428144111, Test Loss: 29.358880699454964\n",
      "Epoch [60/150], Train Loss: 25.638619707451493, Test Loss: 28.619495391845703\n",
      "Epoch [70/150], Train Loss: 26.581567226472448, Test Loss: 28.381914758062983\n",
      "Epoch [80/150], Train Loss: 25.916764218689966, Test Loss: 27.966556177510842\n",
      "Epoch [90/150], Train Loss: 25.08447132423276, Test Loss: 27.950415128237243\n",
      "Epoch [100/150], Train Loss: 24.30432445338515, Test Loss: 31.168894507668234\n",
      "Epoch [110/150], Train Loss: 26.409964489545978, Test Loss: 27.917247722675274\n",
      "Epoch [120/150], Train Loss: 25.05681962810579, Test Loss: 29.987609120158407\n",
      "Epoch [130/150], Train Loss: 24.939942954016512, Test Loss: 28.356376425012364\n",
      "Epoch [140/150], Train Loss: 24.523531554175204, Test Loss: 28.360049805083833\n",
      "Epoch [150/150], Train Loss: 25.581505640999215, Test Loss: 27.073357916497564\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.693264751747005, Test Loss: 31.04060091291155\n",
      "Epoch [20/150], Train Loss: 26.29628562927246, Test Loss: 29.545115953916078\n",
      "Epoch [30/150], Train Loss: 27.162025301573706, Test Loss: 28.41153181992568\n",
      "Epoch [40/150], Train Loss: 27.13995060529865, Test Loss: 28.220850634884524\n",
      "Epoch [50/150], Train Loss: 26.179221969354348, Test Loss: 28.064437990064746\n",
      "Epoch [60/150], Train Loss: 26.486526889488346, Test Loss: 27.91987639588195\n",
      "Epoch [70/150], Train Loss: 26.002641490248383, Test Loss: 27.80523007875913\n",
      "Epoch [80/150], Train Loss: 24.407313384384405, Test Loss: 27.647046497889928\n",
      "Epoch [90/150], Train Loss: 24.828607302806418, Test Loss: 28.09646120938388\n",
      "Epoch [100/150], Train Loss: 24.745566452526656, Test Loss: 32.59158062625241\n",
      "Epoch [110/150], Train Loss: 24.128376388549803, Test Loss: 28.298298129787693\n",
      "Epoch [120/150], Train Loss: 24.870401394953493, Test Loss: 28.269492657153638\n",
      "Epoch [130/150], Train Loss: 25.42018987937052, Test Loss: 27.705529918918362\n",
      "Epoch [140/150], Train Loss: 26.14212547052102, Test Loss: 28.855592455182755\n",
      "Epoch [150/150], Train Loss: 23.040445008825085, Test Loss: 29.762716664896384\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.659528913654263, Test Loss: 29.301280975341797\n",
      "Epoch [20/150], Train Loss: 27.869762833392034, Test Loss: 30.41783097502473\n",
      "Epoch [30/150], Train Loss: 26.310790465308017, Test Loss: 28.62017418502213\n",
      "Epoch [40/150], Train Loss: 26.878787544125416, Test Loss: 28.63003770407144\n",
      "Epoch [50/150], Train Loss: 26.456706763095543, Test Loss: 31.912507614532075\n",
      "Epoch [60/150], Train Loss: 26.472593482595975, Test Loss: 28.17959857296634\n",
      "Epoch [70/150], Train Loss: 25.50862261662718, Test Loss: 28.266638322310015\n",
      "Epoch [80/150], Train Loss: 27.119163444393973, Test Loss: 27.640329856377143\n",
      "Epoch [90/150], Train Loss: 26.09422058355613, Test Loss: 28.570455997021167\n",
      "Epoch [100/150], Train Loss: 24.800042061727556, Test Loss: 28.62219679200804\n",
      "Epoch [110/150], Train Loss: 26.400001976138256, Test Loss: 27.255492049378233\n",
      "Epoch [120/150], Train Loss: 25.275545013928024, Test Loss: 34.27804379648977\n",
      "Epoch [130/150], Train Loss: 25.049549828201044, Test Loss: 33.918229239327566\n",
      "Epoch [140/150], Train Loss: 26.060667844678534, Test Loss: 28.606429112422003\n",
      "Epoch [150/150], Train Loss: 24.84731110744789, Test Loss: 28.038568348079533\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.67212236123007, Test Loss: 38.60040867793096\n",
      "Epoch [20/150], Train Loss: 27.67999870425365, Test Loss: 29.4109413840554\n",
      "Epoch [30/150], Train Loss: 27.050637967469264, Test Loss: 30.089603300218457\n",
      "Epoch [40/150], Train Loss: 26.746558048686044, Test Loss: 29.788678503655767\n",
      "Epoch [50/150], Train Loss: 26.929136607686026, Test Loss: 27.588082969962777\n",
      "Epoch [60/150], Train Loss: 26.929610955910604, Test Loss: 31.919469189334226\n",
      "Epoch [70/150], Train Loss: 26.72264377406386, Test Loss: 28.183284561355393\n",
      "Epoch [80/150], Train Loss: 25.38916248258997, Test Loss: 29.470165252685547\n",
      "Epoch [90/150], Train Loss: 25.79501558522709, Test Loss: 28.607652862350662\n",
      "Epoch [100/150], Train Loss: 25.4148723789903, Test Loss: 27.993722296380376\n",
      "Epoch [110/150], Train Loss: 26.11272060206679, Test Loss: 27.545356007365438\n",
      "Epoch [120/150], Train Loss: 26.55744486949483, Test Loss: 31.644841107455168\n",
      "Epoch [130/150], Train Loss: 25.969773089299437, Test Loss: 27.919963762357636\n",
      "Epoch [140/150], Train Loss: 26.177866588655064, Test Loss: 27.70272500174386\n",
      "Epoch [150/150], Train Loss: 25.925261356791513, Test Loss: 28.29998306175331\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.480873970907243, Test Loss: 32.82959910801479\n",
      "Epoch [20/150], Train Loss: 26.584586721951844, Test Loss: 29.879386332127954\n",
      "Epoch [30/150], Train Loss: 26.93778126826052, Test Loss: 29.89076921537325\n",
      "Epoch [40/150], Train Loss: 26.215221092349193, Test Loss: 28.51643193232549\n",
      "Epoch [50/150], Train Loss: 25.96321710680352, Test Loss: 28.493594949895684\n",
      "Epoch [60/150], Train Loss: 26.154790972099928, Test Loss: 28.156552327143682\n",
      "Epoch [70/150], Train Loss: 27.35875660630523, Test Loss: 27.874490366353616\n",
      "Epoch [80/150], Train Loss: 27.78039668348969, Test Loss: 28.576527830842252\n",
      "Epoch [90/150], Train Loss: 25.534257982597975, Test Loss: 29.826208312790115\n",
      "Epoch [100/150], Train Loss: 26.349599050302974, Test Loss: 28.28012350008085\n",
      "Epoch [110/150], Train Loss: 25.30368709876889, Test Loss: 34.55351871639103\n",
      "Epoch [120/150], Train Loss: 25.978465821313076, Test Loss: 29.92989002574574\n",
      "Epoch [130/150], Train Loss: 25.320238426083424, Test Loss: 27.603728207674894\n",
      "Epoch [140/150], Train Loss: 25.82474902418793, Test Loss: 27.85128063350529\n",
      "Epoch [150/150], Train Loss: 25.044449434124054, Test Loss: 28.77911480990323\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.26810342757428, Test Loss: 32.28946472762467\n",
      "Epoch [20/150], Train Loss: 27.792418070308496, Test Loss: 30.134813680277244\n",
      "Epoch [30/150], Train Loss: 27.33786454747935, Test Loss: 31.584652318582908\n",
      "Epoch [40/150], Train Loss: 27.510208411294904, Test Loss: 35.520078460891526\n",
      "Epoch [50/150], Train Loss: 26.47164931375472, Test Loss: 29.033889820049335\n",
      "Epoch [60/150], Train Loss: 26.14969359225914, Test Loss: 42.97794723510742\n",
      "Epoch [70/150], Train Loss: 26.785402122872775, Test Loss: 29.55156031521884\n",
      "Epoch [80/150], Train Loss: 25.939588227819225, Test Loss: 28.973281711726994\n",
      "Epoch [90/150], Train Loss: 26.334897619778992, Test Loss: 28.46384333325671\n",
      "Epoch [100/150], Train Loss: 26.864815546254643, Test Loss: 29.66762520430924\n",
      "Epoch [110/150], Train Loss: 26.70384331374872, Test Loss: 28.794506370247184\n",
      "Epoch [120/150], Train Loss: 25.664514322749906, Test Loss: 28.475710311493316\n",
      "Epoch [130/150], Train Loss: 26.710209968441823, Test Loss: 28.329807875992415\n",
      "Epoch [140/150], Train Loss: 26.270109032802896, Test Loss: 29.720326906674867\n",
      "Epoch [150/150], Train Loss: 26.210455209700786, Test Loss: 29.714250044389203\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.70425495710529, Test Loss: 35.740007772074115\n",
      "Epoch [20/150], Train Loss: 29.23186049539535, Test Loss: 31.111961488599903\n",
      "Epoch [30/150], Train Loss: 27.94146963025703, Test Loss: 29.49515840604708\n",
      "Epoch [40/150], Train Loss: 27.107763109050815, Test Loss: 30.077163448581448\n",
      "Epoch [50/150], Train Loss: 27.801677841436668, Test Loss: 31.032837335165446\n",
      "Epoch [60/150], Train Loss: 26.900498186955687, Test Loss: 28.901987818928507\n",
      "Epoch [70/150], Train Loss: 25.84511791291784, Test Loss: 28.89143084241198\n",
      "Epoch [80/150], Train Loss: 26.316139314995436, Test Loss: 28.339274245423155\n",
      "Epoch [90/150], Train Loss: 26.60362379980869, Test Loss: 31.705353328159877\n",
      "Epoch [100/150], Train Loss: 27.021461749467694, Test Loss: 29.25238918948483\n",
      "Epoch [110/150], Train Loss: 26.14629256451716, Test Loss: 28.71823313329127\n",
      "Epoch [120/150], Train Loss: 27.74859293953317, Test Loss: 28.64150569965313\n",
      "Epoch [130/150], Train Loss: 27.49254671941038, Test Loss: 28.760467851316776\n",
      "Epoch [140/150], Train Loss: 27.564075507492316, Test Loss: 28.06453246574897\n",
      "Epoch [150/150], Train Loss: 26.54057065619797, Test Loss: 29.009996389413807\n",
      "Training model with hidden size 128, batch size 32, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.054425592891505, Test Loss: 34.354240467021995\n",
      "Epoch [20/150], Train Loss: 27.9468545006924, Test Loss: 34.83620814533977\n",
      "Epoch [30/150], Train Loss: 27.16152548868148, Test Loss: 35.48423152774959\n",
      "Epoch [40/150], Train Loss: 27.327865600585938, Test Loss: 31.686241001277775\n",
      "Epoch [50/150], Train Loss: 27.373575079245647, Test Loss: 29.825427934720917\n",
      "Epoch [60/150], Train Loss: 28.412438220665102, Test Loss: 30.4776912788292\n",
      "Epoch [70/150], Train Loss: 26.207771576428023, Test Loss: 30.83601035080947\n",
      "Epoch [80/150], Train Loss: 27.5843795276079, Test Loss: 32.94723993772036\n",
      "Epoch [90/150], Train Loss: 26.27616415805504, Test Loss: 33.477528832175516\n",
      "Epoch [100/150], Train Loss: 26.45014379532611, Test Loss: 28.02357495295537\n",
      "Epoch [110/150], Train Loss: 26.034255650004404, Test Loss: 30.30696053938432\n",
      "Epoch [120/150], Train Loss: 27.263942005595222, Test Loss: 30.458800848428304\n",
      "Epoch [130/150], Train Loss: 26.267425843535875, Test Loss: 30.3296152337805\n",
      "Epoch [140/150], Train Loss: 25.663441917544507, Test Loss: 29.33302790158755\n",
      "Epoch [150/150], Train Loss: 26.29285460925493, Test Loss: 28.589123044695175\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.550612602859246, Test Loss: 45.95827969637784\n",
      "Epoch [20/150], Train Loss: 42.30623664230597, Test Loss: 40.64697265625\n",
      "Epoch [30/150], Train Loss: 38.144824493908494, Test Loss: 39.247125204507405\n",
      "Epoch [40/150], Train Loss: 35.08130510048788, Test Loss: 35.7804631319913\n",
      "Epoch [50/150], Train Loss: 31.697506851446434, Test Loss: 32.8975261836857\n",
      "Epoch [60/150], Train Loss: 29.548336585623318, Test Loss: 29.959233321152723\n",
      "Epoch [70/150], Train Loss: 27.665581875160093, Test Loss: 29.88629695347377\n",
      "Epoch [80/150], Train Loss: 25.677992160984726, Test Loss: 28.711008790251498\n",
      "Epoch [90/150], Train Loss: 25.93702265630003, Test Loss: 28.09969490843934\n",
      "Epoch [100/150], Train Loss: 24.671066402998125, Test Loss: 28.392054991288617\n",
      "Epoch [110/150], Train Loss: 24.88285996483975, Test Loss: 28.42191099191641\n",
      "Epoch [120/150], Train Loss: 23.305812091514714, Test Loss: 28.67228716070002\n",
      "Epoch [130/150], Train Loss: 23.69372519821417, Test Loss: 28.703218261916916\n",
      "Epoch [140/150], Train Loss: 22.810164398443504, Test Loss: 28.908017988328808\n",
      "Epoch [150/150], Train Loss: 22.42396668606117, Test Loss: 27.824861997133727\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.66132305958232, Test Loss: 45.77959793883485\n",
      "Epoch [20/150], Train Loss: 42.332951142358, Test Loss: 40.89294106619699\n",
      "Epoch [30/150], Train Loss: 37.90973932234967, Test Loss: 37.99553606107637\n",
      "Epoch [40/150], Train Loss: 34.55516143548684, Test Loss: 36.05782283436168\n",
      "Epoch [50/150], Train Loss: 32.169099551341574, Test Loss: 34.82599501176314\n",
      "Epoch [60/150], Train Loss: 29.892845410206277, Test Loss: 30.27483454617587\n",
      "Epoch [70/150], Train Loss: 27.127742917420434, Test Loss: 30.25331583890048\n",
      "Epoch [80/150], Train Loss: 26.08356785383381, Test Loss: 28.43296920479118\n",
      "Epoch [90/150], Train Loss: 25.951313462804574, Test Loss: 29.644146683928255\n",
      "Epoch [100/150], Train Loss: 24.261532674070263, Test Loss: 29.116707021539863\n",
      "Epoch [110/150], Train Loss: 23.8406129430552, Test Loss: 29.090103025560254\n",
      "Epoch [120/150], Train Loss: 23.023422503862225, Test Loss: 29.042403976638596\n",
      "Epoch [130/150], Train Loss: 23.083168311197248, Test Loss: 28.60006936804041\n",
      "Epoch [140/150], Train Loss: 22.3458034953133, Test Loss: 28.732746421516715\n",
      "Epoch [150/150], Train Loss: 21.487147309350185, Test Loss: 29.750822092031505\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.87045970979284, Test Loss: 43.9615424515365\n",
      "Epoch [20/150], Train Loss: 42.65282318865667, Test Loss: 41.382672891988385\n",
      "Epoch [30/150], Train Loss: 38.51402384023197, Test Loss: 39.55103505122197\n",
      "Epoch [40/150], Train Loss: 35.633772790627404, Test Loss: 35.0012703932725\n",
      "Epoch [50/150], Train Loss: 32.54353064240002, Test Loss: 36.84850038800921\n",
      "Epoch [60/150], Train Loss: 29.85003213100746, Test Loss: 31.470847934871525\n",
      "Epoch [70/150], Train Loss: 27.30525453286093, Test Loss: 29.865458154059077\n",
      "Epoch [80/150], Train Loss: 26.848457111296106, Test Loss: 30.111459855909473\n",
      "Epoch [90/150], Train Loss: 26.203440225319785, Test Loss: 28.697591211888696\n",
      "Epoch [100/150], Train Loss: 23.60684443614522, Test Loss: 29.077398052463284\n",
      "Epoch [110/150], Train Loss: 23.34850068014176, Test Loss: 28.28233074832272\n",
      "Epoch [120/150], Train Loss: 24.488883865856735, Test Loss: 28.77172244678844\n",
      "Epoch [130/150], Train Loss: 22.97458650557721, Test Loss: 28.17175278106293\n",
      "Epoch [140/150], Train Loss: 23.002474906796316, Test Loss: 29.03315204768986\n",
      "Epoch [150/150], Train Loss: 23.13128472625232, Test Loss: 29.227326479825106\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.20174316656394, Test Loss: 47.069247456340044\n",
      "Epoch [20/150], Train Loss: 41.66919828321113, Test Loss: 40.92707804890422\n",
      "Epoch [30/150], Train Loss: 37.69618585305136, Test Loss: 39.23624628240412\n",
      "Epoch [40/150], Train Loss: 34.789800550116865, Test Loss: 33.73424326909053\n",
      "Epoch [50/150], Train Loss: 31.50530680672067, Test Loss: 33.301015482320416\n",
      "Epoch [60/150], Train Loss: 28.79397608647581, Test Loss: 30.869789792345717\n",
      "Epoch [70/150], Train Loss: 27.074305249823897, Test Loss: 29.236590744613054\n",
      "Epoch [80/150], Train Loss: 25.748697956272814, Test Loss: 28.30915877106902\n",
      "Epoch [90/150], Train Loss: 24.901350878105788, Test Loss: 28.863323385065254\n",
      "Epoch [100/150], Train Loss: 25.336724678414768, Test Loss: 29.052706904225534\n",
      "Epoch [110/150], Train Loss: 23.587426820348522, Test Loss: 28.121911482377485\n",
      "Epoch [120/150], Train Loss: 22.88852669137423, Test Loss: 28.713749897944464\n",
      "Epoch [130/150], Train Loss: 22.996446753329916, Test Loss: 28.45734804326838\n",
      "Epoch [140/150], Train Loss: 22.967005732802093, Test Loss: 28.492605556141246\n",
      "Epoch [150/150], Train Loss: 21.445137724329214, Test Loss: 29.355246110395953\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.55685089611616, Test Loss: 46.59939704003272\n",
      "Epoch [20/150], Train Loss: 41.96929214977827, Test Loss: 40.149683122511036\n",
      "Epoch [30/150], Train Loss: 37.83221439299036, Test Loss: 38.248097853227094\n",
      "Epoch [40/150], Train Loss: 34.31013772683065, Test Loss: 34.43632812004585\n",
      "Epoch [50/150], Train Loss: 31.584510221637665, Test Loss: 32.47067860194615\n",
      "Epoch [60/150], Train Loss: 28.88124506590796, Test Loss: 29.922732489449636\n",
      "Epoch [70/150], Train Loss: 27.10259350010606, Test Loss: 29.40994775450075\n",
      "Epoch [80/150], Train Loss: 26.572924817194703, Test Loss: 28.005990288474344\n",
      "Epoch [90/150], Train Loss: 24.623611462702517, Test Loss: 28.294020566073332\n",
      "Epoch [100/150], Train Loss: 24.829577924384445, Test Loss: 27.924006424941027\n",
      "Epoch [110/150], Train Loss: 23.273976222804336, Test Loss: 29.2055379941866\n",
      "Epoch [120/150], Train Loss: 22.694882408517305, Test Loss: 28.443695910565264\n",
      "Epoch [130/150], Train Loss: 23.29212612714924, Test Loss: 27.546658875106218\n",
      "Epoch [140/150], Train Loss: 22.549440027455816, Test Loss: 28.57472563409186\n",
      "Epoch [150/150], Train Loss: 20.96275155739706, Test Loss: 27.776252251166802\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.07250553819, Test Loss: 46.961266257546164\n",
      "Epoch [20/150], Train Loss: 42.823344521444355, Test Loss: 39.023220359505\n",
      "Epoch [30/150], Train Loss: 38.72311064923396, Test Loss: 39.58633432759867\n",
      "Epoch [40/150], Train Loss: 36.28851437177814, Test Loss: 37.79779305396142\n",
      "Epoch [50/150], Train Loss: 32.83640762078957, Test Loss: 34.72944071385768\n",
      "Epoch [60/150], Train Loss: 29.767156400836882, Test Loss: 31.452065529761377\n",
      "Epoch [70/150], Train Loss: 27.98831841515713, Test Loss: 30.067098369846097\n",
      "Epoch [80/150], Train Loss: 26.153295910944703, Test Loss: 30.412095429061296\n",
      "Epoch [90/150], Train Loss: 24.981315562764152, Test Loss: 28.484592536827186\n",
      "Epoch [100/150], Train Loss: 24.526407742109456, Test Loss: 28.657339170381622\n",
      "Epoch [110/150], Train Loss: 23.652353768270522, Test Loss: 28.479159318007433\n",
      "Epoch [120/150], Train Loss: 22.579463452198468, Test Loss: 28.464087052778765\n",
      "Epoch [130/150], Train Loss: 22.35580847067911, Test Loss: 28.761683154415774\n",
      "Epoch [140/150], Train Loss: 22.620924527527855, Test Loss: 30.061386727667475\n",
      "Epoch [150/150], Train Loss: 21.829677087752547, Test Loss: 28.319280401452794\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.48489782614786, Test Loss: 47.183489217386615\n",
      "Epoch [20/150], Train Loss: 41.92343923850137, Test Loss: 37.80255543101918\n",
      "Epoch [30/150], Train Loss: 37.740493874471696, Test Loss: 37.34426964103402\n",
      "Epoch [40/150], Train Loss: 34.53475106661437, Test Loss: 33.998945657309\n",
      "Epoch [50/150], Train Loss: 31.906491914342663, Test Loss: 34.106213061840506\n",
      "Epoch [60/150], Train Loss: 29.24918511187444, Test Loss: 30.380176866209354\n",
      "Epoch [70/150], Train Loss: 27.398669170942462, Test Loss: 29.28701589014623\n",
      "Epoch [80/150], Train Loss: 25.93712389586402, Test Loss: 28.38521097852038\n",
      "Epoch [90/150], Train Loss: 25.34961552229084, Test Loss: 28.061684645615614\n",
      "Epoch [100/150], Train Loss: 25.052756356411294, Test Loss: 28.191292378809546\n",
      "Epoch [110/150], Train Loss: 23.733050981115124, Test Loss: 28.608348945518593\n",
      "Epoch [120/150], Train Loss: 24.02583212305288, Test Loss: 28.684731297678763\n",
      "Epoch [130/150], Train Loss: 23.10975232984199, Test Loss: 27.998547591172255\n",
      "Epoch [140/150], Train Loss: 24.076708302732374, Test Loss: 28.753577665849164\n",
      "Epoch [150/150], Train Loss: 24.563760588599035, Test Loss: 29.04900969468154\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 45.999137653288294, Test Loss: 47.04799429162756\n",
      "Epoch [20/150], Train Loss: 41.38552461217662, Test Loss: 41.14745717234426\n",
      "Epoch [30/150], Train Loss: 37.41472792078237, Test Loss: 38.85742975210215\n",
      "Epoch [40/150], Train Loss: 34.48250210871462, Test Loss: 36.32748901069938\n",
      "Epoch [50/150], Train Loss: 31.124348274606174, Test Loss: 34.775471030891715\n",
      "Epoch [60/150], Train Loss: 29.110203964983832, Test Loss: 32.63378737808822\n",
      "Epoch [70/150], Train Loss: 27.005812635578092, Test Loss: 29.928632612352246\n",
      "Epoch [80/150], Train Loss: 27.37725597444128, Test Loss: 27.723467195188846\n",
      "Epoch [90/150], Train Loss: 25.47379180407915, Test Loss: 28.984022388210544\n",
      "Epoch [100/150], Train Loss: 24.66836181390481, Test Loss: 28.56465991131671\n",
      "Epoch [110/150], Train Loss: 24.176005329069543, Test Loss: 28.8029058134401\n",
      "Epoch [120/150], Train Loss: 23.761691571845382, Test Loss: 27.871557991226\n",
      "Epoch [130/150], Train Loss: 23.552623260998335, Test Loss: 27.805495299302137\n",
      "Epoch [140/150], Train Loss: 22.659541095671106, Test Loss: 27.752576406899983\n",
      "Epoch [150/150], Train Loss: 22.520781244997117, Test Loss: 27.65879794529506\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 46.96222821845383, Test Loss: 45.8538297677969\n",
      "Epoch [20/150], Train Loss: 42.74112258660989, Test Loss: 41.34663708179028\n",
      "Epoch [30/150], Train Loss: 38.56409864581999, Test Loss: 38.166323723731104\n",
      "Epoch [40/150], Train Loss: 35.44456478181433, Test Loss: 36.44418446429364\n",
      "Epoch [50/150], Train Loss: 32.14978537637679, Test Loss: 34.94830991076184\n",
      "Epoch [60/150], Train Loss: 29.69579026269131, Test Loss: 32.43180772855684\n",
      "Epoch [70/150], Train Loss: 28.371530013787943, Test Loss: 30.258994560737115\n",
      "Epoch [80/150], Train Loss: 26.679537282224562, Test Loss: 28.573418233301734\n",
      "Epoch [90/150], Train Loss: 26.09109233168305, Test Loss: 28.382111239742922\n",
      "Epoch [100/150], Train Loss: 26.24121277605901, Test Loss: 29.49070918095576\n",
      "Epoch [110/150], Train Loss: 25.018152587140193, Test Loss: 29.264078090717266\n",
      "Epoch [120/150], Train Loss: 23.45891526018987, Test Loss: 27.844592973783417\n",
      "Epoch [130/150], Train Loss: 23.335971488327278, Test Loss: 27.346790363262226\n",
      "Epoch [140/150], Train Loss: 22.839878457491515, Test Loss: 28.142741562484147\n",
      "Epoch [150/150], Train Loss: 22.245363685732983, Test Loss: 28.092883444451665\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.193605135307937, Test Loss: 33.69441309842196\n",
      "Epoch [20/150], Train Loss: 26.23772406030874, Test Loss: 28.205251644184063\n",
      "Epoch [30/150], Train Loss: 25.15723735621718, Test Loss: 29.72758469024262\n",
      "Epoch [40/150], Train Loss: 25.32806022518971, Test Loss: 28.774541433755452\n",
      "Epoch [50/150], Train Loss: 25.101621046222625, Test Loss: 28.45456918493494\n",
      "Epoch [60/150], Train Loss: 23.592302803915054, Test Loss: 27.851620116791167\n",
      "Epoch [70/150], Train Loss: 22.772289501252722, Test Loss: 30.346555140111352\n",
      "Epoch [80/150], Train Loss: 22.5791000616355, Test Loss: 28.726614766306692\n",
      "Epoch [90/150], Train Loss: 23.663622246413933, Test Loss: 27.92665330465738\n",
      "Epoch [100/150], Train Loss: 22.968810416049646, Test Loss: 29.033291853867567\n",
      "Epoch [110/150], Train Loss: 23.33384039206583, Test Loss: 27.644451859709505\n",
      "Epoch [120/150], Train Loss: 21.608657299104284, Test Loss: 28.645594807414266\n",
      "Epoch [130/150], Train Loss: 21.59676568703573, Test Loss: 28.278240724043414\n",
      "Epoch [140/150], Train Loss: 21.262631031724272, Test Loss: 28.637408888185178\n",
      "Epoch [150/150], Train Loss: 21.197242793098823, Test Loss: 28.569846215186182\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.734443727086802, Test Loss: 32.079531483835986\n",
      "Epoch [20/150], Train Loss: 25.82490921645868, Test Loss: 30.36620504205877\n",
      "Epoch [30/150], Train Loss: 25.642851088476963, Test Loss: 27.740092289912237\n",
      "Epoch [40/150], Train Loss: 25.34174605822954, Test Loss: 28.073370128482967\n",
      "Epoch [50/150], Train Loss: 24.906840415079085, Test Loss: 30.536644353495017\n",
      "Epoch [60/150], Train Loss: 24.012758229990474, Test Loss: 28.980881232719916\n",
      "Epoch [70/150], Train Loss: 23.47118465861336, Test Loss: 29.373005359203784\n",
      "Epoch [80/150], Train Loss: 22.86165144248087, Test Loss: 28.39975158889572\n",
      "Epoch [90/150], Train Loss: 23.10495708027824, Test Loss: 27.28163360001205\n",
      "Epoch [100/150], Train Loss: 22.038934413722306, Test Loss: 27.619556476543476\n",
      "Epoch [110/150], Train Loss: 22.15868791048644, Test Loss: 28.43691352745155\n",
      "Epoch [120/150], Train Loss: 22.098417894957496, Test Loss: 26.987839364386225\n",
      "Epoch [130/150], Train Loss: 22.326788154977265, Test Loss: 28.339493689598974\n",
      "Epoch [140/150], Train Loss: 21.38634918713179, Test Loss: 28.18145325896028\n",
      "Epoch [150/150], Train Loss: 20.82787881444712, Test Loss: 29.121856392203988\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.150370863617443, Test Loss: 30.33430220864036\n",
      "Epoch [20/150], Train Loss: 25.38405090081887, Test Loss: 30.686982935125176\n",
      "Epoch [30/150], Train Loss: 24.791824509667567, Test Loss: 30.018722757116542\n",
      "Epoch [40/150], Train Loss: 24.35788901907499, Test Loss: 27.593815493893313\n",
      "Epoch [50/150], Train Loss: 24.34067657345631, Test Loss: 29.787295675896978\n",
      "Epoch [60/150], Train Loss: 24.74578285217285, Test Loss: 28.394446509225027\n",
      "Epoch [70/150], Train Loss: 23.042404099761463, Test Loss: 29.513388918591783\n",
      "Epoch [80/150], Train Loss: 23.564287767253937, Test Loss: 28.467847576389065\n",
      "Epoch [90/150], Train Loss: 22.64900309453245, Test Loss: 30.73631388181216\n",
      "Epoch [100/150], Train Loss: 24.218235359817253, Test Loss: 27.981497603577452\n",
      "Epoch [110/150], Train Loss: 22.90034502373367, Test Loss: 28.72142638169326\n",
      "Epoch [120/150], Train Loss: 21.036406926639746, Test Loss: 30.21654983619591\n",
      "Epoch [130/150], Train Loss: 21.83031417971752, Test Loss: 29.979185946576006\n",
      "Epoch [140/150], Train Loss: 22.17249223052478, Test Loss: 29.165759346701883\n",
      "Epoch [150/150], Train Loss: 23.268829164348666, Test Loss: 30.639911106654576\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.271188610889872, Test Loss: 32.54391719768574\n",
      "Epoch [20/150], Train Loss: 25.7778130453141, Test Loss: 30.047760529951617\n",
      "Epoch [30/150], Train Loss: 26.119360401591315, Test Loss: 27.82124521825221\n",
      "Epoch [40/150], Train Loss: 24.336972358578542, Test Loss: 28.84798708829013\n",
      "Epoch [50/150], Train Loss: 25.375060140891154, Test Loss: 27.68810661117752\n",
      "Epoch [60/150], Train Loss: 24.357859714695664, Test Loss: 28.415794719349254\n",
      "Epoch [70/150], Train Loss: 23.932640550957352, Test Loss: 29.521523488032354\n",
      "Epoch [80/150], Train Loss: 24.597678412765752, Test Loss: 28.383913585117885\n",
      "Epoch [90/150], Train Loss: 24.566427415316223, Test Loss: 28.530125556054053\n",
      "Epoch [100/150], Train Loss: 23.347786137315094, Test Loss: 28.252347351668718\n",
      "Epoch [110/150], Train Loss: 24.296757013289653, Test Loss: 29.130966508543338\n",
      "Epoch [120/150], Train Loss: 23.642609342981558, Test Loss: 29.11526023567497\n",
      "Epoch [130/150], Train Loss: 23.56465850579934, Test Loss: 28.912279649214312\n",
      "Epoch [140/150], Train Loss: 22.513025408885518, Test Loss: 28.32760919843401\n",
      "Epoch [150/150], Train Loss: 21.87907702961906, Test Loss: 27.976255193933262\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.3643211052066, Test Loss: 31.75421734599324\n",
      "Epoch [20/150], Train Loss: 26.524062334904905, Test Loss: 30.690114950204823\n",
      "Epoch [30/150], Train Loss: 25.3475203091981, Test Loss: 29.94918897554472\n",
      "Epoch [40/150], Train Loss: 24.394571141727635, Test Loss: 29.97294029632172\n",
      "Epoch [50/150], Train Loss: 24.2766242855885, Test Loss: 28.11389511901063\n",
      "Epoch [60/150], Train Loss: 24.0737657328121, Test Loss: 29.63202070260977\n",
      "Epoch [70/150], Train Loss: 23.065037724229157, Test Loss: 31.690149728353923\n",
      "Epoch [80/150], Train Loss: 23.70306104441158, Test Loss: 31.236485790896726\n",
      "Epoch [90/150], Train Loss: 24.056197419713754, Test Loss: 28.734824960882012\n",
      "Epoch [100/150], Train Loss: 24.329226609527087, Test Loss: 29.622508185250418\n",
      "Epoch [110/150], Train Loss: 21.76532075600546, Test Loss: 29.28255031635235\n",
      "Epoch [120/150], Train Loss: 21.90910298081695, Test Loss: 28.43716879014845\n",
      "Epoch [130/150], Train Loss: 22.617646595689116, Test Loss: 28.725246528526405\n",
      "Epoch [140/150], Train Loss: 22.894969777591893, Test Loss: 28.80433545793806\n",
      "Epoch [150/150], Train Loss: 21.470516630078926, Test Loss: 28.844231097729175\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.66219350470871, Test Loss: 31.328968518740172\n",
      "Epoch [20/150], Train Loss: 24.93261007090084, Test Loss: 30.513307942972556\n",
      "Epoch [30/150], Train Loss: 25.4928584364594, Test Loss: 29.576078266292424\n",
      "Epoch [40/150], Train Loss: 25.270262871413934, Test Loss: 28.5300367528742\n",
      "Epoch [50/150], Train Loss: 25.508514185420804, Test Loss: 28.772299333052203\n",
      "Epoch [60/150], Train Loss: 24.746877426397607, Test Loss: 28.605758468826096\n",
      "Epoch [70/150], Train Loss: 24.008454144587283, Test Loss: 29.401126217532468\n",
      "Epoch [80/150], Train Loss: 22.84411916263768, Test Loss: 29.097654937149642\n",
      "Epoch [90/150], Train Loss: 23.49324180102739, Test Loss: 28.655492683509728\n",
      "Epoch [100/150], Train Loss: 23.978739041187723, Test Loss: 27.113300769360034\n",
      "Epoch [110/150], Train Loss: 23.168526352428998, Test Loss: 27.6295994597596\n",
      "Epoch [120/150], Train Loss: 23.67266447974033, Test Loss: 28.745348744578177\n",
      "Epoch [130/150], Train Loss: 22.29984878790183, Test Loss: 28.240871231277268\n",
      "Epoch [140/150], Train Loss: 23.106401631089508, Test Loss: 28.184510342486494\n",
      "Epoch [150/150], Train Loss: 22.619425057583168, Test Loss: 29.489850849300236\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.08646492879899, Test Loss: 36.536310468401226\n",
      "Epoch [20/150], Train Loss: 26.100653563952836, Test Loss: 38.219621980345096\n",
      "Epoch [30/150], Train Loss: 25.483976676815846, Test Loss: 29.508721339238154\n",
      "Epoch [40/150], Train Loss: 26.25541202357558, Test Loss: 29.43064583121956\n",
      "Epoch [50/150], Train Loss: 25.598351581761094, Test Loss: 28.828285068660588\n",
      "Epoch [60/150], Train Loss: 26.534299644095, Test Loss: 28.696829956847353\n",
      "Epoch [70/150], Train Loss: 26.31273877503442, Test Loss: 27.830918027208998\n",
      "Epoch [80/150], Train Loss: 24.90142551484655, Test Loss: 29.821984873189553\n",
      "Epoch [90/150], Train Loss: 24.71020703550245, Test Loss: 29.410058281638406\n",
      "Epoch [100/150], Train Loss: 25.112323573378266, Test Loss: 29.321681877235314\n",
      "Epoch [110/150], Train Loss: 24.178097490404472, Test Loss: 27.554792713809324\n",
      "Epoch [120/150], Train Loss: 24.72459788713299, Test Loss: 28.127643882454215\n",
      "Epoch [130/150], Train Loss: 23.72957178960081, Test Loss: 30.008255203048904\n",
      "Epoch [140/150], Train Loss: 23.872082450741626, Test Loss: 28.561366291789266\n",
      "Epoch [150/150], Train Loss: 24.17622004649678, Test Loss: 27.291669548331917\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.270658436759575, Test Loss: 33.711255432723405\n",
      "Epoch [20/150], Train Loss: 25.68612253157819, Test Loss: 31.951753046605493\n",
      "Epoch [30/150], Train Loss: 26.07797181801718, Test Loss: 31.624216921917803\n",
      "Epoch [40/150], Train Loss: 25.712914970272877, Test Loss: 31.853378593147575\n",
      "Epoch [50/150], Train Loss: 24.482142188900806, Test Loss: 29.393706408413973\n",
      "Epoch [60/150], Train Loss: 25.798092795200034, Test Loss: 29.746083123343332\n",
      "Epoch [70/150], Train Loss: 25.02355948276207, Test Loss: 28.63181716126281\n",
      "Epoch [80/150], Train Loss: 25.355513788442142, Test Loss: 27.509846922639127\n",
      "Epoch [90/150], Train Loss: 25.12260428256676, Test Loss: 27.472550615087734\n",
      "Epoch [100/150], Train Loss: 25.304855678120596, Test Loss: 28.216152959055716\n",
      "Epoch [110/150], Train Loss: 25.026904471975858, Test Loss: 29.107571044525542\n",
      "Epoch [120/150], Train Loss: 23.726826245667503, Test Loss: 28.027942682241466\n",
      "Epoch [130/150], Train Loss: 24.36830928990098, Test Loss: 30.433751638833577\n",
      "Epoch [140/150], Train Loss: 24.377455833309988, Test Loss: 27.607028688703263\n",
      "Epoch [150/150], Train Loss: 24.319409173433897, Test Loss: 29.073411322259282\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.143076118094022, Test Loss: 36.23452887597022\n",
      "Epoch [20/150], Train Loss: 27.183594037665696, Test Loss: 31.527109071805878\n",
      "Epoch [30/150], Train Loss: 26.56347665005043, Test Loss: 30.38528046050629\n",
      "Epoch [40/150], Train Loss: 25.057380426125448, Test Loss: 29.04837960082215\n",
      "Epoch [50/150], Train Loss: 26.133465451099834, Test Loss: 28.88245867444323\n",
      "Epoch [60/150], Train Loss: 25.362930698082096, Test Loss: 29.000671411489513\n",
      "Epoch [70/150], Train Loss: 25.905154512749345, Test Loss: 27.818894770238305\n",
      "Epoch [80/150], Train Loss: 25.74136301884886, Test Loss: 28.292411705116173\n",
      "Epoch [90/150], Train Loss: 24.852716646038118, Test Loss: 28.608955804403728\n",
      "Epoch [100/150], Train Loss: 24.615337928396755, Test Loss: 27.308829889669045\n",
      "Epoch [110/150], Train Loss: 24.248327499139503, Test Loss: 28.802977623877588\n",
      "Epoch [120/150], Train Loss: 23.882449703529232, Test Loss: 29.7820949802151\n",
      "Epoch [130/150], Train Loss: 23.668821359853276, Test Loss: 28.378654925854175\n",
      "Epoch [140/150], Train Loss: 24.166887583498095, Test Loss: 27.368645209770698\n",
      "Epoch [150/150], Train Loss: 23.89153679394331, Test Loss: 26.725941608478497\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.204644200059235, Test Loss: 30.27424141029259\n",
      "Epoch [20/150], Train Loss: 25.712386021848584, Test Loss: 30.94489233834403\n",
      "Epoch [30/150], Train Loss: 25.354330137909436, Test Loss: 29.83120296527813\n",
      "Epoch [40/150], Train Loss: 24.811932829559826, Test Loss: 28.133425105701793\n",
      "Epoch [50/150], Train Loss: 25.4771852086802, Test Loss: 28.597094325276164\n",
      "Epoch [60/150], Train Loss: 25.229858073250192, Test Loss: 29.994338320447252\n",
      "Epoch [70/150], Train Loss: 25.29509912709721, Test Loss: 27.32067410357587\n",
      "Epoch [80/150], Train Loss: 24.432483860703766, Test Loss: 28.262753226540305\n",
      "Epoch [90/150], Train Loss: 24.68494710453221, Test Loss: 32.656546332619406\n",
      "Epoch [100/150], Train Loss: 24.97314018499656, Test Loss: 27.79226419523165\n",
      "Epoch [110/150], Train Loss: 23.55439146073138, Test Loss: 27.684978064004476\n",
      "Epoch [120/150], Train Loss: 24.158892784744012, Test Loss: 27.34603807523653\n",
      "Epoch [130/150], Train Loss: 24.762790260940303, Test Loss: 30.33501248545461\n",
      "Epoch [140/150], Train Loss: 23.134520777717967, Test Loss: 27.814031724805957\n",
      "Epoch [150/150], Train Loss: 24.122900021662478, Test Loss: 27.661218246856293\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.92916920771364, Test Loss: 30.435885887641412\n",
      "Epoch [20/150], Train Loss: 26.910405881287623, Test Loss: 28.950630757715796\n",
      "Epoch [30/150], Train Loss: 25.990340730010487, Test Loss: 28.43742038677265\n",
      "Epoch [40/150], Train Loss: 25.248703478203446, Test Loss: 27.896592870935216\n",
      "Epoch [50/150], Train Loss: 25.85317177381672, Test Loss: 29.04929032263818\n",
      "Epoch [60/150], Train Loss: 24.834763173587987, Test Loss: 28.19415236138678\n",
      "Epoch [70/150], Train Loss: 23.636246997020283, Test Loss: 27.20274660184786\n",
      "Epoch [80/150], Train Loss: 23.92478997902792, Test Loss: 29.159424967580026\n",
      "Epoch [90/150], Train Loss: 23.63356742858887, Test Loss: 28.879980979027685\n",
      "Epoch [100/150], Train Loss: 24.231178289945007, Test Loss: 28.320567044344816\n",
      "Epoch [110/150], Train Loss: 23.34628068267322, Test Loss: 28.58796590334409\n",
      "Epoch [120/150], Train Loss: 23.16717463008693, Test Loss: 28.104414407309\n",
      "Epoch [130/150], Train Loss: 22.834412190171538, Test Loss: 27.596801237626508\n",
      "Epoch [140/150], Train Loss: 24.575312173561972, Test Loss: 28.00689392585259\n",
      "Epoch [150/150], Train Loss: 22.881926395854013, Test Loss: 27.620092862612243\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.011553198392274, Test Loss: 38.12878640905603\n",
      "Epoch [20/150], Train Loss: 26.83053029795162, Test Loss: 29.69188690185547\n",
      "Epoch [30/150], Train Loss: 26.40884829036525, Test Loss: 33.01709985113763\n",
      "Epoch [40/150], Train Loss: 25.927692219468415, Test Loss: 28.43796811785017\n",
      "Epoch [50/150], Train Loss: 25.10014198803511, Test Loss: 28.978650130234755\n",
      "Epoch [60/150], Train Loss: 26.002741654192814, Test Loss: 27.97607186552766\n",
      "Epoch [70/150], Train Loss: 24.969288566464282, Test Loss: 29.777855290995017\n",
      "Epoch [80/150], Train Loss: 24.34712703892442, Test Loss: 27.721048305561016\n",
      "Epoch [90/150], Train Loss: 25.05481863803551, Test Loss: 28.197134166568905\n",
      "Epoch [100/150], Train Loss: 24.22870457758669, Test Loss: 28.441482717340644\n",
      "Epoch [110/150], Train Loss: 23.97933250177102, Test Loss: 29.682046023282137\n",
      "Epoch [120/150], Train Loss: 24.64763150449659, Test Loss: 28.613329850233995\n",
      "Epoch [130/150], Train Loss: 22.76105640598985, Test Loss: 30.592467394742098\n",
      "Epoch [140/150], Train Loss: 24.42709231767498, Test Loss: 29.035104454337777\n",
      "Epoch [150/150], Train Loss: 23.689938554607455, Test Loss: 29.554406401398893\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.125566476290345, Test Loss: 33.93249422544009\n",
      "Epoch [20/150], Train Loss: 27.253628158569335, Test Loss: 36.6670394996544\n",
      "Epoch [30/150], Train Loss: 25.88005392355997, Test Loss: 29.775396817690368\n",
      "Epoch [40/150], Train Loss: 26.815857608982775, Test Loss: 29.393455307205002\n",
      "Epoch [50/150], Train Loss: 25.19220362803975, Test Loss: 30.63159063264921\n",
      "Epoch [60/150], Train Loss: 25.18479331594999, Test Loss: 28.65929214675705\n",
      "Epoch [70/150], Train Loss: 26.12405493064005, Test Loss: 28.971178847473936\n",
      "Epoch [80/150], Train Loss: 26.143031611208055, Test Loss: 31.009358096432376\n",
      "Epoch [90/150], Train Loss: 24.657813000288165, Test Loss: 28.2538518038663\n",
      "Epoch [100/150], Train Loss: 24.85779939245005, Test Loss: 27.88239828332678\n",
      "Epoch [110/150], Train Loss: 25.27212583823282, Test Loss: 30.150556935892478\n",
      "Epoch [120/150], Train Loss: 25.138808072199588, Test Loss: 29.055993488856725\n",
      "Epoch [130/150], Train Loss: 23.82686944555064, Test Loss: 29.20514711157068\n",
      "Epoch [140/150], Train Loss: 24.096357795840404, Test Loss: 28.488564206408217\n",
      "Epoch [150/150], Train Loss: 24.58190833545122, Test Loss: 29.7315646827995\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.992144300116866, Test Loss: 37.43014556092101\n",
      "Epoch [20/150], Train Loss: 26.595566333708216, Test Loss: 32.40278333193296\n",
      "Epoch [30/150], Train Loss: 27.575684119052575, Test Loss: 30.272606911597315\n",
      "Epoch [40/150], Train Loss: 25.73057491114882, Test Loss: 28.994282957795377\n",
      "Epoch [50/150], Train Loss: 26.728209110947905, Test Loss: 27.81922214062183\n",
      "Epoch [60/150], Train Loss: 25.480458431556578, Test Loss: 29.663248805256632\n",
      "Epoch [70/150], Train Loss: 25.879568906690253, Test Loss: 27.04140804340313\n",
      "Epoch [80/150], Train Loss: 26.556961910060195, Test Loss: 29.546964992176402\n",
      "Epoch [90/150], Train Loss: 24.48241867315574, Test Loss: 30.327947889055526\n",
      "Epoch [100/150], Train Loss: 25.360335941002017, Test Loss: 28.473682898979682\n",
      "Epoch [110/150], Train Loss: 24.449978074871126, Test Loss: 28.501126103586966\n",
      "Epoch [120/150], Train Loss: 24.875568027183657, Test Loss: 28.10239838934564\n",
      "Epoch [130/150], Train Loss: 25.59594006147541, Test Loss: 29.015006226378603\n",
      "Epoch [140/150], Train Loss: 24.288836769979508, Test Loss: 27.757495558107053\n",
      "Epoch [150/150], Train Loss: 24.897101042700594, Test Loss: 27.63692635375184\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.16688540099097, Test Loss: 32.941057205200195\n",
      "Epoch [20/150], Train Loss: 27.255382168879276, Test Loss: 30.698418481009348\n",
      "Epoch [30/150], Train Loss: 27.39859637901431, Test Loss: 28.737025768725903\n",
      "Epoch [40/150], Train Loss: 25.851719365354445, Test Loss: 29.984624540650998\n",
      "Epoch [50/150], Train Loss: 25.086467323928584, Test Loss: 29.862500525140142\n",
      "Epoch [60/150], Train Loss: 25.389556396984663, Test Loss: 27.396045858209785\n",
      "Epoch [70/150], Train Loss: 25.33866367652768, Test Loss: 28.296410919783952\n",
      "Epoch [80/150], Train Loss: 24.01105878235864, Test Loss: 27.376378591958577\n",
      "Epoch [90/150], Train Loss: 25.888389749995998, Test Loss: 28.975641944191672\n",
      "Epoch [100/150], Train Loss: 27.03190532746862, Test Loss: 27.554225624381722\n",
      "Epoch [110/150], Train Loss: 24.962702823076093, Test Loss: 28.488176172429863\n",
      "Epoch [120/150], Train Loss: 24.568975386072378, Test Loss: 27.468000065196644\n",
      "Epoch [130/150], Train Loss: 25.400753265130714, Test Loss: 28.83849993619052\n",
      "Epoch [140/150], Train Loss: 24.82831557852323, Test Loss: 28.3115518495634\n",
      "Epoch [150/150], Train Loss: 24.404529821677286, Test Loss: 27.50242074743494\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.525234904054734, Test Loss: 35.34090844686929\n",
      "Epoch [20/150], Train Loss: 28.54304865227371, Test Loss: 30.073554001845324\n",
      "Epoch [30/150], Train Loss: 26.794023107309812, Test Loss: 32.99411679552747\n",
      "Epoch [40/150], Train Loss: 26.27430809521284, Test Loss: 30.78773795784294\n",
      "Epoch [50/150], Train Loss: 26.021422639440317, Test Loss: 32.2794211994518\n",
      "Epoch [60/150], Train Loss: 27.04257574237761, Test Loss: 30.201241257902865\n",
      "Epoch [70/150], Train Loss: 25.862577607201747, Test Loss: 28.10544172509924\n",
      "Epoch [80/150], Train Loss: 26.34107551574707, Test Loss: 28.853037772240576\n",
      "Epoch [90/150], Train Loss: 25.120652946096953, Test Loss: 27.651077295278576\n",
      "Epoch [100/150], Train Loss: 25.924640899408057, Test Loss: 27.66356240309678\n",
      "Epoch [110/150], Train Loss: 25.658890752323337, Test Loss: 28.031593644773807\n",
      "Epoch [120/150], Train Loss: 25.482843599163118, Test Loss: 28.92263293575931\n",
      "Epoch [130/150], Train Loss: 25.58305597774318, Test Loss: 27.299356733049667\n",
      "Epoch [140/150], Train Loss: 24.70987558208528, Test Loss: 29.870999447711103\n",
      "Epoch [150/150], Train Loss: 24.931004583640178, Test Loss: 29.455354120824243\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.890367126464845, Test Loss: 35.36395006055956\n",
      "Epoch [20/150], Train Loss: 28.181193023431497, Test Loss: 33.576425428514355\n",
      "Epoch [30/150], Train Loss: 26.855756547021084, Test Loss: 40.587990996125455\n",
      "Epoch [40/150], Train Loss: 28.079324222001873, Test Loss: 30.94486543729708\n",
      "Epoch [50/150], Train Loss: 27.88663622121342, Test Loss: 33.962901747071896\n",
      "Epoch [60/150], Train Loss: 26.027293177120022, Test Loss: 29.15409856028371\n",
      "Epoch [70/150], Train Loss: 26.198451945820793, Test Loss: 29.461291498952097\n",
      "Epoch [80/150], Train Loss: 26.636634207553552, Test Loss: 29.716535939798728\n",
      "Epoch [90/150], Train Loss: 25.539062156051887, Test Loss: 28.741630975302165\n",
      "Epoch [100/150], Train Loss: 25.433503729398133, Test Loss: 28.638529740370714\n",
      "Epoch [110/150], Train Loss: 27.048037725980166, Test Loss: 28.349694462565633\n",
      "Epoch [120/150], Train Loss: 25.601461366747248, Test Loss: 28.90218568777109\n",
      "Epoch [130/150], Train Loss: 24.943044718758003, Test Loss: 30.519939001504476\n",
      "Epoch [140/150], Train Loss: 25.56788351340372, Test Loss: 33.230224188272054\n",
      "Epoch [150/150], Train Loss: 25.907138236624295, Test Loss: 29.576867561835748\n",
      "Training model with hidden size 128, batch size 64, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.780953691826493, Test Loss: 33.92169863217837\n",
      "Epoch [20/150], Train Loss: 26.39804647476947, Test Loss: 35.546934425056754\n",
      "Epoch [30/150], Train Loss: 27.746229465672226, Test Loss: 28.428921315577124\n",
      "Epoch [40/150], Train Loss: 26.15991171539807, Test Loss: 29.82795697992498\n",
      "Epoch [50/150], Train Loss: 26.262155695430568, Test Loss: 29.53239089173156\n",
      "Epoch [60/150], Train Loss: 26.40490789569792, Test Loss: 28.899632986489827\n",
      "Epoch [70/150], Train Loss: 25.321156961409773, Test Loss: 31.33539645702808\n",
      "Epoch [80/150], Train Loss: 26.15515386862833, Test Loss: 37.94945090157645\n",
      "Epoch [90/150], Train Loss: 25.458427741879323, Test Loss: 30.445665260414025\n",
      "Epoch [100/150], Train Loss: 26.222551064413103, Test Loss: 30.002325999272333\n",
      "Epoch [110/150], Train Loss: 26.00219651519275, Test Loss: 32.44643803385945\n",
      "Epoch [120/150], Train Loss: 25.39941487546827, Test Loss: 28.13273095465326\n",
      "Epoch [130/150], Train Loss: 23.532035783861506, Test Loss: 31.535342526126218\n",
      "Epoch [140/150], Train Loss: 25.44717797451332, Test Loss: 27.644553518914556\n",
      "Epoch [150/150], Train Loss: 25.012813230420722, Test Loss: 33.67341779733633\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.07172591412654, Test Loss: 48.04701614379883\n",
      "Epoch [20/150], Train Loss: 45.27897846659676, Test Loss: 43.92326354980469\n",
      "Epoch [30/150], Train Loss: 42.41750956050685, Test Loss: 40.844547271728516\n",
      "Epoch [40/150], Train Loss: 40.02678517826268, Test Loss: 38.18051528930664\n",
      "Epoch [50/150], Train Loss: 37.53557197695873, Test Loss: 39.30072021484375\n",
      "Epoch [60/150], Train Loss: 35.06992600237737, Test Loss: 37.04880142211914\n",
      "Epoch [70/150], Train Loss: 33.18446144979508, Test Loss: 34.86808776855469\n",
      "Epoch [80/150], Train Loss: 31.836137890424883, Test Loss: 35.81596374511719\n",
      "Epoch [90/150], Train Loss: 29.792401254372518, Test Loss: 33.368080139160156\n",
      "Epoch [100/150], Train Loss: 28.487769649067864, Test Loss: 32.068885803222656\n",
      "Epoch [110/150], Train Loss: 27.354707830460345, Test Loss: 29.270240783691406\n",
      "Epoch [120/150], Train Loss: 25.934039087764553, Test Loss: 28.746620178222656\n",
      "Epoch [130/150], Train Loss: 25.069350058133484, Test Loss: 28.517526626586914\n",
      "Epoch [140/150], Train Loss: 24.219725036621092, Test Loss: 28.373794555664062\n",
      "Epoch [150/150], Train Loss: 23.530473108760646, Test Loss: 28.804533004760742\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.05525797859567, Test Loss: 51.31793212890625\n",
      "Epoch [20/150], Train Loss: 45.241170226550494, Test Loss: 44.91971969604492\n",
      "Epoch [30/150], Train Loss: 42.42485540421283, Test Loss: 41.050228118896484\n",
      "Epoch [40/150], Train Loss: 39.83016459980949, Test Loss: 39.68439483642578\n",
      "Epoch [50/150], Train Loss: 37.538718389292235, Test Loss: 38.1025276184082\n",
      "Epoch [60/150], Train Loss: 35.208380239517965, Test Loss: 38.73622512817383\n",
      "Epoch [70/150], Train Loss: 33.38131392432041, Test Loss: 35.39509963989258\n",
      "Epoch [80/150], Train Loss: 31.91044057627193, Test Loss: 33.92612838745117\n",
      "Epoch [90/150], Train Loss: 29.577859922315252, Test Loss: 32.92366027832031\n",
      "Epoch [100/150], Train Loss: 28.539479865402473, Test Loss: 31.87523078918457\n",
      "Epoch [110/150], Train Loss: 26.838801768568697, Test Loss: 30.023103713989258\n",
      "Epoch [120/150], Train Loss: 25.684904655081326, Test Loss: 28.69361686706543\n",
      "Epoch [130/150], Train Loss: 25.35429010234895, Test Loss: 29.525026321411133\n",
      "Epoch [140/150], Train Loss: 25.042177875706408, Test Loss: 28.65459442138672\n",
      "Epoch [150/150], Train Loss: 24.19577753661109, Test Loss: 29.22952651977539\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.220859440037465, Test Loss: 46.30495071411133\n",
      "Epoch [20/150], Train Loss: 45.91578347878378, Test Loss: 41.2655143737793\n",
      "Epoch [30/150], Train Loss: 43.01093775014408, Test Loss: 39.643699645996094\n",
      "Epoch [40/150], Train Loss: 40.41155061565462, Test Loss: 41.71902847290039\n",
      "Epoch [50/150], Train Loss: 38.116480042504485, Test Loss: 39.390525817871094\n",
      "Epoch [60/150], Train Loss: 35.79972793078814, Test Loss: 35.85243606567383\n",
      "Epoch [70/150], Train Loss: 33.798273499285585, Test Loss: 33.81715774536133\n",
      "Epoch [80/150], Train Loss: 32.09046658375224, Test Loss: 32.36902618408203\n",
      "Epoch [90/150], Train Loss: 30.14223762887423, Test Loss: 32.222930908203125\n",
      "Epoch [100/150], Train Loss: 29.050704555824154, Test Loss: 30.45366668701172\n",
      "Epoch [110/150], Train Loss: 28.08766583301982, Test Loss: 31.459251403808594\n",
      "Epoch [120/150], Train Loss: 26.278956278816597, Test Loss: 30.534456253051758\n",
      "Epoch [130/150], Train Loss: 25.72313780862777, Test Loss: 28.81698989868164\n",
      "Epoch [140/150], Train Loss: 24.30766291383837, Test Loss: 28.5507755279541\n",
      "Epoch [150/150], Train Loss: 24.2382773289915, Test Loss: 28.65814971923828\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.85794453855421, Test Loss: 48.36195373535156\n",
      "Epoch [20/150], Train Loss: 45.002208309486264, Test Loss: 44.598209381103516\n",
      "Epoch [30/150], Train Loss: 42.174564811831615, Test Loss: 38.27363967895508\n",
      "Epoch [40/150], Train Loss: 40.001371965251984, Test Loss: 39.320552825927734\n",
      "Epoch [50/150], Train Loss: 36.96857907029449, Test Loss: 36.246646881103516\n",
      "Epoch [60/150], Train Loss: 34.96037526990546, Test Loss: 34.73093032836914\n",
      "Epoch [70/150], Train Loss: 32.86850477124824, Test Loss: 35.31336212158203\n",
      "Epoch [80/150], Train Loss: 31.701996812664095, Test Loss: 32.248130798339844\n",
      "Epoch [90/150], Train Loss: 30.324850976662557, Test Loss: 31.06066131591797\n",
      "Epoch [100/150], Train Loss: 27.702543477543063, Test Loss: 30.55137062072754\n",
      "Epoch [110/150], Train Loss: 26.630924124795882, Test Loss: 29.550317764282227\n",
      "Epoch [120/150], Train Loss: 25.49779869454806, Test Loss: 29.23596954345703\n",
      "Epoch [130/150], Train Loss: 25.5143283468778, Test Loss: 29.095884323120117\n",
      "Epoch [140/150], Train Loss: 24.14518915395268, Test Loss: 29.037151336669922\n",
      "Epoch [150/150], Train Loss: 23.52574791204734, Test Loss: 28.678409576416016\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.08137907434682, Test Loss: 50.76573181152344\n",
      "Epoch [20/150], Train Loss: 45.24417200557521, Test Loss: 45.124061584472656\n",
      "Epoch [30/150], Train Loss: 42.47433342855485, Test Loss: 40.791465759277344\n",
      "Epoch [40/150], Train Loss: 40.024207756167556, Test Loss: 39.02460861206055\n",
      "Epoch [50/150], Train Loss: 37.68841547731493, Test Loss: 37.637874603271484\n",
      "Epoch [60/150], Train Loss: 35.51051798335841, Test Loss: 35.386085510253906\n",
      "Epoch [70/150], Train Loss: 33.38240953664311, Test Loss: 34.999935150146484\n",
      "Epoch [80/150], Train Loss: 31.322114731835537, Test Loss: 33.79235076904297\n",
      "Epoch [90/150], Train Loss: 29.975063943081214, Test Loss: 31.59244728088379\n",
      "Epoch [100/150], Train Loss: 28.12802309130059, Test Loss: 30.03250503540039\n",
      "Epoch [110/150], Train Loss: 27.131766166061652, Test Loss: 30.061153411865234\n",
      "Epoch [120/150], Train Loss: 25.762668415757478, Test Loss: 30.538511276245117\n",
      "Epoch [130/150], Train Loss: 25.393849569852236, Test Loss: 28.17254066467285\n",
      "Epoch [140/150], Train Loss: 24.553552296122568, Test Loss: 28.433618545532227\n",
      "Epoch [150/150], Train Loss: 23.063839959316567, Test Loss: 28.20848274230957\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.315372954821974, Test Loss: 47.226951599121094\n",
      "Epoch [20/150], Train Loss: 45.75542931478532, Test Loss: 43.53831100463867\n",
      "Epoch [30/150], Train Loss: 43.203838173287814, Test Loss: 40.89680862426758\n",
      "Epoch [40/150], Train Loss: 40.69412606661437, Test Loss: 37.90235900878906\n",
      "Epoch [50/150], Train Loss: 38.659162365022254, Test Loss: 38.500404357910156\n",
      "Epoch [60/150], Train Loss: 36.05086220913246, Test Loss: 38.1302490234375\n",
      "Epoch [70/150], Train Loss: 34.088338958239945, Test Loss: 34.505210876464844\n",
      "Epoch [80/150], Train Loss: 32.084839692662975, Test Loss: 33.221317291259766\n",
      "Epoch [90/150], Train Loss: 30.258155872782723, Test Loss: 33.307228088378906\n",
      "Epoch [100/150], Train Loss: 28.98183478683722, Test Loss: 30.893367767333984\n",
      "Epoch [110/150], Train Loss: 27.591343276227107, Test Loss: 30.368680953979492\n",
      "Epoch [120/150], Train Loss: 26.59696777844038, Test Loss: 30.459218978881836\n",
      "Epoch [130/150], Train Loss: 25.783008875612353, Test Loss: 30.34830665588379\n",
      "Epoch [140/150], Train Loss: 25.0609251491359, Test Loss: 30.007038116455078\n",
      "Epoch [150/150], Train Loss: 24.53195095374936, Test Loss: 28.508747100830078\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 47.913708671194605, Test Loss: 50.108577728271484\n",
      "Epoch [20/150], Train Loss: 45.23651060510854, Test Loss: 45.93439865112305\n",
      "Epoch [30/150], Train Loss: 42.47884302608303, Test Loss: 43.64236068725586\n",
      "Epoch [40/150], Train Loss: 39.84450065737865, Test Loss: 40.16019821166992\n",
      "Epoch [50/150], Train Loss: 37.3029248534656, Test Loss: 38.075950622558594\n",
      "Epoch [60/150], Train Loss: 35.18421188104348, Test Loss: 35.965755462646484\n",
      "Epoch [70/150], Train Loss: 33.30942753025743, Test Loss: 39.301673889160156\n",
      "Epoch [80/150], Train Loss: 31.177980054011112, Test Loss: 31.262115478515625\n",
      "Epoch [90/150], Train Loss: 29.6392780116347, Test Loss: 31.544063568115234\n",
      "Epoch [100/150], Train Loss: 28.180744758981174, Test Loss: 31.737648010253906\n",
      "Epoch [110/150], Train Loss: 27.362003526531282, Test Loss: 31.381567001342773\n",
      "Epoch [120/150], Train Loss: 26.97042676581711, Test Loss: 29.421588897705078\n",
      "Epoch [130/150], Train Loss: 25.400883283771453, Test Loss: 30.26944923400879\n",
      "Epoch [140/150], Train Loss: 25.28674175074843, Test Loss: 29.76580810546875\n",
      "Epoch [150/150], Train Loss: 23.90499520223649, Test Loss: 28.15688133239746\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.021246725613956, Test Loss: 49.693138122558594\n",
      "Epoch [20/150], Train Loss: 45.21741619422787, Test Loss: 44.79694366455078\n",
      "Epoch [30/150], Train Loss: 42.44375560322746, Test Loss: 41.38693618774414\n",
      "Epoch [40/150], Train Loss: 39.99975777297723, Test Loss: 39.12942886352539\n",
      "Epoch [50/150], Train Loss: 37.45229674792681, Test Loss: 38.23746109008789\n",
      "Epoch [60/150], Train Loss: 35.68462501275735, Test Loss: 35.06285095214844\n",
      "Epoch [70/150], Train Loss: 33.819466049944765, Test Loss: 34.541786193847656\n",
      "Epoch [80/150], Train Loss: 31.535126751759012, Test Loss: 34.02675247192383\n",
      "Epoch [90/150], Train Loss: 30.149521561919666, Test Loss: 31.72747230529785\n",
      "Epoch [100/150], Train Loss: 28.76533254404537, Test Loss: 29.787124633789062\n",
      "Epoch [110/150], Train Loss: 27.264623204215628, Test Loss: 28.694992065429688\n",
      "Epoch [120/150], Train Loss: 26.171149382044057, Test Loss: 29.10728645324707\n",
      "Epoch [130/150], Train Loss: 25.958416985683755, Test Loss: 28.723220825195312\n",
      "Epoch [140/150], Train Loss: 26.101048497684666, Test Loss: 28.847579956054688\n",
      "Epoch [150/150], Train Loss: 24.065653685272718, Test Loss: 28.19733238220215\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 48.26521032364642, Test Loss: 46.95663070678711\n",
      "Epoch [20/150], Train Loss: 45.821683076952326, Test Loss: 44.97312927246094\n",
      "Epoch [30/150], Train Loss: 43.088378643598716, Test Loss: 41.36872482299805\n",
      "Epoch [40/150], Train Loss: 40.3681737055544, Test Loss: 37.71839141845703\n",
      "Epoch [50/150], Train Loss: 38.038703680820156, Test Loss: 40.8116455078125\n",
      "Epoch [60/150], Train Loss: 35.531334236019944, Test Loss: 37.01957702636719\n",
      "Epoch [70/150], Train Loss: 33.7111744177146, Test Loss: 33.763031005859375\n",
      "Epoch [80/150], Train Loss: 31.910980162073354, Test Loss: 31.977697372436523\n",
      "Epoch [90/150], Train Loss: 30.49929967161085, Test Loss: 31.031028747558594\n",
      "Epoch [100/150], Train Loss: 28.793087943655546, Test Loss: 31.671539306640625\n",
      "Epoch [110/150], Train Loss: 27.82582660737585, Test Loss: 30.58725357055664\n",
      "Epoch [120/150], Train Loss: 26.38454885013768, Test Loss: 30.675134658813477\n",
      "Epoch [130/150], Train Loss: 25.727222742799853, Test Loss: 28.82693862915039\n",
      "Epoch [140/150], Train Loss: 25.458392177644324, Test Loss: 29.382732391357422\n",
      "Epoch [150/150], Train Loss: 24.682501946120965, Test Loss: 28.94154167175293\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.752118095022734, Test Loss: 35.78879165649414\n",
      "Epoch [20/150], Train Loss: 26.532978345527024, Test Loss: 30.335582733154297\n",
      "Epoch [30/150], Train Loss: 25.120472292040215, Test Loss: 28.218324661254883\n",
      "Epoch [40/150], Train Loss: 24.335242799852715, Test Loss: 31.944875717163086\n",
      "Epoch [50/150], Train Loss: 24.787890518688766, Test Loss: 29.429750442504883\n",
      "Epoch [60/150], Train Loss: 23.60739308341605, Test Loss: 27.792476654052734\n",
      "Epoch [70/150], Train Loss: 22.78200243340164, Test Loss: 30.747732162475586\n",
      "Epoch [80/150], Train Loss: 23.429237991082864, Test Loss: 29.16265869140625\n",
      "Epoch [90/150], Train Loss: 23.1703589267418, Test Loss: 29.53057098388672\n",
      "Epoch [100/150], Train Loss: 23.057443474941568, Test Loss: 30.03045654296875\n",
      "Epoch [110/150], Train Loss: 22.295038229520202, Test Loss: 29.488061904907227\n",
      "Epoch [120/150], Train Loss: 21.227542996015703, Test Loss: 28.006868362426758\n",
      "Epoch [130/150], Train Loss: 21.75644504359511, Test Loss: 29.003538131713867\n",
      "Epoch [140/150], Train Loss: 21.792544593185674, Test Loss: 29.236265182495117\n",
      "Epoch [150/150], Train Loss: 21.101397742599737, Test Loss: 29.58831787109375\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.39110915387263, Test Loss: 36.56674575805664\n",
      "Epoch [20/150], Train Loss: 26.657588808653784, Test Loss: 34.82941818237305\n",
      "Epoch [30/150], Train Loss: 24.596252366363025, Test Loss: 30.555204391479492\n",
      "Epoch [40/150], Train Loss: 24.518998524400054, Test Loss: 27.874670028686523\n",
      "Epoch [50/150], Train Loss: 24.502847627733576, Test Loss: 27.938762664794922\n",
      "Epoch [60/150], Train Loss: 23.63629846416536, Test Loss: 35.524078369140625\n",
      "Epoch [70/150], Train Loss: 23.49734235669746, Test Loss: 29.1069393157959\n",
      "Epoch [80/150], Train Loss: 22.781764696465164, Test Loss: 28.426742553710938\n",
      "Epoch [90/150], Train Loss: 22.785875902019562, Test Loss: 28.256202697753906\n",
      "Epoch [100/150], Train Loss: 22.04293463660068, Test Loss: 29.004566192626953\n",
      "Epoch [110/150], Train Loss: 22.62279643699771, Test Loss: 31.40644645690918\n",
      "Epoch [120/150], Train Loss: 21.849369611896453, Test Loss: 29.50709342956543\n",
      "Epoch [130/150], Train Loss: 22.40944293913294, Test Loss: 30.776334762573242\n",
      "Epoch [140/150], Train Loss: 21.862433880665264, Test Loss: 31.049985885620117\n",
      "Epoch [150/150], Train Loss: 20.353179181208375, Test Loss: 29.765384674072266\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.561895326708186, Test Loss: 33.111480712890625\n",
      "Epoch [20/150], Train Loss: 25.646336389760503, Test Loss: 29.22586441040039\n",
      "Epoch [30/150], Train Loss: 24.026031006359663, Test Loss: 29.67481803894043\n",
      "Epoch [40/150], Train Loss: 24.98646966902936, Test Loss: 31.076908111572266\n",
      "Epoch [50/150], Train Loss: 24.93816005519179, Test Loss: 28.34185028076172\n",
      "Epoch [60/150], Train Loss: 23.876760301433627, Test Loss: 30.09317970275879\n",
      "Epoch [70/150], Train Loss: 22.875887454924037, Test Loss: 30.592037200927734\n",
      "Epoch [80/150], Train Loss: 23.48871883329798, Test Loss: 29.12342071533203\n",
      "Epoch [90/150], Train Loss: 22.85972426992948, Test Loss: 28.604076385498047\n",
      "Epoch [100/150], Train Loss: 23.392141160808627, Test Loss: 28.006420135498047\n",
      "Epoch [110/150], Train Loss: 22.721802702106412, Test Loss: 28.2343692779541\n",
      "Epoch [120/150], Train Loss: 23.14698841532723, Test Loss: 29.05027961730957\n",
      "Epoch [130/150], Train Loss: 22.97749955224209, Test Loss: 28.16586685180664\n",
      "Epoch [140/150], Train Loss: 21.04249333866307, Test Loss: 28.861011505126953\n",
      "Epoch [150/150], Train Loss: 21.783705314261013, Test Loss: 28.545698165893555\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.13842009872687, Test Loss: 37.18602752685547\n",
      "Epoch [20/150], Train Loss: 27.598618447976033, Test Loss: 38.39377212524414\n",
      "Epoch [30/150], Train Loss: 24.587277127875655, Test Loss: 29.468658447265625\n",
      "Epoch [40/150], Train Loss: 25.237461058819882, Test Loss: 29.3103084564209\n",
      "Epoch [50/150], Train Loss: 24.43960006588795, Test Loss: 30.317230224609375\n",
      "Epoch [60/150], Train Loss: 24.113260075303376, Test Loss: 28.2788143157959\n",
      "Epoch [70/150], Train Loss: 23.452674015232773, Test Loss: 29.433738708496094\n",
      "Epoch [80/150], Train Loss: 24.209980148565574, Test Loss: 29.566621780395508\n",
      "Epoch [90/150], Train Loss: 22.89980358061243, Test Loss: 28.56753158569336\n",
      "Epoch [100/150], Train Loss: 23.037448032566758, Test Loss: 29.63751792907715\n",
      "Epoch [110/150], Train Loss: 22.327431031524156, Test Loss: 29.078704833984375\n",
      "Epoch [120/150], Train Loss: 22.872236076730196, Test Loss: 28.729347229003906\n",
      "Epoch [130/150], Train Loss: 22.516807199697027, Test Loss: 29.656225204467773\n",
      "Epoch [140/150], Train Loss: 23.14438850527904, Test Loss: 28.84760856628418\n",
      "Epoch [150/150], Train Loss: 21.787642669677734, Test Loss: 28.911203384399414\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.716213870439372, Test Loss: 35.59247970581055\n",
      "Epoch [20/150], Train Loss: 25.235269627805614, Test Loss: 31.275590896606445\n",
      "Epoch [30/150], Train Loss: 24.335485389584402, Test Loss: 32.086883544921875\n",
      "Epoch [40/150], Train Loss: 23.919687308639777, Test Loss: 27.88242530822754\n",
      "Epoch [50/150], Train Loss: 24.150619675683192, Test Loss: 29.30672264099121\n",
      "Epoch [60/150], Train Loss: 23.494736824661004, Test Loss: 28.618440628051758\n",
      "Epoch [70/150], Train Loss: 23.605671316678407, Test Loss: 28.98204803466797\n",
      "Epoch [80/150], Train Loss: 23.172635875764442, Test Loss: 31.84269142150879\n",
      "Epoch [90/150], Train Loss: 23.45567575048228, Test Loss: 30.11601448059082\n",
      "Epoch [100/150], Train Loss: 23.422729154492988, Test Loss: 27.88197898864746\n",
      "Epoch [110/150], Train Loss: 22.54645149668709, Test Loss: 28.400827407836914\n",
      "Epoch [120/150], Train Loss: 22.124960883719023, Test Loss: 29.66678237915039\n",
      "Epoch [130/150], Train Loss: 22.087715574170723, Test Loss: 30.411649703979492\n",
      "Epoch [140/150], Train Loss: 21.429317862088563, Test Loss: 29.57025909423828\n",
      "Epoch [150/150], Train Loss: 21.668852752935692, Test Loss: 29.407896041870117\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.558573244438797, Test Loss: 35.73509216308594\n",
      "Epoch [20/150], Train Loss: 26.42626190810907, Test Loss: 29.215354919433594\n",
      "Epoch [30/150], Train Loss: 25.20620532426678, Test Loss: 29.902740478515625\n",
      "Epoch [40/150], Train Loss: 25.922281315287606, Test Loss: 28.50260353088379\n",
      "Epoch [50/150], Train Loss: 24.84615788694288, Test Loss: 27.536291122436523\n",
      "Epoch [60/150], Train Loss: 23.729966022929208, Test Loss: 28.148006439208984\n",
      "Epoch [70/150], Train Loss: 23.610201419767787, Test Loss: 29.370912551879883\n",
      "Epoch [80/150], Train Loss: 24.07336266314397, Test Loss: 27.937255859375\n",
      "Epoch [90/150], Train Loss: 23.064252459416625, Test Loss: 31.6401424407959\n",
      "Epoch [100/150], Train Loss: 23.302023815717853, Test Loss: 28.595041275024414\n",
      "Epoch [110/150], Train Loss: 22.73490086539847, Test Loss: 28.645416259765625\n",
      "Epoch [120/150], Train Loss: 22.830702203219055, Test Loss: 29.926647186279297\n",
      "Epoch [130/150], Train Loss: 21.705578069217868, Test Loss: 28.91221046447754\n",
      "Epoch [140/150], Train Loss: 21.712580390054672, Test Loss: 27.597415924072266\n",
      "Epoch [150/150], Train Loss: 23.283480891243357, Test Loss: 29.7053165435791\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.968117154230836, Test Loss: 37.85066604614258\n",
      "Epoch [20/150], Train Loss: 27.22309804822578, Test Loss: 32.51810836791992\n",
      "Epoch [30/150], Train Loss: 24.968343447075515, Test Loss: 32.262046813964844\n",
      "Epoch [40/150], Train Loss: 25.309481586393762, Test Loss: 29.041828155517578\n",
      "Epoch [50/150], Train Loss: 25.301648818469438, Test Loss: 29.076213836669922\n",
      "Epoch [60/150], Train Loss: 25.10646706878162, Test Loss: 27.987993240356445\n",
      "Epoch [70/150], Train Loss: 24.600012857405865, Test Loss: 31.888212203979492\n",
      "Epoch [80/150], Train Loss: 24.745283608358413, Test Loss: 30.205677032470703\n",
      "Epoch [90/150], Train Loss: 25.775608650582736, Test Loss: 28.268646240234375\n",
      "Epoch [100/150], Train Loss: 24.728269045470192, Test Loss: 27.817228317260742\n",
      "Epoch [110/150], Train Loss: 24.755944586581872, Test Loss: 29.08330535888672\n",
      "Epoch [120/150], Train Loss: 24.164486869436796, Test Loss: 29.10601043701172\n",
      "Epoch [130/150], Train Loss: 24.092305930716094, Test Loss: 27.67174530029297\n",
      "Epoch [140/150], Train Loss: 24.194590152677943, Test Loss: 28.28384017944336\n",
      "Epoch [150/150], Train Loss: 22.898978868078014, Test Loss: 28.292293548583984\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.990856258204726, Test Loss: 36.48903274536133\n",
      "Epoch [20/150], Train Loss: 29.19069315175541, Test Loss: 31.194196701049805\n",
      "Epoch [30/150], Train Loss: 26.247190888201605, Test Loss: 32.47005081176758\n",
      "Epoch [40/150], Train Loss: 26.122044972904394, Test Loss: 29.873865127563477\n",
      "Epoch [50/150], Train Loss: 26.100083679449362, Test Loss: 29.96420669555664\n",
      "Epoch [60/150], Train Loss: 25.169761263737914, Test Loss: 31.617183685302734\n",
      "Epoch [70/150], Train Loss: 24.594380356835536, Test Loss: 28.473360061645508\n",
      "Epoch [80/150], Train Loss: 24.442458143390592, Test Loss: 28.85323715209961\n",
      "Epoch [90/150], Train Loss: 24.90986533243148, Test Loss: 28.511798858642578\n",
      "Epoch [100/150], Train Loss: 24.59115924522525, Test Loss: 31.070709228515625\n",
      "Epoch [110/150], Train Loss: 24.114291888377707, Test Loss: 28.221147537231445\n",
      "Epoch [120/150], Train Loss: 23.77643413856381, Test Loss: 28.797880172729492\n",
      "Epoch [130/150], Train Loss: 24.35252895667905, Test Loss: 27.36674690246582\n",
      "Epoch [140/150], Train Loss: 23.371360297281235, Test Loss: 28.65655517578125\n",
      "Epoch [150/150], Train Loss: 23.309777206671043, Test Loss: 31.218294143676758\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.707362790967597, Test Loss: 36.933040618896484\n",
      "Epoch [20/150], Train Loss: 26.88374352376969, Test Loss: 33.044464111328125\n",
      "Epoch [30/150], Train Loss: 25.678611642806256, Test Loss: 30.587522506713867\n",
      "Epoch [40/150], Train Loss: 26.050330127653528, Test Loss: 29.32212257385254\n",
      "Epoch [50/150], Train Loss: 25.42643193729588, Test Loss: 30.497928619384766\n",
      "Epoch [60/150], Train Loss: 25.474157614786115, Test Loss: 29.681100845336914\n",
      "Epoch [70/150], Train Loss: 24.41287088863185, Test Loss: 29.23685073852539\n",
      "Epoch [80/150], Train Loss: 24.362020267424036, Test Loss: 29.765104293823242\n",
      "Epoch [90/150], Train Loss: 24.746501503616084, Test Loss: 29.26736831665039\n",
      "Epoch [100/150], Train Loss: 24.37552368914495, Test Loss: 28.26539421081543\n",
      "Epoch [110/150], Train Loss: 24.03224545463187, Test Loss: 29.860553741455078\n",
      "Epoch [120/150], Train Loss: 24.905569558065444, Test Loss: 29.782907485961914\n",
      "Epoch [130/150], Train Loss: 24.017308457171332, Test Loss: 30.593008041381836\n",
      "Epoch [140/150], Train Loss: 23.740453601274336, Test Loss: 32.13300704956055\n",
      "Epoch [150/150], Train Loss: 24.53717143574699, Test Loss: 27.591413497924805\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.37696984713195, Test Loss: 32.11742401123047\n",
      "Epoch [20/150], Train Loss: 26.317387358868707, Test Loss: 32.75912857055664\n",
      "Epoch [30/150], Train Loss: 25.566497158613362, Test Loss: 34.91007614135742\n",
      "Epoch [40/150], Train Loss: 26.266172546636863, Test Loss: 29.504907608032227\n",
      "Epoch [50/150], Train Loss: 25.811028221005298, Test Loss: 30.341022491455078\n",
      "Epoch [60/150], Train Loss: 25.71723123769291, Test Loss: 29.483049392700195\n",
      "Epoch [70/150], Train Loss: 24.29818539853956, Test Loss: 28.320964813232422\n",
      "Epoch [80/150], Train Loss: 24.749132387755346, Test Loss: 30.896148681640625\n",
      "Epoch [90/150], Train Loss: 24.557527698454308, Test Loss: 27.442445755004883\n",
      "Epoch [100/150], Train Loss: 24.046629671190605, Test Loss: 31.22407341003418\n",
      "Epoch [110/150], Train Loss: 24.472411765426887, Test Loss: 27.250925064086914\n",
      "Epoch [120/150], Train Loss: 23.590652103111392, Test Loss: 29.183656692504883\n",
      "Epoch [130/150], Train Loss: 23.819516766657596, Test Loss: 28.60987663269043\n",
      "Epoch [140/150], Train Loss: 23.43016218591909, Test Loss: 26.762042999267578\n",
      "Epoch [150/150], Train Loss: 22.614622466290584, Test Loss: 27.773221969604492\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.217553473300622, Test Loss: 33.3135986328125\n",
      "Epoch [20/150], Train Loss: 26.805327981417296, Test Loss: 28.61945152282715\n",
      "Epoch [30/150], Train Loss: 26.202256137034933, Test Loss: 30.414011001586914\n",
      "Epoch [40/150], Train Loss: 25.397897507714443, Test Loss: 29.28997802734375\n",
      "Epoch [50/150], Train Loss: 24.56648076792232, Test Loss: 29.263092041015625\n",
      "Epoch [60/150], Train Loss: 24.24696262547227, Test Loss: 28.382936477661133\n",
      "Epoch [70/150], Train Loss: 25.37051441630379, Test Loss: 27.449792861938477\n",
      "Epoch [80/150], Train Loss: 24.138499388147572, Test Loss: 30.543581008911133\n",
      "Epoch [90/150], Train Loss: 23.83649253845215, Test Loss: 30.358234405517578\n",
      "Epoch [100/150], Train Loss: 24.219142000792456, Test Loss: 29.802417755126953\n",
      "Epoch [110/150], Train Loss: 24.196097883630973, Test Loss: 28.865764617919922\n",
      "Epoch [120/150], Train Loss: 23.360486515232775, Test Loss: 28.71078872680664\n",
      "Epoch [130/150], Train Loss: 22.66419785921691, Test Loss: 29.814821243286133\n",
      "Epoch [140/150], Train Loss: 23.301074912899832, Test Loss: 29.684383392333984\n",
      "Epoch [150/150], Train Loss: 22.14044647842157, Test Loss: 28.16107177734375\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.140230635345958, Test Loss: 36.15271759033203\n",
      "Epoch [20/150], Train Loss: 26.0111852614606, Test Loss: 32.851905822753906\n",
      "Epoch [30/150], Train Loss: 26.41559738409324, Test Loss: 28.342336654663086\n",
      "Epoch [40/150], Train Loss: 25.50981834286549, Test Loss: 28.76988410949707\n",
      "Epoch [50/150], Train Loss: 24.720181637122984, Test Loss: 27.935514450073242\n",
      "Epoch [60/150], Train Loss: 24.712527884811653, Test Loss: 30.83970069885254\n",
      "Epoch [70/150], Train Loss: 24.675236479962457, Test Loss: 28.19534683227539\n",
      "Epoch [80/150], Train Loss: 23.79566697918001, Test Loss: 28.784208297729492\n",
      "Epoch [90/150], Train Loss: 24.57084440012447, Test Loss: 27.664377212524414\n",
      "Epoch [100/150], Train Loss: 23.442905007034053, Test Loss: 28.069700241088867\n",
      "Epoch [110/150], Train Loss: 24.2708390658019, Test Loss: 30.53863525390625\n",
      "Epoch [120/150], Train Loss: 24.17341511210457, Test Loss: 28.932268142700195\n",
      "Epoch [130/150], Train Loss: 23.220348439451122, Test Loss: 30.415571212768555\n",
      "Epoch [140/150], Train Loss: 23.449811397615026, Test Loss: 31.737281799316406\n",
      "Epoch [150/150], Train Loss: 23.16710623444104, Test Loss: 27.548397064208984\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 32.20914938879795, Test Loss: 33.154701232910156\n",
      "Epoch [20/150], Train Loss: 25.956677715113905, Test Loss: 34.05810546875\n",
      "Epoch [30/150], Train Loss: 25.28772457935771, Test Loss: 34.56728744506836\n",
      "Epoch [40/150], Train Loss: 26.30156870357326, Test Loss: 31.170127868652344\n",
      "Epoch [50/150], Train Loss: 25.829382411769178, Test Loss: 29.47846794128418\n",
      "Epoch [60/150], Train Loss: 25.40761076505067, Test Loss: 29.083166122436523\n",
      "Epoch [70/150], Train Loss: 24.69883770551838, Test Loss: 29.624515533447266\n",
      "Epoch [80/150], Train Loss: 24.749907871934234, Test Loss: 30.075883865356445\n",
      "Epoch [90/150], Train Loss: 24.83202654103764, Test Loss: 30.77239418029785\n",
      "Epoch [100/150], Train Loss: 24.705158611985503, Test Loss: 30.498689651489258\n",
      "Epoch [110/150], Train Loss: 23.500462159954132, Test Loss: 30.466758728027344\n",
      "Epoch [120/150], Train Loss: 23.970952474875528, Test Loss: 31.450021743774414\n",
      "Epoch [130/150], Train Loss: 24.88381305131756, Test Loss: 28.599733352661133\n",
      "Epoch [140/150], Train Loss: 24.08697859341981, Test Loss: 28.524763107299805\n",
      "Epoch [150/150], Train Loss: 23.56095157060467, Test Loss: 36.127037048339844\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.992184316916543, Test Loss: 34.434303283691406\n",
      "Epoch [20/150], Train Loss: 26.503063764728484, Test Loss: 32.43476486206055\n",
      "Epoch [30/150], Train Loss: 26.46220648718662, Test Loss: 31.740982055664062\n",
      "Epoch [40/150], Train Loss: 27.587405276689374, Test Loss: 32.501609802246094\n",
      "Epoch [50/150], Train Loss: 24.766485376827053, Test Loss: 29.84126091003418\n",
      "Epoch [60/150], Train Loss: 25.41628646850586, Test Loss: 31.942195892333984\n",
      "Epoch [70/150], Train Loss: 25.756057364041688, Test Loss: 29.098121643066406\n",
      "Epoch [80/150], Train Loss: 24.67823901567303, Test Loss: 29.516645431518555\n",
      "Epoch [90/150], Train Loss: 26.047932233966765, Test Loss: 29.46453857421875\n",
      "Epoch [100/150], Train Loss: 24.88723027588891, Test Loss: 29.456539154052734\n",
      "Epoch [110/150], Train Loss: 25.706102990322425, Test Loss: 27.3574161529541\n",
      "Epoch [120/150], Train Loss: 24.584979010409995, Test Loss: 31.41943359375\n",
      "Epoch [130/150], Train Loss: 24.55985327548668, Test Loss: 29.284921646118164\n",
      "Epoch [140/150], Train Loss: 23.870753116294985, Test Loss: 30.150358200073242\n",
      "Epoch [150/150], Train Loss: 23.624593415807507, Test Loss: 28.542888641357422\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.089275816620372, Test Loss: 31.6210880279541\n",
      "Epoch [20/150], Train Loss: 26.749655057563157, Test Loss: 34.60935592651367\n",
      "Epoch [30/150], Train Loss: 27.61762293831247, Test Loss: 28.10940933227539\n",
      "Epoch [40/150], Train Loss: 25.627796517043816, Test Loss: 28.687240600585938\n",
      "Epoch [50/150], Train Loss: 26.5689847914899, Test Loss: 30.780216217041016\n",
      "Epoch [60/150], Train Loss: 25.858771740022252, Test Loss: 30.184614181518555\n",
      "Epoch [70/150], Train Loss: 25.738001538886397, Test Loss: 29.53624153137207\n",
      "Epoch [80/150], Train Loss: 25.91635874138504, Test Loss: 30.389820098876953\n",
      "Epoch [90/150], Train Loss: 24.59746265098697, Test Loss: 29.09406089782715\n",
      "Epoch [100/150], Train Loss: 24.550713686083185, Test Loss: 28.92197608947754\n",
      "Epoch [110/150], Train Loss: 24.849835211331726, Test Loss: 28.957624435424805\n",
      "Epoch [120/150], Train Loss: 24.819163581973218, Test Loss: 28.929941177368164\n",
      "Epoch [130/150], Train Loss: 24.4680525420142, Test Loss: 28.119232177734375\n",
      "Epoch [140/150], Train Loss: 24.169862984829262, Test Loss: 27.964397430419922\n",
      "Epoch [150/150], Train Loss: 24.60327679368316, Test Loss: 29.089244842529297\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.332722985939903, Test Loss: 36.78037643432617\n",
      "Epoch [20/150], Train Loss: 27.344249781624214, Test Loss: 32.338619232177734\n",
      "Epoch [30/150], Train Loss: 28.12933609133861, Test Loss: 32.01581954956055\n",
      "Epoch [40/150], Train Loss: 25.67728156418097, Test Loss: 32.98886489868164\n",
      "Epoch [50/150], Train Loss: 27.352377012909436, Test Loss: 32.12746810913086\n",
      "Epoch [60/150], Train Loss: 26.670969334586722, Test Loss: 30.956989288330078\n",
      "Epoch [70/150], Train Loss: 26.494120181974818, Test Loss: 29.112863540649414\n",
      "Epoch [80/150], Train Loss: 25.221432226212297, Test Loss: 30.243244171142578\n",
      "Epoch [90/150], Train Loss: 25.429532648305425, Test Loss: 28.086627960205078\n",
      "Epoch [100/150], Train Loss: 25.27016228222456, Test Loss: 29.992231369018555\n",
      "Epoch [110/150], Train Loss: 24.653898232882142, Test Loss: 30.53862762451172\n",
      "Epoch [120/150], Train Loss: 25.395468621175798, Test Loss: 32.28932189941406\n",
      "Epoch [130/150], Train Loss: 24.463479107716044, Test Loss: 30.880203247070312\n",
      "Epoch [140/150], Train Loss: 25.038586738461355, Test Loss: 28.35809326171875\n",
      "Epoch [150/150], Train Loss: 24.389465519639312, Test Loss: 28.867895126342773\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.762001450335394, Test Loss: 36.026241302490234\n",
      "Epoch [20/150], Train Loss: 26.614976226306354, Test Loss: 64.09676361083984\n",
      "Epoch [30/150], Train Loss: 27.899099593866065, Test Loss: 32.544620513916016\n",
      "Epoch [40/150], Train Loss: 27.004520991591157, Test Loss: 29.036304473876953\n",
      "Epoch [50/150], Train Loss: 26.11187412699715, Test Loss: 31.53213119506836\n",
      "Epoch [60/150], Train Loss: 25.811061296306672, Test Loss: 41.627037048339844\n",
      "Epoch [70/150], Train Loss: 26.522343457331424, Test Loss: 40.61416244506836\n",
      "Epoch [80/150], Train Loss: 25.52179443484447, Test Loss: 34.465293884277344\n",
      "Epoch [90/150], Train Loss: 25.418442891855708, Test Loss: 29.5961971282959\n",
      "Epoch [100/150], Train Loss: 26.171506750388225, Test Loss: 34.53585433959961\n",
      "Epoch [110/150], Train Loss: 26.26308482435883, Test Loss: 32.25718307495117\n",
      "Epoch [120/150], Train Loss: 24.82447892486072, Test Loss: 33.13060760498047\n",
      "Epoch [130/150], Train Loss: 25.48583758619965, Test Loss: 30.074459075927734\n",
      "Epoch [140/150], Train Loss: 24.888113828565253, Test Loss: 32.928775787353516\n",
      "Epoch [150/150], Train Loss: 27.203994425789254, Test Loss: 29.488567352294922\n",
      "Training model with hidden size 128, batch size 128, learning rate 0.5, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 30.315893842353194, Test Loss: 35.99497985839844\n",
      "Epoch [20/150], Train Loss: 28.29983380427126, Test Loss: 33.8162956237793\n",
      "Epoch [30/150], Train Loss: 26.074259579767947, Test Loss: 40.26689910888672\n",
      "Epoch [40/150], Train Loss: 25.985853701732196, Test Loss: 35.76690673828125\n",
      "Epoch [50/150], Train Loss: 26.7828673565974, Test Loss: 30.063318252563477\n",
      "Epoch [60/150], Train Loss: 25.346335789414702, Test Loss: 33.15008544921875\n",
      "Epoch [70/150], Train Loss: 25.873837811829613, Test Loss: 31.405475616455078\n",
      "Epoch [80/150], Train Loss: 25.38137973097504, Test Loss: 29.03740119934082\n",
      "Epoch [90/150], Train Loss: 26.240121641315397, Test Loss: 30.669065475463867\n",
      "Epoch [100/150], Train Loss: 25.848842514538376, Test Loss: 29.926197052001953\n",
      "Epoch [110/150], Train Loss: 25.261468562141793, Test Loss: 29.808393478393555\n",
      "Epoch [120/150], Train Loss: 24.585601850415838, Test Loss: 29.363473892211914\n",
      "Epoch [130/150], Train Loss: 25.56855636971896, Test Loss: 29.012426376342773\n",
      "Epoch [140/150], Train Loss: 25.594326306952805, Test Loss: 29.920141220092773\n",
      "Epoch [150/150], Train Loss: 25.401273827474625, Test Loss: 31.73843002319336\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 37.954203333620164, Test Loss: 35.442860541405615\n",
      "Epoch [20/150], Train Loss: 30.727419456106716, Test Loss: 31.209198295296012\n",
      "Epoch [30/150], Train Loss: 27.222018345066758, Test Loss: 29.139016039959795\n",
      "Epoch [40/150], Train Loss: 25.522861186793595, Test Loss: 28.14087473881709\n",
      "Epoch [50/150], Train Loss: 25.567617860387582, Test Loss: 28.147067924598595\n",
      "Epoch [60/150], Train Loss: 25.395456870657497, Test Loss: 28.283927322982194\n",
      "Epoch [70/150], Train Loss: 24.268633432857325, Test Loss: 28.471910105123147\n",
      "Epoch [80/150], Train Loss: 24.09098445704726, Test Loss: 28.332088594312793\n",
      "Epoch [90/150], Train Loss: 23.746378204470776, Test Loss: 27.818728533658113\n",
      "Epoch [100/150], Train Loss: 23.712461915563363, Test Loss: 27.649783716573342\n",
      "Epoch [110/150], Train Loss: 25.009539826189886, Test Loss: 27.69806569582456\n",
      "Epoch [120/150], Train Loss: 23.800973298119718, Test Loss: 28.73242982641443\n",
      "Epoch [130/150], Train Loss: 22.782660937700115, Test Loss: 28.28449838811701\n",
      "Epoch [140/150], Train Loss: 23.202518531924387, Test Loss: 27.971577681504286\n",
      "Epoch [150/150], Train Loss: 24.108353705484358, Test Loss: 28.003185841944312\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 39.36070200185307, Test Loss: 38.69152128541624\n",
      "Epoch [20/150], Train Loss: 30.35091914192575, Test Loss: 32.68467643044212\n",
      "Epoch [30/150], Train Loss: 27.219525330965638, Test Loss: 29.580231010139762\n",
      "Epoch [40/150], Train Loss: 25.56106949165219, Test Loss: 27.708009769390156\n",
      "Epoch [50/150], Train Loss: 25.288382458295978, Test Loss: 27.56772180037065\n",
      "Epoch [60/150], Train Loss: 24.89721905442535, Test Loss: 28.42451098058131\n",
      "Epoch [70/150], Train Loss: 25.077896512140992, Test Loss: 28.01597567967006\n",
      "Epoch [80/150], Train Loss: 24.977065386537646, Test Loss: 28.94664583577738\n",
      "Epoch [90/150], Train Loss: 25.06647148132324, Test Loss: 27.713375240177303\n",
      "Epoch [100/150], Train Loss: 23.236215291257764, Test Loss: 27.964313977724547\n",
      "Epoch [110/150], Train Loss: 23.59163444393971, Test Loss: 27.970090122966024\n",
      "Epoch [120/150], Train Loss: 24.78322850211722, Test Loss: 28.051415034702845\n",
      "Epoch [130/150], Train Loss: 24.913548835379178, Test Loss: 28.092603262368733\n",
      "Epoch [140/150], Train Loss: 25.062214673151736, Test Loss: 27.786404300045657\n",
      "Epoch [150/150], Train Loss: 22.56156090283003, Test Loss: 28.00601981522201\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 40.151816996590036, Test Loss: 36.89310187798042\n",
      "Epoch [20/150], Train Loss: 31.96217888378706, Test Loss: 33.91125324794224\n",
      "Epoch [30/150], Train Loss: 28.282754879310485, Test Loss: 29.62144460306539\n",
      "Epoch [40/150], Train Loss: 26.72280252800613, Test Loss: 28.265600229238537\n",
      "Epoch [50/150], Train Loss: 24.311123094402376, Test Loss: 28.18244825090681\n",
      "Epoch [60/150], Train Loss: 24.316865276899495, Test Loss: 29.059133703058418\n",
      "Epoch [70/150], Train Loss: 25.166353544641712, Test Loss: 27.736687523978098\n",
      "Epoch [80/150], Train Loss: 25.39948440926974, Test Loss: 27.566578753582842\n",
      "Epoch [90/150], Train Loss: 23.318006371670084, Test Loss: 27.91157492105063\n",
      "Epoch [100/150], Train Loss: 25.246280088581024, Test Loss: 27.420829995886074\n",
      "Epoch [110/150], Train Loss: 23.32931276540287, Test Loss: 28.47899159518155\n",
      "Epoch [120/150], Train Loss: 22.845562656590197, Test Loss: 29.110964366367885\n",
      "Epoch [130/150], Train Loss: 23.156072335165057, Test Loss: 28.114480204396433\n",
      "Epoch [140/150], Train Loss: 23.610467216616772, Test Loss: 29.611488639534294\n",
      "Epoch [150/150], Train Loss: 23.578083876312757, Test Loss: 27.998937458186955\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 39.19095466488697, Test Loss: 37.101056879216976\n",
      "Epoch [20/150], Train Loss: 30.66090124786877, Test Loss: 35.22994593830852\n",
      "Epoch [30/150], Train Loss: 27.519762383132683, Test Loss: 28.739611588515245\n",
      "Epoch [40/150], Train Loss: 27.853048405881786, Test Loss: 28.007222881564847\n",
      "Epoch [50/150], Train Loss: 26.232967414230597, Test Loss: 27.661183988893185\n",
      "Epoch [60/150], Train Loss: 25.446517262693312, Test Loss: 27.61582661913587\n",
      "Epoch [70/150], Train Loss: 24.14117530447538, Test Loss: 27.88760011846369\n",
      "Epoch [80/150], Train Loss: 25.541591750598343, Test Loss: 27.04479036702738\n",
      "Epoch [90/150], Train Loss: 24.64791166586954, Test Loss: 27.30555975282347\n",
      "Epoch [100/150], Train Loss: 23.526295677560274, Test Loss: 27.452230874594157\n",
      "Epoch [110/150], Train Loss: 25.12560128383949, Test Loss: 28.332498030229047\n",
      "Epoch [120/150], Train Loss: 23.59425398404481, Test Loss: 28.1694620058134\n",
      "Epoch [130/150], Train Loss: 23.65324299921755, Test Loss: 27.013860999763786\n",
      "Epoch [140/150], Train Loss: 23.76982825232334, Test Loss: 27.634129833865476\n",
      "Epoch [150/150], Train Loss: 22.486740912765754, Test Loss: 28.87332975709593\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 39.428457428979094, Test Loss: 44.2377504125818\n",
      "Epoch [20/150], Train Loss: 30.820005254276463, Test Loss: 37.488224029541016\n",
      "Epoch [30/150], Train Loss: 25.91479043178871, Test Loss: 30.387746885225372\n",
      "Epoch [40/150], Train Loss: 26.30555305480957, Test Loss: 27.746261720533496\n",
      "Epoch [50/150], Train Loss: 24.945763028254273, Test Loss: 27.840000796627688\n",
      "Epoch [60/150], Train Loss: 25.65101583512103, Test Loss: 27.273802150379527\n",
      "Epoch [70/150], Train Loss: 24.904013024001824, Test Loss: 27.141287816035284\n",
      "Epoch [80/150], Train Loss: 25.80862593103628, Test Loss: 27.753575164002257\n",
      "Epoch [90/150], Train Loss: 23.88476134753618, Test Loss: 28.29963696467412\n",
      "Epoch [100/150], Train Loss: 24.709081355860977, Test Loss: 28.143797589586928\n",
      "Epoch [110/150], Train Loss: 24.08095028361336, Test Loss: 27.877054858517337\n",
      "Epoch [120/150], Train Loss: 24.31076226156266, Test Loss: 27.585016448776443\n",
      "Epoch [130/150], Train Loss: 23.862220939261015, Test Loss: 28.098932191923065\n",
      "Epoch [140/150], Train Loss: 24.366179700757638, Test Loss: 27.943249095569957\n",
      "Epoch [150/150], Train Loss: 23.54341063577621, Test Loss: 28.3771739315677\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 40.57080574661005, Test Loss: 38.37684775018073\n",
      "Epoch [20/150], Train Loss: 31.250365407349634, Test Loss: 31.578567207633675\n",
      "Epoch [30/150], Train Loss: 27.491251432700235, Test Loss: 33.187970421530984\n",
      "Epoch [40/150], Train Loss: 26.7483256730877, Test Loss: 27.539289177238167\n",
      "Epoch [50/150], Train Loss: 24.35875648123319, Test Loss: 28.707272195196772\n",
      "Epoch [60/150], Train Loss: 25.42184160576492, Test Loss: 28.088476924153117\n",
      "Epoch [70/150], Train Loss: 25.36702793934306, Test Loss: 28.384936320317255\n",
      "Epoch [80/150], Train Loss: 26.15000584086434, Test Loss: 28.091344387500317\n",
      "Epoch [90/150], Train Loss: 25.164882065819913, Test Loss: 28.48284032747343\n",
      "Epoch [100/150], Train Loss: 24.698269653320313, Test Loss: 27.70661061769956\n",
      "Epoch [110/150], Train Loss: 23.281127591992988, Test Loss: 27.933975219726562\n",
      "Epoch [120/150], Train Loss: 24.630624958726226, Test Loss: 28.70042412002365\n",
      "Epoch [130/150], Train Loss: 23.264639182168928, Test Loss: 28.399037224905832\n",
      "Epoch [140/150], Train Loss: 22.940158593850057, Test Loss: 28.53793325052633\n",
      "Epoch [150/150], Train Loss: 22.8381627973963, Test Loss: 28.48383730108088\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 38.144924088775134, Test Loss: 37.97347299154703\n",
      "Epoch [20/150], Train Loss: 30.23870212992684, Test Loss: 31.463248413878603\n",
      "Epoch [30/150], Train Loss: 26.804993738893604, Test Loss: 29.89310274495707\n",
      "Epoch [40/150], Train Loss: 26.08418076311956, Test Loss: 29.504465722418452\n",
      "Epoch [50/150], Train Loss: 25.88375868875472, Test Loss: 27.649092339850093\n",
      "Epoch [60/150], Train Loss: 25.026088739614018, Test Loss: 27.397473000860835\n",
      "Epoch [70/150], Train Loss: 24.260884306860753, Test Loss: 27.941785366504224\n",
      "Epoch [80/150], Train Loss: 25.594823743476244, Test Loss: 28.792796518895532\n",
      "Epoch [90/150], Train Loss: 24.44765468659948, Test Loss: 27.868291136506315\n",
      "Epoch [100/150], Train Loss: 24.335893812335904, Test Loss: 27.626967033782563\n",
      "Epoch [110/150], Train Loss: 24.02638532919962, Test Loss: 28.523193805248706\n",
      "Epoch [120/150], Train Loss: 23.760191270171617, Test Loss: 28.612779815475662\n",
      "Epoch [130/150], Train Loss: 24.686650279310882, Test Loss: 27.282053513960406\n",
      "Epoch [140/150], Train Loss: 24.699842352945296, Test Loss: 28.527727622490424\n",
      "Epoch [150/150], Train Loss: 23.46401683619765, Test Loss: 28.503676922290357\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 38.957784308761845, Test Loss: 37.60865357634309\n",
      "Epoch [20/150], Train Loss: 31.36031708013816, Test Loss: 31.077196740484858\n",
      "Epoch [30/150], Train Loss: 28.081819590584175, Test Loss: 29.41097452733424\n",
      "Epoch [40/150], Train Loss: 25.204860643480647, Test Loss: 27.436960715752143\n",
      "Epoch [50/150], Train Loss: 26.337566288181993, Test Loss: 29.29792931792024\n",
      "Epoch [60/150], Train Loss: 25.35261389622923, Test Loss: 27.56430061142166\n",
      "Epoch [70/150], Train Loss: 25.784494919073385, Test Loss: 27.572462527782886\n",
      "Epoch [80/150], Train Loss: 24.72671157336626, Test Loss: 28.16841108148748\n",
      "Epoch [90/150], Train Loss: 24.314654147038695, Test Loss: 27.86068170720881\n",
      "Epoch [100/150], Train Loss: 24.03299259123255, Test Loss: 27.691089679668476\n",
      "Epoch [110/150], Train Loss: 24.01863133164703, Test Loss: 27.675749716820654\n",
      "Epoch [120/150], Train Loss: 23.345262214785716, Test Loss: 27.413022053706182\n",
      "Epoch [130/150], Train Loss: 23.141458229940447, Test Loss: 27.850420122022754\n",
      "Epoch [140/150], Train Loss: 22.965265608615564, Test Loss: 28.033089278580306\n",
      "Epoch [150/150], Train Loss: 23.400739632278192, Test Loss: 28.411031921188552\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.01, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 39.898688006791915, Test Loss: 41.133569643094944\n",
      "Epoch [20/150], Train Loss: 32.040257200647574, Test Loss: 37.182236460896284\n",
      "Epoch [30/150], Train Loss: 28.812150379868804, Test Loss: 30.640542092261377\n",
      "Epoch [40/150], Train Loss: 26.363747481049085, Test Loss: 28.217617530327338\n",
      "Epoch [50/150], Train Loss: 25.419235492143475, Test Loss: 28.326267688305347\n",
      "Epoch [60/150], Train Loss: 25.30391683109471, Test Loss: 27.706153943941192\n",
      "Epoch [70/150], Train Loss: 25.42048472420114, Test Loss: 27.635470848578912\n",
      "Epoch [80/150], Train Loss: 25.34755506671843, Test Loss: 28.337227437403296\n",
      "Epoch [90/150], Train Loss: 25.25870086794994, Test Loss: 27.494312880875228\n",
      "Epoch [100/150], Train Loss: 24.99210950194812, Test Loss: 29.930242835701286\n",
      "Epoch [110/150], Train Loss: 25.1070855062516, Test Loss: 28.292930949818004\n",
      "Epoch [120/150], Train Loss: 23.970592311171234, Test Loss: 28.63624622295429\n",
      "Epoch [130/150], Train Loss: 23.477117006895973, Test Loss: 28.16916255207805\n",
      "Epoch [140/150], Train Loss: 23.710926587464378, Test Loss: 28.16049603053502\n",
      "Epoch [150/150], Train Loss: 23.81835265237777, Test Loss: 28.09553324711787\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.16790702069392, Test Loss: 30.646958239666827\n",
      "Epoch [20/150], Train Loss: 26.871239609014793, Test Loss: 29.913927003934784\n",
      "Epoch [30/150], Train Loss: 25.154501161418978, Test Loss: 28.71842825257933\n",
      "Epoch [40/150], Train Loss: 25.608183119726963, Test Loss: 30.176240599000607\n",
      "Epoch [50/150], Train Loss: 26.365619371758132, Test Loss: 27.461788499510135\n",
      "Epoch [60/150], Train Loss: 25.532202235987928, Test Loss: 28.26003458592799\n",
      "Epoch [70/150], Train Loss: 25.33196430518979, Test Loss: 27.9338875807725\n",
      "Epoch [80/150], Train Loss: 24.569911243876472, Test Loss: 29.9504786652404\n",
      "Epoch [90/150], Train Loss: 25.497766400946947, Test Loss: 29.343727433836307\n",
      "Epoch [100/150], Train Loss: 25.60197615076284, Test Loss: 27.958665203738523\n",
      "Epoch [110/150], Train Loss: 23.624546882754466, Test Loss: 29.001461177677303\n",
      "Epoch [120/150], Train Loss: 23.446331180509972, Test Loss: 29.381289717438932\n",
      "Epoch [130/150], Train Loss: 24.42778874381644, Test Loss: 29.494243720909218\n",
      "Epoch [140/150], Train Loss: 22.116263542800652, Test Loss: 28.283083184972988\n",
      "Epoch [150/150], Train Loss: 22.613781544419584, Test Loss: 28.69209002209948\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.171411845723135, Test Loss: 36.81166975838797\n",
      "Epoch [20/150], Train Loss: 25.842653824853116, Test Loss: 28.619849811900746\n",
      "Epoch [30/150], Train Loss: 25.23431751688973, Test Loss: 28.423519654707476\n",
      "Epoch [40/150], Train Loss: 25.782763978301503, Test Loss: 28.091693803861542\n",
      "Epoch [50/150], Train Loss: 25.161154043479044, Test Loss: 28.507935709767526\n",
      "Epoch [60/150], Train Loss: 26.263422074865122, Test Loss: 27.434496867192255\n",
      "Epoch [70/150], Train Loss: 25.877707146816565, Test Loss: 27.418338763249384\n",
      "Epoch [80/150], Train Loss: 25.687274044849833, Test Loss: 29.881016694106066\n",
      "Epoch [90/150], Train Loss: 24.719507336225664, Test Loss: 29.403846542556565\n",
      "Epoch [100/150], Train Loss: 24.24606592772437, Test Loss: 27.86415838266348\n",
      "Epoch [110/150], Train Loss: 25.285374331865153, Test Loss: 28.853709010334757\n",
      "Epoch [120/150], Train Loss: 23.93003734526087, Test Loss: 31.237115587506974\n",
      "Epoch [130/150], Train Loss: 23.976225330790534, Test Loss: 33.144101576371625\n",
      "Epoch [140/150], Train Loss: 24.399725044750777, Test Loss: 29.13415931107162\n",
      "Epoch [150/150], Train Loss: 22.958755799590563, Test Loss: 28.754960493607953\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.516171746175797, Test Loss: 29.63920496655749\n",
      "Epoch [20/150], Train Loss: 27.11214301937916, Test Loss: 28.940787525920125\n",
      "Epoch [30/150], Train Loss: 25.555457674870727, Test Loss: 29.00203982266513\n",
      "Epoch [40/150], Train Loss: 26.596790326227907, Test Loss: 28.38596244911095\n",
      "Epoch [50/150], Train Loss: 26.60316436017146, Test Loss: 28.48826264715814\n",
      "Epoch [60/150], Train Loss: 24.299944730664862, Test Loss: 27.645641128738205\n",
      "Epoch [70/150], Train Loss: 24.9245114185771, Test Loss: 29.477214738920136\n",
      "Epoch [80/150], Train Loss: 24.55815375281162, Test Loss: 28.588275339696313\n",
      "Epoch [90/150], Train Loss: 25.075524064361073, Test Loss: 27.57575887209409\n",
      "Epoch [100/150], Train Loss: 25.254969462410347, Test Loss: 29.96880442136294\n",
      "Epoch [110/150], Train Loss: 25.113030155369493, Test Loss: 28.44122700877004\n",
      "Epoch [120/150], Train Loss: 24.239468421310676, Test Loss: 27.928784556203073\n",
      "Epoch [130/150], Train Loss: 24.92885323196161, Test Loss: 28.612179471300795\n",
      "Epoch [140/150], Train Loss: 22.176024014832542, Test Loss: 28.241927580399945\n",
      "Epoch [150/150], Train Loss: 24.45693534475858, Test Loss: 28.59187326802836\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.82920727338947, Test Loss: 34.332267736459706\n",
      "Epoch [20/150], Train Loss: 27.5849717937532, Test Loss: 29.695698056902206\n",
      "Epoch [30/150], Train Loss: 26.06090347415111, Test Loss: 34.73043072688115\n",
      "Epoch [40/150], Train Loss: 26.685397488953637, Test Loss: 28.575131503018465\n",
      "Epoch [50/150], Train Loss: 25.935600868600314, Test Loss: 28.44739666232815\n",
      "Epoch [60/150], Train Loss: 25.47980031498143, Test Loss: 27.39073339685217\n",
      "Epoch [70/150], Train Loss: 25.81747995595463, Test Loss: 27.477832298774224\n",
      "Epoch [80/150], Train Loss: 24.94417181171355, Test Loss: 29.144353643640294\n",
      "Epoch [90/150], Train Loss: 24.84247971831775, Test Loss: 28.228431528264824\n",
      "Epoch [100/150], Train Loss: 26.02542754626665, Test Loss: 28.644912645414276\n",
      "Epoch [110/150], Train Loss: 25.18527605650855, Test Loss: 28.771138748565278\n",
      "Epoch [120/150], Train Loss: 24.690760309188093, Test Loss: 26.839923982496387\n",
      "Epoch [130/150], Train Loss: 24.040545179023116, Test Loss: 28.758256392045453\n",
      "Epoch [140/150], Train Loss: 25.44413574093678, Test Loss: 28.175137705617136\n",
      "Epoch [150/150], Train Loss: 24.422410671046524, Test Loss: 28.181814738682338\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.319685951608125, Test Loss: 40.48880460664824\n",
      "Epoch [20/150], Train Loss: 27.05697027112617, Test Loss: 30.852419939908113\n",
      "Epoch [30/150], Train Loss: 26.24518510161853, Test Loss: 28.387553351266043\n",
      "Epoch [40/150], Train Loss: 25.34032564006868, Test Loss: 31.676989146641322\n",
      "Epoch [50/150], Train Loss: 25.659214182369045, Test Loss: 27.44958074990805\n",
      "Epoch [60/150], Train Loss: 25.08950897592013, Test Loss: 27.17606831835462\n",
      "Epoch [70/150], Train Loss: 24.81830304880611, Test Loss: 27.588317301366235\n",
      "Epoch [80/150], Train Loss: 24.994982078427174, Test Loss: 27.97798453987419\n",
      "Epoch [90/150], Train Loss: 25.86770698047075, Test Loss: 28.46879530572272\n",
      "Epoch [100/150], Train Loss: 26.230828763617843, Test Loss: 29.50498680015663\n",
      "Epoch [110/150], Train Loss: 25.77508446114962, Test Loss: 27.735960675524428\n",
      "Epoch [120/150], Train Loss: 24.36028278538438, Test Loss: 28.478388550993685\n",
      "Epoch [130/150], Train Loss: 24.908461092339188, Test Loss: 32.95469611031668\n",
      "Epoch [140/150], Train Loss: 25.197763511782785, Test Loss: 27.748736443457666\n",
      "Epoch [150/150], Train Loss: 24.507219045670308, Test Loss: 27.326040961525656\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 26.73441906913382, Test Loss: 30.84021248755517\n",
      "Epoch [20/150], Train Loss: 27.08212507904553, Test Loss: 28.57512310573033\n",
      "Epoch [30/150], Train Loss: 27.827511768653746, Test Loss: 29.60594437339089\n",
      "Epoch [40/150], Train Loss: 26.111593828044953, Test Loss: 28.45179964040781\n",
      "Epoch [50/150], Train Loss: 26.645787436063173, Test Loss: 27.806091209510704\n",
      "Epoch [60/150], Train Loss: 26.856097599717437, Test Loss: 28.556531163005086\n",
      "Epoch [70/150], Train Loss: 26.143810322245614, Test Loss: 28.671536408461535\n",
      "Epoch [80/150], Train Loss: 25.136711039308643, Test Loss: 27.363754916500735\n",
      "Epoch [90/150], Train Loss: 24.895374085473232, Test Loss: 27.519459699655506\n",
      "Epoch [100/150], Train Loss: 24.79185286975298, Test Loss: 28.582862829233143\n",
      "Epoch [110/150], Train Loss: 24.884265011646708, Test Loss: 28.09526644124613\n",
      "Epoch [120/150], Train Loss: 24.994091593632934, Test Loss: 27.712354907741794\n",
      "Epoch [130/150], Train Loss: 25.060516751398804, Test Loss: 27.279290533685064\n",
      "Epoch [140/150], Train Loss: 23.982665702944896, Test Loss: 26.838966035223628\n",
      "Epoch [150/150], Train Loss: 24.72904409189693, Test Loss: 28.087759290422714\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.16060730981045, Test Loss: 32.08214036520425\n",
      "Epoch [20/150], Train Loss: 28.195903327816822, Test Loss: 31.076197116405933\n",
      "Epoch [30/150], Train Loss: 26.61991862000012, Test Loss: 30.344514648635666\n",
      "Epoch [40/150], Train Loss: 26.894810573390274, Test Loss: 28.87965777013209\n",
      "Epoch [50/150], Train Loss: 26.405399929109166, Test Loss: 29.975845931412337\n",
      "Epoch [60/150], Train Loss: 25.502193357123705, Test Loss: 28.641193043101918\n",
      "Epoch [70/150], Train Loss: 26.505040684684378, Test Loss: 28.64015467755206\n",
      "Epoch [80/150], Train Loss: 25.995602148087297, Test Loss: 29.971717586765042\n",
      "Epoch [90/150], Train Loss: 25.803784442338788, Test Loss: 31.294294827944274\n",
      "Epoch [100/150], Train Loss: 25.304575591790872, Test Loss: 28.499093885545605\n",
      "Epoch [110/150], Train Loss: 25.65601604649278, Test Loss: 27.6995430488091\n",
      "Epoch [120/150], Train Loss: 26.1628912253458, Test Loss: 27.904256919761757\n",
      "Epoch [130/150], Train Loss: 25.266676730796938, Test Loss: 28.189753123692103\n",
      "Epoch [140/150], Train Loss: 25.976364636030354, Test Loss: 30.32517220137955\n",
      "Epoch [150/150], Train Loss: 24.97414899732246, Test Loss: 28.17621763650473\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.599386496622053, Test Loss: 33.76597748793565\n",
      "Epoch [20/150], Train Loss: 27.269711203653305, Test Loss: 35.13467890256411\n",
      "Epoch [30/150], Train Loss: 26.969485323546362, Test Loss: 33.3193673715963\n",
      "Epoch [40/150], Train Loss: 26.36837218863065, Test Loss: 29.850527701439795\n",
      "Epoch [50/150], Train Loss: 26.262669979157995, Test Loss: 31.221086006659966\n",
      "Epoch [60/150], Train Loss: 26.2866201869777, Test Loss: 28.095233124571962\n",
      "Epoch [70/150], Train Loss: 25.818476092229123, Test Loss: 28.508814551613547\n",
      "Epoch [80/150], Train Loss: 25.538948102857244, Test Loss: 28.588744324523134\n",
      "Epoch [90/150], Train Loss: 25.72525887411149, Test Loss: 28.53134145365133\n",
      "Epoch [100/150], Train Loss: 25.334426135704167, Test Loss: 28.592788349498402\n",
      "Epoch [110/150], Train Loss: 26.632387867911916, Test Loss: 29.225175485982522\n",
      "Epoch [120/150], Train Loss: 24.95050359006788, Test Loss: 27.652144097662593\n",
      "Epoch [130/150], Train Loss: 25.23259029701108, Test Loss: 32.55070842396129\n",
      "Epoch [140/150], Train Loss: 25.710479792610542, Test Loss: 27.137045476343726\n",
      "Epoch [150/150], Train Loss: 24.907569066032035, Test Loss: 29.4604941529113\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.1, regularization param 0.01, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.72558419899862, Test Loss: 33.75772248305284\n",
      "Epoch [20/150], Train Loss: 27.493018109681177, Test Loss: 48.44231246353744\n",
      "Epoch [30/150], Train Loss: 25.97486123256996, Test Loss: 33.571288591855534\n",
      "Epoch [40/150], Train Loss: 27.1311894213567, Test Loss: 29.31410128110415\n",
      "Epoch [50/150], Train Loss: 26.79170717333184, Test Loss: 29.095026065776874\n",
      "Epoch [60/150], Train Loss: 27.062319270900037, Test Loss: 30.277105083713284\n",
      "Epoch [70/150], Train Loss: 25.423842608342405, Test Loss: 31.108247880811817\n",
      "Epoch [80/150], Train Loss: 25.227037085861458, Test Loss: 29.314389315518465\n",
      "Epoch [90/150], Train Loss: 24.88843762131988, Test Loss: 28.197091263610048\n",
      "Epoch [100/150], Train Loss: 26.414908093311748, Test Loss: 28.243194728702694\n",
      "Epoch [110/150], Train Loss: 26.066187045613272, Test Loss: 29.23541898851271\n",
      "Epoch [120/150], Train Loss: 24.67189287279473, Test Loss: 29.22083785317161\n",
      "Epoch [130/150], Train Loss: 25.902901096031314, Test Loss: 29.61474411208908\n",
      "Epoch [140/150], Train Loss: 25.704377871654074, Test Loss: 27.91252966050978\n",
      "Epoch [150/150], Train Loss: 24.837077494136622, Test Loss: 29.39328520638602\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 28.345009350385823, Test Loss: 29.928465731732256\n",
      "Epoch [20/150], Train Loss: 28.41042357272789, Test Loss: 30.712186838125255\n",
      "Epoch [30/150], Train Loss: 25.31322259746614, Test Loss: 35.32271382715795\n",
      "Epoch [40/150], Train Loss: 27.077264282351635, Test Loss: 29.24122158892743\n",
      "Epoch [50/150], Train Loss: 25.154365921020506, Test Loss: 28.304927949781543\n",
      "Epoch [60/150], Train Loss: 25.740507657410667, Test Loss: 29.761161234471704\n",
      "Epoch [70/150], Train Loss: 26.34864805878186, Test Loss: 30.876725630326703\n",
      "Epoch [80/150], Train Loss: 25.056177920982485, Test Loss: 28.756557216891995\n",
      "Epoch [90/150], Train Loss: 25.08297637564237, Test Loss: 33.19903091331581\n",
      "Epoch [100/150], Train Loss: 24.65219469539455, Test Loss: 29.6610451983167\n",
      "Epoch [110/150], Train Loss: 25.262222577704758, Test Loss: 28.266318482237978\n",
      "Epoch [120/150], Train Loss: 24.546772616026832, Test Loss: 27.369509090076793\n",
      "Epoch [130/150], Train Loss: 25.772705509623542, Test Loss: 28.843292112474316\n",
      "Epoch [140/150], Train Loss: 24.697619491327004, Test Loss: 28.749801437576096\n",
      "Epoch [150/150], Train Loss: 23.99105049508517, Test Loss: 29.004508996938732\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.0, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 27.54225203091981, Test Loss: 31.678116909869306\n",
      "Epoch [20/150], Train Loss: 26.83013532669818, Test Loss: 35.134154183524\n",
      "Epoch [30/150], Train Loss: 26.79713746367908, Test Loss: 28.05924940728522\n",
      "Epoch [40/150], Train Loss: 27.012130186987704, Test Loss: 27.55985148541339\n",
      "Epoch [50/150], Train Loss: 25.458629307981397, Test Loss: 28.66689902466613\n",
      "Epoch [60/150], Train Loss: 26.45161141567543, Test Loss: 29.579395789604682\n",
      "Epoch [70/150], Train Loss: 26.998766364425908, Test Loss: 30.15198888407125\n",
      "Epoch [80/150], Train Loss: 25.58979136357542, Test Loss: 28.862963143881267\n",
      "Epoch [90/150], Train Loss: 25.40906674744653, Test Loss: 30.80537127209948\n",
      "Epoch [100/150], Train Loss: 25.722864957715643, Test Loss: 28.372121662288517\n",
      "Epoch [110/150], Train Loss: 25.462591265068678, Test Loss: 27.27059047253101\n",
      "Epoch [120/150], Train Loss: 26.679089342961547, Test Loss: 28.33035501257166\n",
      "Epoch [130/150], Train Loss: 25.680941759953733, Test Loss: 29.799188316642464\n",
      "Epoch [140/150], Train Loss: 25.153571907418673, Test Loss: 28.719849375935343\n",
      "Epoch [150/150], Train Loss: 24.718877060686957, Test Loss: 26.87134364982704\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.0001, dropout p 0.1, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.346777881559778, Test Loss: 32.11064024095411\n",
      "Epoch [20/150], Train Loss: 27.705860200475474, Test Loss: 30.0071426242977\n",
      "Epoch [30/150], Train Loss: 28.51790309968542, Test Loss: 29.245975915487712\n",
      "Epoch [40/150], Train Loss: 26.351673739073707, Test Loss: 28.195840860342052\n",
      "Epoch [50/150], Train Loss: 26.36056077050381, Test Loss: 29.192033965866287\n",
      "Epoch [60/150], Train Loss: 25.485734545598266, Test Loss: 30.376309035660384\n",
      "Epoch [70/150], Train Loss: 25.795598858692607, Test Loss: 28.475899956443094\n",
      "Epoch [80/150], Train Loss: 26.410607622490556, Test Loss: 29.17008667487603\n",
      "Epoch [90/150], Train Loss: 24.20956814250008, Test Loss: 27.722297247354085\n",
      "Epoch [100/150], Train Loss: 26.255174167820666, Test Loss: 29.52762762292639\n",
      "Epoch [110/150], Train Loss: 27.242946193257318, Test Loss: 29.40442692149769\n",
      "Epoch [120/150], Train Loss: 26.135445704225635, Test Loss: 28.291452283983105\n",
      "Epoch [130/150], Train Loss: 23.678957842217116, Test Loss: 29.366305239788897\n",
      "Epoch [140/150], Train Loss: 25.091315604037927, Test Loss: 26.36963804666098\n",
      "Epoch [150/150], Train Loss: 24.006748787301486, Test Loss: 27.01824718326717\n",
      "Training model with hidden size 256, batch size 32, learning rate 0.5, regularization param 0.001, dropout p 0.01, num_epochs 150\n",
      "Epoch [10/150], Train Loss: 29.22936817857086, Test Loss: 34.86408176669827\n",
      "Epoch [20/150], Train Loss: 26.648723452208472, Test Loss: 33.03210952065208\n",
      "Epoch [30/150], Train Loss: 26.685070087870614, Test Loss: 30.42994974805163\n",
      "Epoch [40/150], Train Loss: 27.24149663956439, Test Loss: 30.81751533607384\n",
      "Epoch [50/150], Train Loss: 26.05070764510358, Test Loss: 29.29323538247641\n",
      "Epoch [60/150], Train Loss: 26.350337056644626, Test Loss: 29.319312553901177\n",
      "Epoch [70/150], Train Loss: 27.528699768566696, Test Loss: 33.07154137747629\n",
      "Epoch [80/150], Train Loss: 26.975207500770445, Test Loss: 28.56902469288219\n",
      "Epoch [90/150], Train Loss: 25.9648313866287, Test Loss: 28.914419644838805\n",
      "Epoch [100/150], Train Loss: 25.512962666495902, Test Loss: 28.15215980232536\n",
      "Epoch [110/150], Train Loss: 25.950012582247375, Test Loss: 28.16910082334048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Definir la arquitectura de la CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=84, dropout_p=0.01):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_size, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size * (input_size - 2), hidden_size)  # Adjust according to the output size of conv1d\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.unsqueeze(1))  # Add channel dimension\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output of the convolution\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def grid_search(X_train, X_test, y_train, y_test, hidden_sizes, batch_sizes, learning_rates, regularization_params, dropout_ps, num_epochs_range, epoch_interval):\n",
    "    results = []\n",
    "    for num_epochs in num_epochs_range:\n",
    "        for hidden_size, batch_size, learning_rate, regularization_param, dropout_p in itertools.product(hidden_sizes, batch_sizes, learning_rates, regularization_params, dropout_ps):\n",
    "            print(f'Training model with hidden size {hidden_size}, batch size {batch_size}, learning rate {learning_rate}, regularization param {regularization_param}, dropout p {dropout_p}, num_epochs {num_epochs}')\n",
    "            # Convertir los datos de DataFrame a numpy arrays\n",
    "            X_train_np, y_train_np = X_train.to_numpy(), y_train.to_numpy().flatten()\n",
    "            X_test_np, y_test_np = X_test.to_numpy(), y_test.to_numpy().flatten()\n",
    "\n",
    "            # Inicializar el modelo\n",
    "            model = CNN(input_size=X_train_np.shape[1], hidden_size=hidden_size, dropout_p=dropout_p)\n",
    "\n",
    "            # Definir la funciÃ³n de pÃ©rdida y el optimizador\n",
    "            loss_fn = nn.SmoothL1Loss()\n",
    "            optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=regularization_param)\n",
    "\n",
    "            # Crear DataLoader\n",
    "            train_ds = TensorDataset(torch.tensor(X_train_np, dtype=torch.float32), torch.tensor(y_train_np, dtype=torch.float32))\n",
    "            test_ds = TensorDataset(torch.tensor(X_test_np, dtype=torch.float32), torch.tensor(y_test_np, dtype=torch.float32))\n",
    "            train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "            test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "            # Entrenamiento del modelo\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                for inputs, labels in train_dl:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "                train_loss /= len(train_dl.dataset)\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_dl:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_fn(outputs, labels.unsqueeze(1))\n",
    "                        test_loss += loss.item() * inputs.size(0)\n",
    "                test_loss /= len(test_dl.dataset)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                if (epoch + 1) % epoch_interval == 0:\n",
    "                    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss}, Test Loss: {test_loss}')\n",
    "\n",
    "            # Guardar los resultados del modelo\n",
    "            results.append({\n",
    "                'hidden_size': hidden_size,\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': learning_rate,\n",
    "                'regularization_param': regularization_param,\n",
    "                'dropout_p': dropout_p,\n",
    "                'num_epochs': num_epochs,\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'final_test_loss': test_loss\n",
    "            })\n",
    "\n",
    "    # Encontrar el modelo con el menor test loss final\n",
    "    best_model_result = min(results, key=lambda x: x['final_test_loss'])\n",
    "\n",
    "    return best_model_result, results\n",
    "\n",
    "# Definir los rangos para los hiperparÃ¡metros\n",
    "\n",
    "\n",
    "hidden_sizes = [64, 128, 256]\n",
    "num_epochs_range = [50, 100, 150]\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "regularization_params = [0.0001, 0.001, 0.01]\n",
    "dropout_ps = [0.01, 0., 0.1]\n",
    "epoch_interval = 10\n",
    "\n",
    "\n",
    "# Realizar grid search\n",
    "best_model, all_results = grid_search(X_train, X_test, y_train, y_test, hidden_sizes, batch_sizes, learning_rates, regularization_params, dropout_ps, num_epochs_range, epoch_interval)\n",
    "\n",
    "# Imprimir los hiperparÃ¡metros del mejor modelo\n",
    "print('Best Model Hyperparameters:')\n",
    "print(best_model)\n",
    "\n",
    "# GrÃ¡fico de pÃ©rdidas\n",
    "plt.figure(figsize=(10, 6))\n",
    "for result in all_results:\n",
    "    plt.plot(range(len(result['train_losses'])), result['train_losses'], label=f\"Hidden Size: {result['hidden_size']}, Batch Size: {result['batch_size']}, Learning Rate: {result['learning_rate']}, Regularization Param: {result['regularization_param']}, Dropout P: {result['dropout_p']}, Num Epochs: {result['num_epochs']}\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Losses for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
